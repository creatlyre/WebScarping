{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "import csv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import importlib\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePathManager:\n",
    "    def __init__(self, site, city):\n",
    "        self.site = site\n",
    "        self.city = city\n",
    "        self.date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "        # self.date_today = '2024-07-11'  # Uncomment for fixed date testing\n",
    "\n",
    "        # Define the file paths\n",
    "        self.output = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/{self.site}/Daily'\n",
    "        self.archive_folder = fr'{self.output}/Archive'\n",
    "        self.file_path_done = fr'{self.output}/{self.date_today}-DONE-{self.site}.csv'\n",
    "        self.file_path_done_city = fr'{self.output}/{self.date_today}-{self.city}-{self.site}.csv'\n",
    "        self.file_path_output = fr\"{self.output}/{self.site} - {self.date_today}.xlsx\"\n",
    "        self.link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/{self.site}_links.csv'\n",
    "        self.logs_path = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/{self.site}'\n",
    "        self.storage_account_name = \"storagemyotas\"\n",
    "        self.storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "        # Local file path\n",
    "        self.local_file_path = f\"{self.output}/{self.site} - {self.date_today}.xlsx\"\n",
    "\n",
    "        # Azure Storage containers and blob name\n",
    "        self.container_name_raw = f\"raw/daily/{self.site}\"\n",
    "        self.container_name_refined = f\"refined/daily/{self.site}\"\n",
    "        self.blob_name = fr'{self.site} - {self.date_today}.xlsx'\n",
    "\n",
    "        # Logs processed path\n",
    "        self.file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{self.blob_name.split(\".\")[0]}'\n",
    "\n",
    "    def get_file_paths(self):\n",
    "        return {\n",
    "            'output': self.output,\n",
    "            'archive_folder': self.archive_folder,\n",
    "            'file_path_done': self.file_path_done,\n",
    "            'file_path_done_city': self.file_path_done_city,\n",
    "            'file_path_output': self.file_path_output,\n",
    "            'link_file': self.link_file,\n",
    "            'logs_path': self.logs_path,\n",
    "            'local_file_path': self.local_file_path,\n",
    "            'container_name_raw': self.container_name_raw,\n",
    "            'container_name_refined': self.container_name_refined,\n",
    "            'blob_name': self.blob_name,\n",
    "            'file_path_logs_processed': self.file_path_logs_processed,\n",
    "            \"storage_account_name\": self.storage_account_name,\n",
    "            \"storage_account_key\": self.storage_account_key,\n",
    "            \"date_today\": self.date_today\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoggerManager class to handle logging configuration and operations\n",
    "class LoggerManager:\n",
    "    def __init__(self, file_manager):\n",
    "        self.logs_path = file_manager.logs_path\n",
    "        self.ensure_log_folder_exists()  # Ensure log folder exists\n",
    "        \n",
    "        # Create logger objects for error, info, and done logs\n",
    "        self.logger_err = logging.getLogger('Error_logger')\n",
    "        self.logger_err.setLevel(logging.DEBUG)\n",
    "\n",
    "        self.logger_info = logging.getLogger('Info_logger')\n",
    "        self.logger_info.setLevel(logging.DEBUG)\n",
    "\n",
    "        self.logger_done = logging.getLogger('Done_logger')\n",
    "        self.logger_done.setLevel(logging.DEBUG)\n",
    "\n",
    "        # Create handlers\n",
    "        self.ch = logging.StreamHandler()\n",
    "        self.ch.setLevel(logging.DEBUG)\n",
    "\n",
    "        self.fh_error = logging.FileHandler(fr'{self.logs_path}/error_logs.log')\n",
    "        self.fh_error.setLevel(logging.DEBUG)\n",
    "\n",
    "        self.fh_info = logging.FileHandler(fr'{self.logs_path}/info_logs.log')\n",
    "        self.fh_info.setLevel(logging.INFO)\n",
    "\n",
    "        self.fh_done = logging.FileHandler(fr'{self.logs_path}/done_logs.log')\n",
    "        self.fh_done.setLevel(logging.INFO)\n",
    "\n",
    "        # Create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # Add formatter to handlers\n",
    "        self.ch.setFormatter(formatter)\n",
    "        self.fh_error.setFormatter(formatter)\n",
    "        self.fh_info.setFormatter(formatter)\n",
    "        self.fh_done.setFormatter(formatter)\n",
    "\n",
    "        # Add handlers to loggers\n",
    "        self.logger_err.addHandler(self.ch)\n",
    "        self.logger_err.addHandler(self.fh_error)\n",
    "\n",
    "        self.logger_info.addHandler(self.ch)\n",
    "        self.logger_info.addHandler(self.fh_info)\n",
    "\n",
    "        self.logger_done.addHandler(self.ch)\n",
    "        self.logger_done.addHandler(self.fh_done)\n",
    "\n",
    "    def ensure_log_folder_exists(self):\n",
    "        if not os.path.exists(self.logs_path):\n",
    "            os.makedirs(self.logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductScraper:\n",
    "    def __init__(self, url, city, css_selectors, file_manager, logger):\n",
    "        self.logger = logger\n",
    "        self.url = url\n",
    "        self.city = city\n",
    "        self.file_manager = file_manager\n",
    "        self.css_currency = css_selectors['currency']\n",
    "        self.css_currency_list = css_selectors['currency_list']\n",
    "        self.css_products_count = css_selectors['products_count']\n",
    "        self.css_product_card = css_selectors['product_card']\n",
    "        self.css_tour_price = css_selectors['tour_price']\n",
    "        self.css_tour_price_discount = css_selectors['tour_price_discount']\n",
    "        self.css_ratings = css_selectors['ratings']\n",
    "        self.css_review_count = css_selectors['review_count']\n",
    "        self.css_category_label = css_selectors['category_label']\n",
    "        self.driver = self.initilize_driver()\n",
    "\n",
    "        self.logger.logger_info.info(\"Successfully initiated ProductScraper for city: %s\", self.city)\n",
    "\n",
    "\n",
    "    def initilize_driver(self) -> WebDriver:\n",
    "        try:\n",
    "            self.logger.logger_info.info(\"Initializing the Chrome driver and logging into the website\")\n",
    "\n",
    "            # Setting up Chrome options\n",
    "            options = webdriver.ChromeOptions()\n",
    "            # options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "            options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "\n",
    "            # Initialize the Chrome driver\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.maximize_window()\n",
    "            \n",
    "            return driver\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.logger_err.error(f\"An error occurred during login: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def quit_driver(self, driver: WebDriver) -> None:\n",
    "        driver.quit()    \n",
    "    \n",
    "    def get_url(self):\n",
    "        self.driver.get(self.url)\n",
    "        time.sleep(1)\n",
    "\n",
    "    def select_currency(self):\n",
    "        currency_button = self.driver.find_element(By.CSS_SELECTOR, self.css_currency)\n",
    "        if \"EUR\" not in currency_button.get_attribute('innerHTML'):\n",
    "            currency_button.click()\n",
    "            currency_list = self.driver.find_elements(By.CSS_SELECTOR, self.css_currency_list)\n",
    "            for currency in currency_list:\n",
    "                if 'EUR' in currency.get_attribute('innerHTML'):\n",
    "                    currency.click()\n",
    "                    break\n",
    "\n",
    "    def get_product_count(self):\n",
    "        products_count_selenium = self.driver.find_element(By.CSS_SELECTOR, self.css_products_count)\n",
    "        if 'Loading' in products_count_selenium.get_attribute('innerHTML'):\n",
    "            time.sleep(1.5)\n",
    "        products_count_selenium = self.driver.find_element(By.CSS_SELECTOR, self.css_products_count)\n",
    "        products_count = int(products_count_selenium.get_attribute('innerHTML').split(' ')[0])\n",
    "        return products_count\n",
    "\n",
    "    def load_all_products(self, products_count, scroll_attempts=5, scroll_step=200):\n",
    "        # Load the page with all products\n",
    "        self.driver.get(f\"{self.url}?limit={products_count}\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        total_height = self.driver.execute_script(\"return document.body.scrollHeight\") * 0.9\n",
    "        target_scroll_increment = total_height / scroll_attempts\n",
    "        current_scroll_position = 0\n",
    "\n",
    "        for _ in range(scroll_attempts):\n",
    "            target_scroll_position = current_scroll_position + target_scroll_increment\n",
    "            \n",
    "            while current_scroll_position < target_scroll_position:\n",
    "                self.driver.execute_script(f\"window.scrollBy(0, {scroll_step});\")\n",
    "                current_scroll_position += scroll_step\n",
    "                time.sleep(0.01)  # Fast scrolling\n",
    "            \n",
    "            time.sleep(1)  # Allow content to load\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if current_scroll_position + self.driver.execute_script(\"return window.innerHeight\") >= new_height:\n",
    "                break\n",
    "\n",
    "    def scrape_products(self):\n",
    "        products = self.driver.find_elements(By.CSS_SELECTOR, self.css_product_card)\n",
    "        data = []\n",
    "        position = 1\n",
    "        date_today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "        product_site = \"Headout\"\n",
    "        \n",
    "        for product in products:\n",
    "            product_data = self.extract_product_data(product, position, date_today, product_site)\n",
    "            data.append(product_data)\n",
    "            position += 1\n",
    "        \n",
    "        return pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto'])\n",
    "\n",
    "    def extract_product_data(self, product, position, date_today, product_site):\n",
    "        product_title = product.find_element(By.TAG_NAME, 'a').text\n",
    "        product_url = product.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "\n",
    "        try:\n",
    "            product_price = product.find_element(By.CSS_SELECTOR, self.css_tour_price).text\n",
    "        except:\n",
    "            product_price = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            product_discount_price = product.find_element(By.CSS_SELECTOR, self.css_tour_price_discount).text\n",
    "            if product_discount_price == \"from\":\n",
    "                product_discount_price = \"N/A\"\n",
    "        except:\n",
    "            product_discount_price = \"N/A\"\n",
    "\n",
    "        if product_discount_price != 'N/A' :\n",
    "            product_discount_price, product_price = product_price, product_discount_price\n",
    "\n",
    "        product_ratings = product.find_element(By.CSS_SELECTOR, self.css_ratings).text\n",
    "        \n",
    "        try:\n",
    "            product_review_count = product.find_element(By.CSS_SELECTOR, self.css_review_count).text\n",
    "        except:\n",
    "            product_review_count = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            product_category = product.find_element(By.CSS_SELECTOR, self.css_category_label).text\n",
    "        except:\n",
    "            product_category = \"N/A\"\n",
    "\n",
    "        return [\n",
    "            product_title, product_url, product_price, product_ratings, product_review_count,\n",
    "            product_discount_price, date_today, position, product_category, product_site, self.city\n",
    "        ]\n",
    "    def save_to_csv(self, df):\n",
    "\n",
    "        self.quit_driver(self.driver)\n",
    "        # Save the DataFrame to CSV using paths from FilePathManager\n",
    "        file_path = self.file_manager.get_file_paths()['file_path_done_city']\n",
    "        df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')\n",
    "        self.logger.logger_done.info(f\"Rows: {len(df)} Data saved to {file_path}\")\n",
    "\n",
    "    def is_city_already_done(self):\n",
    "        file_path = self.file_manager.get_file_paths()['file_path_done_city']\n",
    "        return os.path.exists(file_path)  # Check if the file already exists\n",
    "    \n",
    "\n",
    "     # New method to combine CSV files into a single Excel file\n",
    "    def combine_csv_to_xlsx(self):\n",
    "        csv_files_locations = self.file_manager.get_file_paths()['output']\n",
    "        archive_folder = self.file_manager.get_file_paths()['archive_folder']\n",
    "        date_today = self.file_manager.get_file_paths()['date_today']\n",
    "        file_path_output = self.file_manager.get_file_paths()['file_path_output']\n",
    "        # Get all CSV files with the specified date prefix in the output directory\n",
    "        csv_files = [file for file in os.listdir(csv_files_locations) if file.endswith('.csv') and file.startswith(date_today)]\n",
    "\n",
    "        # Check if no CSV files were found\n",
    "        if not csv_files:\n",
    "            self.logger.logger_info.info(f\"No CSV files found with the date prefix '{date_today}'\")\n",
    "            return\n",
    "        # Specify the output Excel file path and name\n",
    "        # Ensure the archive folder exists\n",
    "        if not os.path.exists(archive_folder):\n",
    "            os.makedirs(archive_folder)    \n",
    "\n",
    "        writer = pd.ExcelWriter(file_path_output, engine='xlsxwriter')\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(csv_files_locations, csv_file)\n",
    "            \n",
    "            # Generate a sheet name based on the CSV file name\n",
    "            sheet_name = os.path.splitext(csv_file)[0]\n",
    "            sheet_name = sheet_name.split(date_today + '-')[1].split('-Headout')[0]\n",
    "            \n",
    "            df = pd.read_csv(csv_path)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        writer.close()\n",
    "        self.logger.logger_done.info(f\"Combined CSV files into '{file_path_output}'\")\n",
    "\n",
    "        # Move the original CSV files to the archive folder\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(csv_files_locations, csv_file)\n",
    "            destination_path = os.path.join(archive_folder, csv_file)\n",
    "            try:\n",
    "                shutil.move(csv_path, destination_path)\n",
    "                self.logger.logger_info.info(f\"Moved {csv_file} to the archive folder.\")\n",
    "            except FileNotFoundError as e:\n",
    "                self.logger.logger_err.error(f\"Error moving {csv_file}: {str(e)}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzureBlobUploader:\n",
    "    def __init__(self, file_manager, logger):\n",
    "        self.file_manager = file_manager\n",
    "        self.storage_account_name = self.file_manager.get_file_paths()['storage_account_name']\n",
    "        self.storage_account_key = self.file_manager.get_file_paths()['storage_account_key']\n",
    "        self.container_name_raw = self.file_manager.get_file_paths()['container_name_raw']\n",
    "        self.container_name_refined = self.file_manager.get_file_paths()['container_name_refined']\n",
    "        self.blob_name = self.file_manager.get_file_paths()['blob_name']\n",
    "        self.file_path_output = self.file_manager.get_file_paths()['file_path_output']\n",
    "        self.logger = logger\n",
    "        self.connection_string = f\"DefaultEndpointsProtocol=https;AccountName={self.storage_account_name};AccountKey={self.storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "        self.logger.logger_info.info(\"Sucessfuly initiated AzureBlobUploader\")\n",
    "\n",
    "    def upload_excel_to_azure_storage_account(self):\n",
    "        \"\"\"\n",
    "        Uploads the Excel file to Azure Blob Storage under the \"raw\" container.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a BlobServiceClient object using the connection string\n",
    "            blob_service_client = BlobServiceClient.from_connection_string(self.connection_string)\n",
    "\n",
    "            # Get a reference to the container\n",
    "            container_client = blob_service_client.get_container_client(self.container_name_raw)\n",
    "\n",
    "            # Upload the file to Azure Blob Storage\n",
    "            with open(self.file_path_output, \"rb\") as file:\n",
    "                container_client.upload_blob(name=self.blob_name, data=file)\n",
    "            \n",
    "            self.logger.logger_done.info(\"File uploaded successfully to Azure Blob Storage (raw).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.logger_err.error(f\"An error occurred while uploading to raw storage: {e}\")\n",
    "    def transform_upload_to_refined(self):\n",
    "        \"\"\"\n",
    "        Transforms and uploads the Excel file to Azure Blob Storage under the \"refined\" container.\n",
    "        \"\"\"\n",
    "        exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "        output_file_path = \"temp_file.xlsx\"  # Temporary file for transformation\n",
    "\n",
    "        try:\n",
    "            # Read the Excel file into a Pandas DataFrame\n",
    "            excel_data = pd.read_excel(self.file_path_output, sheet_name=None)\n",
    "\n",
    "            # Write the transformed data to a new Excel file\n",
    "            with pd.ExcelWriter(output_file_path) as writer:\n",
    "                for sheet_name, df in excel_data.items():\n",
    "                    if sheet_name in exclude_sheets:\n",
    "                        continue\n",
    "                    # Transform the DataFrame (add your transformation logic here)\n",
    "                    df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "                    df['IloscOpini'] = df['IloscOpini'].fillna(0)\n",
    "                    df['IloscOpini'] = df['IloscOpini'].str.replace('(', '').str.replace(')','')\n",
    "                    df['IloscOpini'] = df['IloscOpini'].apply(lambda x: int(float(x.replace('K', '')) * 1000) if isinstance(x, str) and 'K' in x else x)\n",
    "                    df['Przecena'] = df['Przecena'].str.replace(r'[$€£]', '', regex=True).str.strip()\n",
    "                    df['Cena'] = df['Cena'].str.replace(r'[$€£]', '', regex=True).str.strip()\n",
    "                    df['Opinia'] = df['Opinia'].fillna('N/A')\n",
    "                    df = df[df['Tytul'] != 'Tytul']\n",
    "                    df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "                    df = df[df['Data zestawienia'].str.len() > 4]\n",
    "                    df['Cena'] = df['Cena'].map(lambda x: x.split('from')[-1] if isinstance(x, str) and 'from' in x else x)\n",
    "                    df['Przecena'] = df['Przecena'].map(lambda x: x.split('per person')[0] if isinstance(x, str) and 'per person' in x.lower() else x)\n",
    "                    df['Przecena'] = df['Przecena'].str.replace(r'[$€£]', '', regex=True).str.strip()\n",
    "                    df['Opinia'] = df[\"Opinia\"].str.replace(\"NEW\", '')\n",
    "\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    # Upload the transformed Excel file to Azure Blob Storage\n",
    "            blob_service_client = BlobServiceClient.from_connection_string(self.connection_string)\n",
    "            container_client = blob_service_client.get_container_client(self.container_name_refined)\n",
    "\n",
    "            with open(output_file_path, \"rb\") as data:\n",
    "                container_client.upload_blob(name=self.blob_name, data=data)\n",
    "            \n",
    "            self.logger.logger_done.info(\"File uploaded successfully to Azure Blob Storage (refined).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.logger_err.error(f\"An error occurred while transforming and uploading to refined storage: {e}\")\n",
    "            \n",
    "        finally:\n",
    "            # Clean up the temporary file\n",
    "            if os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_selectors = {\n",
    "    'currency': 'button[class=\"elementText\"]',\n",
    "    'currency_list': 'div[class*=\"symbol-bold\"]',\n",
    "    'products_count': 'span[class*=\"product-count-text\"]',\n",
    "    'show_more_button': 'a[data-qa-marker*=\"loading-button\"]',\n",
    "    'product_card': 'div[id*=\"product-card-container\"]',\n",
    "    'tour_price': 'span[class*=\"tour-price\"]',\n",
    "    'tour_price_discount': 'div[class=\"tour-scratch-price\"]',\n",
    "    'ratings': 'span[class*=\"rating-count\"]',\n",
    "    'review_count': 'span[class*=\"review-count\"]',\n",
    "    'category_label': 'span[class*=\"booster-label\"]'\n",
    "}\n",
    "file_manager_logger = FilePathManager(\"Headout\", \"NA\")\n",
    "logger = LoggerManager(file_manager_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 13:36:12,398 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:36:13,713 - Info_logger - INFO - Successfully initiated ProductScraper for city: Lisbon\n",
      "2024-09-26 13:36:13,714 - Info_logger - INFO - Data for Lisbon already exists. Skipping...\n",
      "2024-09-26 13:36:13,714 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:36:14,930 - Info_logger - INFO - Successfully initiated ProductScraper for city: Rome\n",
      "2024-09-26 13:36:16,964 - Info_logger - INFO - Data for Rome already exists. Skipping...\n",
      "2024-09-26 13:36:16,965 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:36:18,195 - Info_logger - INFO - Successfully initiated ProductScraper for city: Athens\n",
      "2024-09-26 13:36:20,241 - Info_logger - INFO - Data for Athens already exists. Skipping...\n",
      "2024-09-26 13:36:20,244 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:36:21,480 - Info_logger - INFO - Successfully initiated ProductScraper for city: Florence\n",
      "2024-09-26 13:36:51,447 - Done_logger - INFO - Data saved to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Headout/Daily/2024-09-26-Florence-Headout.csv\n",
      "2024-09-26 13:36:51,450 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:36:52,661 - Info_logger - INFO - Successfully initiated ProductScraper for city: London\n",
      "2024-09-26 13:38:29,446 - Done_logger - INFO - Data saved to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Headout/Daily/2024-09-26-London-Headout.csv\n",
      "2024-09-26 13:38:29,448 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:38:30,771 - Info_logger - INFO - Successfully initiated ProductScraper for city: Heraklion\n",
      "2024-09-26 13:38:45,603 - Done_logger - INFO - Data saved to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Headout/Daily/2024-09-26-Heraklion-Headout.csv\n",
      "2024-09-26 13:38:45,604 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:38:46,833 - Info_logger - INFO - Successfully initiated ProductScraper for city: Sintra\n",
      "2024-09-26 13:39:01,120 - Done_logger - INFO - Data saved to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Headout/Daily/2024-09-26-Sintra-Headout.csv\n",
      "2024-09-26 13:39:01,121 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-26 13:39:02,328 - Info_logger - INFO - Successfully initiated ProductScraper for city: Porto\n",
      "2024-09-26 13:39:22,469 - Done_logger - INFO - Data saved to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Headout/Daily/2024-09-26-Porto-Headout.csv\n",
      "2024-09-26 13:39:22,943 - Done_logger - INFO - Combined CSV files into 'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Headout/Daily/Headout - 2024-09-26.xlsx'\n",
      "2024-09-26 13:39:22,949 - Info_logger - INFO - Moved 2024-09-26-Lisbon-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,954 - Info_logger - INFO - Moved 2024-09-26-Rome-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,959 - Info_logger - INFO - Moved 2024-09-26-Athens-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,964 - Info_logger - INFO - Moved 2024-09-26-Florence-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,969 - Info_logger - INFO - Moved 2024-09-26-London-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,974 - Info_logger - INFO - Moved 2024-09-26-Heraklion-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,979 - Info_logger - INFO - Moved 2024-09-26-Sintra-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,984 - Info_logger - INFO - Moved 2024-09-26-Porto-Headout.csv to the archive folder.\n",
      "2024-09-26 13:39:22,986 - Info_logger - INFO - Sucessfuly initiated AzureBlobUploader\n",
      "2024-09-26 13:39:23,579 - Error_logger - ERROR - An error occurred while uploading to raw storage: The specified blob already exists.\n",
      "RequestId:9debec57-c01e-0071-4308-108e36000000\n",
      "Time:2024-09-26T11:39:22.6127162Z\n",
      "ErrorCode:BlobAlreadyExists\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobAlreadyExists</Code><Message>The specified blob already exists.\n",
      "RequestId:9debec57-c01e-0071-4308-108e36000000\n",
      "Time:2024-09-26T11:39:22.6127162Z</Message></Error>\n",
      "2024-09-26 13:39:24,950 - Done_logger - INFO - File uploaded successfully to Azure Blob Storage (refined).\n"
     ]
    }
   ],
   "source": [
    "# Load the config from the JSON file\n",
    "with open('config_headout.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Access the city from the config\n",
    "cities = config['settings']['city']\n",
    "for city in cities:\n",
    "    url = f\"https://www.headout.com/tours/{city}/\"\n",
    "    file_manager = FilePathManager('Headout', city)\n",
    "    scraper = ProductScraper(url, city, css_selectors, file_manager, logger)\n",
    "\n",
    "    if scraper.is_city_already_done():\n",
    "        logger.logger_info.info(f\"Data for {city} already exists. Skipping...\")\n",
    "        continue\n",
    "    scraper.get_url()\n",
    "    scraper.select_currency()\n",
    "    products_count = scraper.get_product_count()\n",
    "    scraper.load_all_products(products_count)\n",
    "    df = scraper.scrape_products()\n",
    "    scraper.save_to_csv(df)\n",
    "\n",
    "scraper.combine_csv_to_xlsx()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 13:42:34,547 - Info_logger - INFO - Sucessfuly initiated AzureBlobUploader\n",
      "2024-09-26 13:42:35,066 - Error_logger - ERROR - An error occurred while uploading to raw storage: The specified blob already exists.\n",
      "RequestId:9b8cd588-601e-0068-4609-100e8d000000\n",
      "Time:2024-09-26T11:42:34.0496634Z\n",
      "ErrorCode:BlobAlreadyExists\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobAlreadyExists</Code><Message>The specified blob already exists.\n",
      "RequestId:9b8cd588-601e-0068-4609-100e8d000000\n",
      "Time:2024-09-26T11:42:34.0496634Z</Message></Error>\n",
      "2024-09-26 13:42:36,248 - Done_logger - INFO - File uploaded successfully to Azure Blob Storage (refined).\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AzureBlobUploader with storage account details\n",
    "blob_uploader = AzureBlobUploader(file_manager, logger)\n",
    "blob_uploader.upload_excel_to_azure_storage_account()\n",
    "blob_uploader.transform_upload_to_refined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
