{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153cdbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viator_daily.py is not running.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import csv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import Azure_stopVM\n",
    "import importlib\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "\n",
    "# date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# output_gyg = r'output/GYG'\n",
    "# archive_folder = fr'{output_gyg}/Archive'\n",
    "# file_path_done =fr'output/GYG/{date_today}-DONE-GYG.csv'  \n",
    "# file_path_output = fr\"output/GYG - {date_today}.xlsx\"\n",
    "# link_file = fr'resource/GYG_links.csv'\n",
    "# avg_file = fr'resource/avg-gyg.csv'\n",
    "# re_run_path = fr'output/GYG/{date_today}-ReRun-GYG.csv'\n",
    "# folder_path_with_txt_to_count_avg = 'Avg/GYG'\n",
    "\n",
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2024-07-11'\n",
    "output_gyg = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/GYG/Daily'\n",
    "archive_folder = fr'{output_gyg}/Archive'\n",
    "file_path_done =fr'{output_gyg}/{date_today}-DONE-GYG.csv'  \n",
    "file_path_output = fr\"{output_gyg}/GYG - {date_today}.xlsx\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/GYG_links.csv'\n",
    "max_page_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/GYG_max_page.csv'\n",
    "avg_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/avg-gyg.csv'\n",
    "re_run_path = fr'{output_gyg}/{date_today}-ReRun-GYG.csv'\n",
    "logs_path = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/GYG'\n",
    "# FOR ONE TIME USED NOT SYNCHORNIEZD WITH RUNING APPLCIATION\n",
    "folder_path_with_txt_to_count_avg = 'Avg/GYG'\n",
    "\n",
    "# Set the path of the local file\n",
    "local_file_path = f\"{output_gyg}/GYG - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/daily/gyg\"\n",
    "container_name_refined = \"refined/daily/gyg\"\n",
    "\n",
    "blob_name = fr'GYG - {date_today}.xlsx'\n",
    "file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8543026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger object\n",
    "logger_err = logging.getLogger('Error_logger')\n",
    "logger_err.setLevel(logging.DEBUG)\n",
    "logger_info = logging.getLogger('Info_logger')\n",
    "logger_info.setLevel(logging.DEBUG)\n",
    "logger_done = logging.getLogger('Done_logger')\n",
    "logger_done.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create file handler for error logs and set level to debug\n",
    "fh_error = logging.FileHandler(fr'{logs_path}/error_logs.log')\n",
    "fh_error.setLevel(logging.DEBUG)\n",
    "\n",
    "# create file handler for info logs and set level to info\n",
    "fh_info = logging.FileHandler(fr'{logs_path}/info_logs.log')\n",
    "fh_info.setLevel(logging.INFO)\n",
    "\n",
    "# create file handler for info logs and set level to info\n",
    "fh_done = logging.FileHandler(fr'{logs_path}/done_logs.log')\n",
    "fh_done.setLevel(logging.INFO)\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to handlers\n",
    "ch.setFormatter(formatter)\n",
    "fh_error.setFormatter(formatter)\n",
    "fh_info.setFormatter(formatter)\n",
    "fh_done.setFormatter(formatter)\n",
    "\n",
    "# add handlers to logger\n",
    "logger_err.addHandler(ch)\n",
    "logger_err.addHandler(fh_error)\n",
    "logger_info.addHandler(ch)\n",
    "logger_info.addHandler(fh_info)\n",
    "logger_done.addHandler(ch)\n",
    "logger_done.addHandler(fh_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00dab1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error_and_rerun(error):\n",
    "#     recipient_error = 'wojbal3@gmail.com'\n",
    "    tb = traceback.format_exc()\n",
    "    logger_err.error('An error occurred: {} on {}'.format(str(error), tb))\n",
    "#     subject = f'Error occurred - {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}'\n",
    "#     message = f'<html><body><p>Error occurred: {str(error)} on {tb}</p></body></html>'\n",
    "#     send_email(subject, message, recipient_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adcdd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_to_xlsx():\n",
    "    \"\"\"\n",
    "    This function combines all CSV files in the specified output directory that have\n",
    "    a filename starting with today's date into a single Excel file.\n",
    "    Each CSV file is written as a separate sheet in the Excel file.\n",
    "    After combining, the original CSV files are moved to the archive folder.\n",
    "    \"\"\"\n",
    "    # Get all CSV files with the specified date prefix in the output directory\n",
    "    csv_files = [file for file in os.listdir(output_gyg) if file.endswith('.csv') and file.startswith(date_today)]\n",
    "\n",
    "\n",
    "    # Check if no CSV files were found and exit the function if true\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found with the date prefix '{date_today}'\")\n",
    "        return\n",
    "\n",
    "    # Specify the output Excel file path and name\n",
    "    output_file = f\"{output_gyg}/GYG - {date_today}.xlsx\"\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Construct the full file path for the CSV file\n",
    "        csv_path = os.path.join(output_gyg, csv_file)\n",
    "        \n",
    "        # Generate a sheet name based on the CSV file name\n",
    "        sheet_name = os.path.splitext(csv_file)[0]\n",
    "        sheet_name = sheet_name.split(date_today + '-')[1].split('-GYG')[0]\n",
    "        \n",
    "        # Read the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Write the DataFrame to the Excel file as a new sheet\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save and close the Excel writer to finalize the Excel file\n",
    "    writer.close()\n",
    "\n",
    "    # Log the successful combination of CSV files\n",
    "    print(f\"Combined CSV files with date prefix '{date_today}' into '{output_file}'\")\n",
    "\n",
    "    # Move the original CSV files to the Archive folder\n",
    "    for csv_file in csv_files:\n",
    "        # Construct the full file path for the CSV file\n",
    "        csv_path = os.path.join(output_gyg, csv_file)\n",
    "        # Specify the destination path in the archive folder\n",
    "        destination_path = os.path.join(archive_folder, csv_file)\n",
    "        # Move the CSV file to the Archive folder\n",
    "        shutil.move(csv_path, destination_path)\n",
    "\n",
    "    # Log the successful archival of CSV files\n",
    "    print(f\"Moved {len(csv_files)} CSV file(s) to the '{archive_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4e9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce039fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_done(log_type):\n",
    "    global file_path_logs_processed\n",
    "    if log_type == 'Raw':\n",
    "        with open(f'{file_path_logs_processed}-raw.txt', 'w') as file:\n",
    "            file.write('Done')\n",
    "    elif log_type == 'Refined':\n",
    "        with open(f'{file_path_logs_processed}-refined.txt', 'w') as file:\n",
    "            file.write('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a40ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name):\n",
    "    try:\n",
    "        # Create a connection string to the Azure Storage account\n",
    "        connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "        # Create a BlobServiceClient object using the connection string\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "        # Get a reference to the container\n",
    "        container_client = blob_service_client.get_container_client(container_name_raw)\n",
    "\n",
    "        # Upload the file to Azure Blob Storage\n",
    "        with open(local_file_path, \"rb\") as file:\n",
    "            container_client.upload_blob(name=blob_name, data=file)\n",
    "        create_log_done('Raw')\n",
    "        print(\"File uploaded successfully to Azure Blob Storage (raw).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e642db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name):\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "    # Define the Azure Blob Storage connection details\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    # Read the Excel file into a Pandas DataFrame\n",
    "    excel_data = pd.read_excel(local_file_path, sheet_name=None)\n",
    "    output_file_path = \"temp_file.xlsx\"\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            if sheet_name in exclude_sheets:\n",
    "                continue\n",
    "            # Make changes to the df DataFrame as needed\n",
    "            df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "            df['IloscOpini'] = df['IloscOpini'].fillna(0)\n",
    "            df['Opinia'] = df['Opinia'].fillna('N/A')\n",
    "            df = df[df['Tytul'] != 'Tytul']\n",
    "            df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "            df = df[df['Data zestawienia'].str.len() > 4]\n",
    "            df = df.drop(columns=['VPN_City', 'Tekst'])\n",
    "            df['Booked'] = df['Booked'].astype('str')\n",
    "            df['Przecena'] = df['Przecena'].astype('str')\n",
    "            df['Cena'] = df['Cena'].map(lambda x: x.lower().split('from')[-1] if 'from' in x.lower() else x)\n",
    "            df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "            df['Booked'] = df['Booked'].str.replace('New activity', 'nan')\n",
    "            df['Booked'] = df['Booked'].map(lambda x: x.split('Booked')[1].split()[0] if len(x) > 5 else x)\n",
    "            df['Przecena'] = df['Przecena'].map(lambda x: x.lower().split('per person')[0] if 'per person' in x.lower() else x)\n",
    "            df['Przecena'] = df['Przecena'].str.replace(r'[$€£]', '', regex=True).str.strip()\n",
    "            df['Przecena'] = df['Przecena'].map(lambda x: x.split()[0] if len(x) > 4 else x)\n",
    "            df['Przecena'] = df['Przecena'].fillna(\"NULL\")\n",
    "            #     df['VPN_City'].fillna(\"NULL\", inplace= True)\n",
    "            df['Booked'] = df['Booked'].fillna(\"NULL\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    # Create a connection to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name_refined)\n",
    "\n",
    "    ## Upload the modified Excel file to Azure Blob Storage\n",
    "    with open(output_file_path, \"rb\") as data:\n",
    "        container_client.upload_blob(name=blob_name, data=data)\n",
    "        \n",
    "    print(\"File uploaded successfully to Azure Blob Storage (refined).\")\n",
    "    os.remove(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4463c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_driver() -> WebDriver:\n",
    "    try:\n",
    "        logger_info.info(\"Initializing the Chrome driver and logging into the website\")\n",
    "\n",
    "        # Setting up Chrome options\n",
    "        options = webdriver.ChromeOptions()\n",
    "        # options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "\n",
    "        # Initialize the Chrome driver\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "        \n",
    "        return driver\n",
    "\n",
    "    except Exception as e:\n",
    "        logger_err.error(f\"An error occurred during login: {e}\")\n",
    "        raise\n",
    "    \n",
    "def quit_driver(driver: WebDriver) -> None:\n",
    "    driver.quit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9af992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def daily_run_gyg(df_links=pd.DataFrame(), re_run=False):\n",
    "    global date_today\n",
    "    global output_gyg\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "#     link_file = fr'resource/GYG_links.csv'\n",
    "# Check if the run is re-run to colelct city or not\n",
    "\n",
    "# DEBUG MODE:\n",
    "#     df_links=pd.DataFrame()\n",
    "#     re_run=False\n",
    "    if len(df_links) == 0:\n",
    "        df_links = pd.read_csv(link_file)\n",
    "    # t_url = df_links.iloc[15]['URL']\n",
    "    # city = df_links.iloc[15]['City']\n",
    "    # category = df_links.iloc[15]['Category']\n",
    "    # t_url   \n",
    "    # df_links = df_links.tail(95)\n",
    "    EUR_City = [\n",
    "        \"Amsterdam\", \"Athens\", \"Barcelona\", \"Berlin\", \"Dublin\", \"Dubrovnik\", \"Florence\", \"Istanbul\",\n",
    "        \"Krakow\", \"Lisbon\", \"Madrid\", \"Milan\", \"Naples\", \"Paris\", \"Porto\", \"Rome\", \"Palermo\", \"Venice\",\n",
    "        \"Taormina\", \"Capri\", \"Sorrento\", \"Mount-Etna\", \"Mount-Vesuvius\", \"Herculaneum\", \"Amalfi-Coast\",\n",
    "        \"Pompeii\"\n",
    "    ]\n",
    "\n",
    "    USD_City = [\n",
    "        \"Las-Vegas\", \"New-York-City\", \"Cancun\", \"Dubai\"\n",
    "    ]\n",
    "\n",
    "    GBP_City = [\n",
    "        \"Edinburgh\", \"London\"\n",
    "    ]\n",
    "#     date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_path_done =fr'output/GYG/{date_today}-DONE-GYG.csv'  \n",
    "#     file_path_output = fr\"output/GYG - {date_today}.xlsx\"\n",
    "    if os.path.exists(file_path_output) and re_run == False:\n",
    "        print(f'Today ({date_today}) GYG done')\n",
    "        return 'Done'\n",
    "\n",
    "    if os.path.exists(file_path_done) and re_run == False:\n",
    "        done_msg = pd.read_csv(file_path_done)\n",
    "        done_msg = done_msg.transpose()\n",
    "        done_msg = done_msg.set_axis(done_msg.iloc[0], axis=1)\n",
    "        done_msg = done_msg.iloc[1:]\n",
    "        done_index = int(done_msg.index.values[0])\n",
    "        df_links = df_links.iloc[(done_index+1):]\n",
    "    elif re_run == True:\n",
    "        print(f'Lenght of links: {len(df_links)}')\n",
    "    else:\n",
    "        print(\"Nothing done yet\")\n",
    "        \n",
    "    \n",
    "#     df_links = df_links[df_links['WhatIsIt'] != 'Category']\n",
    "    df_links = df_links[df_links['Run'] == 1]\n",
    "#     display(df_links)\n",
    "    driver = initilize_driver()\n",
    "    # Define the URL of the website we want to scrape\n",
    "    start_time = time.time()\n",
    "    total_pages = 1\n",
    "    iter = 0\n",
    "    for index, row in df_links.iterrows():\n",
    "        \n",
    "    #     CHECK IF FILE PATH EXISIT IF SO CHECK THE DATA INSIDE\n",
    "#         print(index, row)\n",
    "        page = 1\n",
    "        max_pages = 9999\n",
    "        data = []\n",
    "        position = 0\n",
    "        url_time = time.time()\n",
    "        while page <= max_pages:\n",
    "            if iter % 25 == 0:\n",
    "                driver.quit()\n",
    "                driver = initilize_driver()\n",
    "\n",
    "            iter +=1\n",
    "            url = f'{row[\"URL\"]}&p={page}'\n",
    "            print(url)\n",
    "            if max_pages == 9999:\n",
    "                max_pages = 'Set'\n",
    "        \n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                title_webpage = driver.title\n",
    "                current_url = driver.current_url\n",
    "                print(f'Title: {title_webpage} \\n\\n CURRENT URL: {current_url}')\n",
    "            except WebDriverException:\n",
    "                # If an exception occurs, it might indicate that the page is unresponsive\n",
    "                print(\"The page might be unresponsive (possibly 'Aw, Snap!'). Attempting to refresh...\")\n",
    "                try:\n",
    "                    driver.refresh()\n",
    "                    time.sleep(1)  # Wait for the page to load after refresh\n",
    "                except WebDriverException:\n",
    "                    driver.quit()\n",
    "                    print(\"Failed to refresh the page. Consider checking your setup or the website status.\")\n",
    "                    driver.get(url=url)\n",
    "                    print('Closed and opens once again the webpage')\n",
    "                    time.sleep(4)\n",
    "\n",
    "            \n",
    "            try:\n",
    "                button_currency = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'select[id=\"footer-currency-selector\"]'))\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to click the element: {e}\")\n",
    "\n",
    "            # Create a Select object for the dropdown\n",
    "\n",
    "            select = Select(button_currency)\n",
    "            # Get the currently selected option\n",
    "            selected_option = select.first_selected_option\n",
    "            # Select by visible text for Euro, GBP, or USD\n",
    "            \n",
    "            currency =  selected_option.text.strip()\n",
    "            if row['City'] in EUR_City:\n",
    "                if 'EUR' in currency:\n",
    "                    pass\n",
    "                else:\n",
    "                    select.select_by_visible_text('Euro (€)')\n",
    "                    time.sleep(2)\n",
    "            elif row['City'] in USD_City:\n",
    "                if 'USD' in currency:\n",
    "                    pass\n",
    "                else:\n",
    "                    select.select_by_visible_text('U.S. Dollar ($)')\n",
    "                    time.sleep(2)\n",
    "            elif row['City'] in GBP_City:\n",
    "                if 'GBP' in currency:\n",
    "                    pass\n",
    "                else:\n",
    "                    select.select_by_visible_text('British Pound (£)')\n",
    "                    time.sleep(2)\n",
    "            else:\n",
    "#                 pass\n",
    "                print('Missing from the list:', row['City'])\n",
    "\n",
    "            # Parse the HTML content of the page using Beautiful Soup\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            if max_pages == 'Set':\n",
    "                try:\n",
    "                    max_pages = int((soup.find('span', {'class': 'trip-item-pagination__controls-info'}).text.strip()).split(' ')[-1])        \n",
    "                except:\n",
    "                    try:\n",
    "                        max_pages = round(float(soup.find('div', {'class': 'search-header__left__data-wrapper__count'}).text.strip().split()[0])/40,0)+1\n",
    "                        print('Divided by amount of activiytes: ', max_pages)\n",
    "                    except:\n",
    "                        max_pages = 5\n",
    "                        print('Dindt found max page - new UI')\n",
    "                total_pages = total_pages+max_pages\n",
    "# #############################################                \n",
    "#                 max_pages = 1\n",
    "# #############################################\n",
    "\n",
    "            # Extract the data from the HTML using Beautiful Soup\n",
    "            tour_items = soup.find_all('li', {'class': 'list-element'})\n",
    "            if len(tour_items) == 0:\n",
    "                tour_items = soup.select(\"[data-test-id=vertical-activity-card]\")\n",
    "            # print(tour_items)\n",
    "            date_today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "            for tour_item in tour_items:\n",
    "                title = tour_item.find('h3', {'class': 'vertical-activity-card__title'}).text.strip()\n",
    "                price = tour_item.find('div', {'class': 'activity-price'}).text.strip()\n",
    "#                 product_category = tour_item.find('span', {'class': 'vertical-activity-card__activity-type c-classifier-badge'}).text.strip()\n",
    "                product_url = f\"https://www.getyourguide.com/{tour_item.find('a')['href']}\"\n",
    "                product_url = product_url.split('?ranking_uuid')[0]\n",
    "                try:\n",
    "                    position = int(tour_item['key']) + 1 + (page - 1) * 16\n",
    "                except:\n",
    "                    position = position + 1\n",
    "                siteuse = 'GYG'\n",
    "                city = row['City']\n",
    "                category = row['RawCategory']\n",
    "                try:\n",
    "                    discount = tour_item.find('div', {'class': 'baseline-pricing__value baseline-pricing__value--low'}).text.strip()\n",
    "                except:\n",
    "                    discount = 'N/A'\n",
    "                try:\n",
    "                    amount_reviews = tour_item.find('div', {'class': 'rating-overall__reviews'}).text.strip()\n",
    "                except:\n",
    "                    try:\n",
    "                        amount_reviews = tour_item.find('div', {'class': 'c-activity-rating__label'}).text.strip()\n",
    "                    except:\n",
    "                        amount_reviews = 'N/A'\n",
    "                try:\n",
    "                    stars = tour_item.find('span', {'rating-overall__rating-number rating-overall__rating-number--right'}).text.strip()\n",
    "                except:\n",
    "                    try:\n",
    "                        stars = tour_item.find('span', {'c-activity-rating__rating'}).text.strip()\n",
    "                    except:\n",
    "                        stars = 'N/A'\n",
    "                try:\n",
    "                    booked = tour_item.find('span', {'class': 'c-marketplace-badge c-marketplace-badge--secondary'}).text.strip()\n",
    "                except:\n",
    "                    booked = 'N/A'\n",
    "                try:\n",
    "                    new_activity = tour_item.find('span', {'class': 'activity-info__badge c-marketplace-badge c-marketplace-badge--secondary'}).text.strip()\n",
    "                except:\n",
    "                    new_activity = 'N/A'\n",
    "\n",
    "                text = tour_item.text.strip()\n",
    "\n",
    "                data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, booked, siteuse, city ])\n",
    "\n",
    "\n",
    "            page += 1\n",
    "        url_done = time.time()\n",
    "        message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page'\n",
    "        print(message)\n",
    "        logger_info.info(message)\n",
    "        df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'Booked', 'SiteUse', 'Miasto'])\n",
    "        df['Cena'] = df['Cena'].map(lambda x: x.split(' ')[-1])\n",
    "        df['Przecena'] = df['Przecena'].map(lambda x: x.split('From')[1] if x != 'N/A' else 'N/A')\n",
    "        df['IloscOpini'] = df['IloscOpini'].map(lambda x: x.split('(')[-1].split(')')[0].split(' ')[0].replace(',', '') if x != 'N/A' else x)\n",
    "        df['VPN_City'] = ''\n",
    "        with open(max_page_file, 'a', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "\n",
    "            # Append the data\n",
    "            csvwriter.writerow([city, category, max_pages, date_today])\n",
    "            \n",
    "        file_path = fr'{output_gyg}/{date_today}-{city}-GYG.csv' \n",
    "        df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')\n",
    "        row.to_csv(file_path_done, header=True, index=True)    \n",
    "    driver.quit()\n",
    "    end_time = time.time()\n",
    "    message_done = f'Done {len(df_links)} URLs in {round((end_time - start_time)/60,2)} mins | Pages: {total_pages} | AVG: {round((end_time - start_time)/total_pages, 2)}s'\n",
    "    # print(message_done)\n",
    "\n",
    "    logger_done.info(message_done)\n",
    "    if re_run == False:\n",
    "        combine_csv_to_xlsx()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b91ff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel file which will be checked against avg values\n",
    "def check_amount_data():\n",
    "    global date_today\n",
    "    global output_gyg\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "#     date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#     xls = pd.ExcelFile(fr\"output/GYG - 2023-05-27.xlsx\")\n",
    "    xls = pd.ExcelFile(fr\"{output_gyg}/GYG - {date_today}.xlsx\")\n",
    "#     link_file = fr'resource/GYG_links.csv'\n",
    "#     avg_file = fr'resource/avg-gyg.csv'\n",
    "#     re_run_path = fr'output/GYG/{date_today} - ReRun GYG.csv'\n",
    "    df_links = pd.read_csv(link_file)\n",
    "    df_avg = pd.read_csv(avg_file)\n",
    "    re_run_data = []\n",
    "\n",
    "    city_to_get_data = df_links['City'].drop_duplicates().tolist()\n",
    "    for excel_sheet_name in city_to_get_data:\n",
    "    #     Check if the all excel files which are in df_links are available in created excel file\n",
    "        if excel_sheet_name in xls.sheet_names:\n",
    "    #         Data collected it's loaded excel file for selected city\n",
    "            data_collected = xls.parse(sheet_name=excel_sheet_name)\n",
    "            amount_of_data_collected = len(data_collected)\n",
    "    #         print(excel_sheet_name, amount_of_data_collected)\n",
    "            avg_value_city = int(df_avg[df_avg['City'] == excel_sheet_name]['Avg'])\n",
    "            if abs(amount_of_data_collected - avg_value_city)/avg_value_city > 0.15 :\n",
    "                if amount_of_data_collected < avg_value_city:\n",
    "#                     print(abs(amount_of_data_collected - avg_value_city), excel_sheet_name, amount_of_data_collected, avg_value_city)\n",
    "                    logger_done.info(abs(amount_of_data_collected - avg_value_city), excel_sheet_name, amount_of_data_collected, avg_value_city)\n",
    "                category_to_get = df_links[(df_links['City'] == excel_sheet_name) & (df_links['WhatIsIt'] == 'Category')]['RawCategory'].tolist()\n",
    "                category_collected = data_collected['Kategoria'].drop_duplicates().tolist()\n",
    "    #             display(data_collected.groupby('Kategoria')['Kategoria'].count())\n",
    "                for category_name in category_to_get:\n",
    "                    if category_name in category_collected:\n",
    "                        pass\n",
    "                    else:\n",
    "    #                     If the category is missing in the excel sheet add it to re-run data\n",
    "                        print(f'Missing {category_name} for {excel_sheet_name}')\n",
    "                        re_run_data.append([excel_sheet_name, category_name])\n",
    "#                 FOR TESTING\n",
    "#                 re_run_data.append([excel_sheet_name, category_name])\n",
    "#                 re_run_data.append([excel_sheet_name, 'all'])\n",
    "    #     If the excel sheet is missing add it to re-run data\n",
    "        else:\n",
    "            print(f'Missing {excel_sheet_name} in data')\n",
    "            re_run_data.append([excel_sheet_name, 'all'])\n",
    "    if len(re_run_data) > 0:\n",
    "        pd.DataFrame(re_run_data).to_csv(re_run_path, index=False, header=['City', 'Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed75d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_avg_data_required():\n",
    "    global date_today\n",
    "    global output_gyg\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    # COUNT AVG PER CITY \n",
    "    # Initialize variables\n",
    "    city_counts = []\n",
    "    total_rows = 0\n",
    "    result = []\n",
    "    # Iterate over each text file in the directory\n",
    "    for file_name in os.listdir(folder_path_with_txt_to_count_avg):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path_with_txt_to_count_avg, file_name)\n",
    "\n",
    "            # Open the text file\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Extract the city name using regular expressions\n",
    "                city_list = re.findall(r'\\d+ - ([^\\n]+).', content)\n",
    "                count_list = re.findall(r'\\d+ rows', content)\n",
    "\n",
    "                for item1, item2 in zip(city_list, count_list):\n",
    "                    joined = str(item1) + ' ' + str(item2.split(' ')[0])\n",
    "                    result.append(joined)\n",
    "\n",
    "                for row in result:\n",
    "                    city = row.split(' ')[0]\n",
    "\n",
    "                    # Extract the row count using regular expressions\n",
    "                    count_match = row.split(' ')[1]\n",
    "                    count = int(count_match)\n",
    "                    # Add the city and row count to the list\n",
    "                    city_counts.append((city, count))\n",
    "\n",
    "                    # Update the total row count\n",
    "                    total_rows += count\n",
    "    city_population = {}\n",
    "\n",
    "    # Store population values for each city\n",
    "    for city, row_count in city_counts:\n",
    "        if city in city_population:\n",
    "            city_population[city].append(row_count)\n",
    "        else:\n",
    "            city_population[city] = [row_count]\n",
    "\n",
    "    # Calculate average population for each city\n",
    "    city_avg = {}\n",
    "    for city, population_list in city_population.items():\n",
    "        city_avg[city] = round(sum(population_list) / len(population_list),0)\n",
    "\n",
    "    # Print average population for each city\n",
    "    #     report_str+= f\"{city} - {round(avg, 0)}\"\n",
    "    avg_path_viator = 'resource/avg-gyg.csv'\n",
    "    # with open(avg_path_viator, \"w\") as f:\n",
    "    #                 f.write(report_str)\n",
    "    df = pd.DataFrame(city_avg.items(), columns=['City', 'Avg'])\n",
    "    df.to_csv(avg_path_viator, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d0835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FOR RE-RUN PREPARATION\n",
    "def re_run_daily():\n",
    "    global re_run_path\n",
    "    global link_file\n",
    "    global archive_folder\n",
    "#     re_run_path = fr'output/GYG/2023-05-31-ReRun-GYG.csv'\n",
    "    if os.path.exists(re_run_path):\n",
    "        df_re_run = pd.read_csv(re_run_path)\n",
    "        df_links = pd.read_csv(link_file)\n",
    "        df_links = df_links[df_links['WhatIsIt'] == 'Category']\n",
    "        mergded_df_re_run = pd.merge(df_links,df_re_run, how='right', on=('City'))\n",
    "\n",
    "        for index, row in mergded_df_re_run.iterrows():\n",
    "            if row['Category_y'] == 'all':\n",
    "                continue\n",
    "            if row['Category_y'] != row['RawCategory']:\n",
    "                mergded_df_re_run.drop(index=index, inplace=True)\n",
    "\n",
    "        daily_run_gyg(mergded_df_re_run, True)\n",
    "    else:\n",
    "        print('No missing categories or cities')\n",
    "\n",
    "    \n",
    "#     NOT DONE DATA IS NOT BEING INSERTED TO EXCEL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccfceb1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 10:42:07,824 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-30 10:42:11,204 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.getyourguide.com/s?q=Amsterdam&activity_type=transfer&p=1\n",
      "Title: Book Things To Do, Attractions, and Tours | GetYourGuide \n",
      "\n",
      " CURRENT URL: https://www.getyourguide.com/s?q=Amsterdam&activity_type=transfer&p=1\n",
      "Divided by amount of activiytes:  3.0\n",
      "https://www.getyourguide.com/s?q=Amsterdam&activity_type=transfer&p=2\n",
      "Title: Book Things To Do, Attractions, and Tours | GetYourGuide \n",
      "\n",
      " CURRENT URL: https://www.getyourguide.com/s?q=Amsterdam&activity_type=transfer&p=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 10:42:24,619 - Error_logger - ERROR - An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=129.0.6668.60)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF72088B125+29573]\n",
      "\t(No symbol) [0x00007FF7207FFF50]\n",
      "\t(No symbol) [0x00007FF7206BB6EA]\n",
      "\t(No symbol) [0x00007FF72068FCD5]\n",
      "\t(No symbol) [0x00007FF72073EF67]\n",
      "\t(No symbol) [0x00007FF720757FC1]\n",
      "\t(No symbol) [0x00007FF7207370A3]\n",
      "\t(No symbol) [0x00007FF7207012DF]\n",
      "\t(No symbol) [0x00007FF720702441]\n",
      "\tGetHandleVerifier [0x00007FF720BBC76D+3377613]\n",
      "\tGetHandleVerifier [0x00007FF720C07B67+3685831]\n",
      "\tGetHandleVerifier [0x00007FF720BFCF8B+3641835]\n",
      "\tGetHandleVerifier [0x00007FF72094B2A6+816390]\n",
      "\t(No symbol) [0x00007FF72080B25F]\n",
      "\t(No symbol) [0x00007FF720807084]\n",
      "\t(No symbol) [0x00007FF720807220]\n",
      "\t(No symbol) [0x00007FF7207F607F]\n",
      "\tBaseThreadInitThunk [0x00007FF91039257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF91144AF08+40]\n",
      " on Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_12036\\129008512.py\", line 3, in <module>\n",
      "    gyg_day = daily_run_gyg()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_12036\\3795349016.py\", line 144, in daily_run_gyg\n",
      "    html = driver.page_source\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 448, in page_source\n",
      "    return self.execute(Command.GET_PAGE_SOURCE)[\"value\"]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 347, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 229, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=129.0.6668.60)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF72088B125+29573]\n",
      "\t(No symbol) [0x00007FF7207FFF50]\n",
      "\t(No symbol) [0x00007FF7206BB6EA]\n",
      "\t(No symbol) [0x00007FF72068FCD5]\n",
      "\t(No symbol) [0x00007FF72073EF67]\n",
      "\t(No symbol) [0x00007FF720757FC1]\n",
      "\t(No symbol) [0x00007FF7207370A3]\n",
      "\t(No symbol) [0x00007FF7207012DF]\n",
      "\t(No symbol) [0x00007FF720702441]\n",
      "\tGetHandleVerifier [0x00007FF720BBC76D+3377613]\n",
      "\tGetHandleVerifier [0x00007FF720C07B67+3685831]\n",
      "\tGetHandleVerifier [0x00007FF720BFCF8B+3641835]\n",
      "\tGetHandleVerifier [0x00007FF72094B2A6+816390]\n",
      "\t(No symbol) [0x00007FF72080B25F]\n",
      "\t(No symbol) [0x00007FF720807084]\n",
      "\t(No symbol) [0x00007FF720807220]\n",
      "\t(No symbol) [0x00007FF7207F607F]\n",
      "\tBaseThreadInitThunk [0x00007FF91039257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF91144AF08+40]\n",
      "\n",
      "\n",
      "Exception ignored in: <function Service.__del__ at 0x000001EC8AC1C680>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\common\\service.py\", line 189, in __del__\n",
      "    self.stop()\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\common\\service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\common\\service.py\", line 131, in send_remote_shutdown_command\n",
      "    if not self.is_connectable():\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\common\\service.py\", line 120, in is_connectable\n",
      "    return utils.is_connectable(self.port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\common\\utils.py\", line 101, in is_connectable\n",
      "    socket_ = socket.create_connection((host, port), 1)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\socket.py\", line 843, in create_connection\n",
      "    exceptions.clear()  # raise only the last error\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "2024-09-30 10:42:25,673 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-30 10:42:29,044 - Info_logger - INFO - Initializing the Chrome driver and logging into the website\n",
      "2024-09-30 10:42:34,337 - Error_logger - ERROR - An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=129.0.6668.60)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF72088B125+29573]\n",
      "\t(No symbol) [0x00007FF7207FFF50]\n",
      "\t(No symbol) [0x00007FF7206BB6EA]\n",
      "\t(No symbol) [0x00007FF72068FCD5]\n",
      "\t(No symbol) [0x00007FF72073EF67]\n",
      "\t(No symbol) [0x00007FF720757FC1]\n",
      "\t(No symbol) [0x00007FF7207370A3]\n",
      "\t(No symbol) [0x00007FF7207012DF]\n",
      "\t(No symbol) [0x00007FF720702441]\n",
      "\tGetHandleVerifier [0x00007FF720BBC76D+3377613]\n",
      "\tGetHandleVerifier [0x00007FF720C07B67+3685831]\n",
      "\tGetHandleVerifier [0x00007FF720BFCF8B+3641835]\n",
      "\tGetHandleVerifier [0x00007FF72094B2A6+816390]\n",
      "\t(No symbol) [0x00007FF72080B25F]\n",
      "\t(No symbol) [0x00007FF720807084]\n",
      "\t(No symbol) [0x00007FF720807220]\n",
      "\t(No symbol) [0x00007FF7207F607F]\n",
      "\tBaseThreadInitThunk [0x00007FF91039257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF91144AF08+40]\n",
      " on Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_12036\\129008512.py\", line 3, in <module>\n",
      "    gyg_day = daily_run_gyg()\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_12036\\3795349016.py\", line 84, in daily_run_gyg\n",
      "    driver.get(url)\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 356, in get\n",
      "    self.execute(Command.GET, {\"url\": url})\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 347, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"C:\\Users\\Wojciech\\AppData\\Roaming\\Python\\Python311\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 229, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.NoSuchWindowException: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=129.0.6668.60)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF72088B125+29573]\n",
      "\t(No symbol) [0x00007FF7207FFF50]\n",
      "\t(No symbol) [0x00007FF7206BB6EA]\n",
      "\t(No symbol) [0x00007FF72068FCD5]\n",
      "\t(No symbol) [0x00007FF72073EF67]\n",
      "\t(No symbol) [0x00007FF720757FC1]\n",
      "\t(No symbol) [0x00007FF7207370A3]\n",
      "\t(No symbol) [0x00007FF7207012DF]\n",
      "\t(No symbol) [0x00007FF720702441]\n",
      "\tGetHandleVerifier [0x00007FF720BBC76D+3377613]\n",
      "\tGetHandleVerifier [0x00007FF720C07B67+3685831]\n",
      "\tGetHandleVerifier [0x00007FF720BFCF8B+3641835]\n",
      "\tGetHandleVerifier [0x00007FF72094B2A6+816390]\n",
      "\t(No symbol) [0x00007FF72080B25F]\n",
      "\t(No symbol) [0x00007FF720807084]\n",
      "\t(No symbol) [0x00007FF720807220]\n",
      "\t(No symbol) [0x00007FF7207F607F]\n",
      "\tBaseThreadInitThunk [0x00007FF91039257D+29]\n",
      "\tRtlUserThreadStart [0x00007FF91144AF08+40]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.getyourguide.com/s?q=Amsterdam&activity_type=transfer&p=1\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        gyg_day = daily_run_gyg()\n",
    "        if gyg_day == 'Done':\n",
    "            break\n",
    "        else:\n",
    "            print('re-run not done yet')\n",
    "    except Exception as e:\n",
    "        handle_error_and_rerun(e)\n",
    "\n",
    "# After sucessfull run check amount of data in Excel file if the data is missing collect missing city and/or categories\n",
    "# check_amount_data()\n",
    "# re_run_daily()\n",
    "\n",
    "# Call the function to upload the file to Azure Blob Storage\n",
    "try:\n",
    "    upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name)\n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)\n",
    "\n",
    "try:\n",
    "    transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)    \n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "683d52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'backup' in os.getcwd():\n",
    "    importlib.reload(Azure_stopVM)\n",
    "    script_name = 'Viator_daily.py'\n",
    "\n",
    "    check_if_viator_running = Azure_stopVM.check_if_script_is_running(script_name)\n",
    "    if check_if_viator_running:\n",
    "        logger_done.info(f\"{script_name} is currently running.\")\n",
    "    else:\n",
    "        logger_done.info(f\"{script_name} is not running. Stoping VM\")\n",
    "        Azure_stopVM.stop_vm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fdaee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##################DEBUG CURRENCY SWITCHER\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "# driver.maximize_window()\n",
    "# # Define the URL of the website we want to scrape\n",
    "# start_time = time.time()\n",
    "# total_pages = 0\n",
    "# #     CHECK IF FILE PATH EXISIT IF SO CHECK THE DATA INSIDE\n",
    "# #         print(index, row)\n",
    "# page = 1\n",
    "# max_pages = 9999\n",
    "# data = []\n",
    "# position = 0\n",
    "# url_time = time.time()\n",
    "\n",
    "# url = f'https://www.getyourguide.com/s?q=Amsterdam&p=1'\n",
    "\n",
    "# driver.get(url)\n",
    "# time.sleep(1)\n",
    "# #     VERIFY IF THE CURRENCY IS CORRECT\n",
    "# login_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@title='Profile']\")))\n",
    "# login_button.click()\n",
    "# # currency = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@title='Select Currency']\")))\n",
    "# currency = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='option option-currency']\")))\n",
    "# currency\n",
    "# html = driver.page_source\n",
    "# soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2841482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currency_switcher_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='option option-currency']\")))\n",
    "# # hover over the currency switcher button to show the menu\n",
    "# actions = ActionChains(driver)\n",
    "# actions.move_to_element(currency_switcher_button).perform()\n",
    "# currency_switcher_button .click()\n",
    "# # wait for the EUR currency option to be clickable\n",
    "# eur_currency_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//li[@class='currency-modal-picker__item-parent item__currency-modal item__currency-modal--EUR']\")))\n",
    "# # click on the EUR currency option to change the currency\n",
    "# eur_currency_option.click()\n",
    "\n",
    "# html = driver.page_source\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# tour_items = soup.select(\"[data-test-id=vertical-activity-card]\")\n",
    "# len(tour_items)\n",
    "# title = tour_items[0].find('p', {'class': 'vertical-activity-card__title'}).text.strip()\n",
    "# price = tour_items[0].find('div', {'class': 'baseline-pricing__value'}).text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
