{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153cdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import logging.handlers\n",
    "import traceback\n",
    "import re\n",
    "from threading import Lock, current_thread\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb46920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "\n",
    "# date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# output_viator = r'output/Viator'\n",
    "# archive_folder = fr'{output_viator}/Archive'\n",
    "# file_path_done =fr'output/Viator/{date_today}-DONE-Viator.csv'  \n",
    "# file_path_output = fr\"output/Viator - {date_today}.xlsx\"\n",
    "# link_file = fr'resource/Viator_links.csv'\n",
    "# avg_file = fr'resource/avg-Viator.csv'\n",
    "# re_run_path = fr'output/Viator/{date_today}-ReRun-Viator.csv'\n",
    "# folder_path_with_txt_to_count_avg = 'Avg/Viator'\n",
    "# logs_path = fr'Logs/Viator'\n",
    "\n",
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2024-05-30'\n",
    "output_viator = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily'\n",
    "archive_folder = fr'{output_viator}/Archive'\n",
    "file_path_done =fr'{output_viator}/{date_today}-DONE-Viator.csv'  \n",
    "file_path_output = fr\"{output_viator}/Viator - {date_today}.xlsx\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "max_page_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_max_page.csv'\n",
    "avg_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/avg-viator.csv'\n",
    "re_run_path = fr'{output_viator}/{date_today}-ReRun-Viator.csv'\n",
    "logs_path = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/Viator'\n",
    "# FOR ONE TIME USED NOT SYNCHORNIEZD WITH RUNING APPLCIATION\n",
    "folder_path_with_txt_to_count_avg = 'Avg/Viator'\n",
    "\n",
    "# Set the path of the local file\n",
    "local_file_path = f\"{output_viator}/Viator - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/daily/viator\"\n",
    "container_name_refined = \"refined/daily/viator\"\n",
    "\n",
    "blob_name = fr'Viator - {date_today}.xlsx'\n",
    "file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'ARS\\xa0': 'ARS (Argentine Peso)', 'ARS': 'ARS (Argentine Peso)',\n",
    "                    'A$': 'AUD (Australian Dollar)', 'SGD': 'SGD (Singapur Dolar)'}\n",
    "\n",
    "currency_list = []\n",
    "API_KEY_SCRAPERAPI = '8c36bc42cd11c738c1baad3e2000b40c' # https://dashboard.scraperapi.com/\n",
    "API_KEY_ZENROWS = '56ed5b7f827aa5c258b3f6d3f57d36999aa949e8' # https://app.zenrows.com/buildera\n",
    "file_write_lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0880b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_logs_path = os.path.join(logs_path, 'archive_logs')\n",
    "if not os.path.exists(archive_logs_path):\n",
    "    os.makedirs(archive_logs_path)\n",
    "# Function to get the log file name based on the current month\n",
    "def get_log_file_name(base_file_name):\n",
    "    current_month = time.strftime(\"%Y%m\")\n",
    "    return f\"{current_month}_{base_file_name}.log\"\n",
    "# Create a function to rotate the logs\n",
    "def rotate_logs(handler, logger):\n",
    "    current_month = time.strftime(\"%Y%m\")\n",
    "    if not handler.baseFilename.endswith(current_month + \".log\"):\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "        handler.baseFilename = os.path.join(logs_path, get_log_file_name(handler.baseFilename))\n",
    "        logger.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62665a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger objects\n",
    "logger_err = logging.getLogger('Error_logger')\n",
    "logger_info = logging.getLogger('Info_logger')\n",
    "logger_done = logging.getLogger('Done_logger')\n",
    "\n",
    "# set loggers' level\n",
    "logger_err.setLevel(logging.DEBUG)\n",
    "logger_info.setLevel(logging.DEBUG)\n",
    "logger_done.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to console handler\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# Add console handler to loggers\n",
    "logger_err.addHandler(ch)\n",
    "logger_info.addHandler(ch)\n",
    "logger_done.addHandler(ch)\n",
    "\n",
    "# Create TimedRotatingFileHandlers for each logger\n",
    "fh_error = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename=os.path.join(logs_path, get_log_file_name('error_logs')), \n",
    "    when='M', \n",
    "    interval=1, \n",
    "    backupCount=24\n",
    ")\n",
    "fh_info = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename=os.path.join(logs_path, get_log_file_name('info_logs')), \n",
    "    when='M', \n",
    "    interval=1, \n",
    "    backupCount=24\n",
    ")\n",
    "fh_done = logging.handlers.TimedRotatingFileHandler(\n",
    "    filename=os.path.join(logs_path, get_log_file_name('done_logs')), \n",
    "    when='M', \n",
    "    interval=1, \n",
    "    backupCount=24\n",
    ")\n",
    "\n",
    "# Set level for file handlers\n",
    "fh_error.setLevel(logging.DEBUG)\n",
    "fh_info.setLevel(logging.INFO)\n",
    "fh_done.setLevel(logging.INFO)\n",
    "\n",
    "# Add formatter to file handlers\n",
    "fh_error.setFormatter(formatter)\n",
    "fh_info.setFormatter(formatter)\n",
    "fh_done.setFormatter(formatter)\n",
    "\n",
    "# Add file handlers to loggers\n",
    "logger_err.addHandler(fh_error)\n",
    "logger_info.addHandler(fh_info)\n",
    "logger_done.addHandler(fh_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63807628",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "    ''\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c25470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error_and_rerun(error):\n",
    "#     recipient_error = 'wojbal3@gmail.com'\n",
    "    tb = traceback.format_exc()\n",
    "    logger_err.error('An error occurred: {} on {}'.format(str(error), tb))\n",
    "#     subject = f'Error occurred - {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}'\n",
    "#     message = f'<html><body><p>Error occurred: {str(error)} on {tb}</p></body></html>'\n",
    "#     send_email(subject, message, recipient_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c377fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rates(of_date, currency_code='EUR'):\n",
    "# USING API TO GET RATES FROM SITE https://fixer.io/documentation\n",
    "    res = requests.get(fr'http://data.fixer.io/api/{of_date}?access_key=acfed48df1159d37fa4305e5e95c234f&base={currency_code}')\n",
    "    rates = res.json()['rates']\n",
    "    return rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcdd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_to_xlsx():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    # Get all CSV files with the specified date prefix\n",
    "    csv_files = [file for file in os.listdir(f'{output_viator}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found with the date prefix '{date_today}'\")\n",
    "        return\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    output_file = f\"{output_viator}/Viator - {date_today}.xlsx\"\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_viator}', csv_file)\n",
    "        if 'Viator' not in csv_file:\n",
    "            continue\n",
    "        sheet_name = os.path.splitext(csv_file)[0]\n",
    "        sheet_name = sheet_name.split(date_today + '-')[1].split('-Viator')[0]\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Write the DataFrame to the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save the Excel file\n",
    "    # writer.save()\n",
    "    writer.close()\n",
    "\n",
    "    print(f\"Combined CSV files with date prefix '{date_today}' into '{output_file}'\")\n",
    "\n",
    "    # Remove the CSV files\n",
    "#     for csv_file in csv_files:\n",
    "#         os.remove(csv_file)\n",
    "    # Move the CSV files to the Archive folder\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_viator}', csv_file)\n",
    "        if 'DONE' in csv_file:\n",
    "            df_done = pd.read_csv(csv_path)\n",
    "            df_done = df_done.drop_duplicates(subset=['City', 'Category'])\n",
    "            df_done = df_done.drop(columns=['UrlRequest', 'UrlResponse', 'Status', 'Page'])\n",
    "            df_done['Date'] = date_today\n",
    "            df_done.to_csv(max_page_file, mode='a', index=False, header=False)\n",
    "        destination_path = os.path.join(archive_folder, csv_file)\n",
    "        shutil.move(csv_path, destination_path)\n",
    "        \n",
    "\n",
    "    print(f\"Moved {len(csv_files)} CSV file(s) to the '{archive_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1717f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_done(log_type):\n",
    "    global file_path_logs_processed\n",
    "    if log_type == 'Raw':\n",
    "        with open(f'{file_path_logs_processed}-raw.txt', 'w') as file:\n",
    "            file.write('Done')\n",
    "    elif log_type == 'Refined':\n",
    "        with open(f'{file_path_logs_processed}-refined.txt', 'w') as file:\n",
    "            file.write('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cbbd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name):\n",
    "    try:\n",
    "        # Create a connection string to the Azure Storage account\n",
    "        connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "        # Create a BlobServiceClient object using the connection string\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "        # Get a reference to the container\n",
    "        container_client = blob_service_client.get_container_client(container_name_raw)\n",
    "\n",
    "        # Upload the file to Azure Blob Storage\n",
    "        with open(local_file_path, \"rb\") as file:\n",
    "            container_client.upload_blob(name=blob_name, data=file, )\n",
    "        create_log_done('Raw')\n",
    "        print(\"File uploaded successfully to Azure Blob Storage (raw).\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730dfcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138e9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name):\n",
    "    global mapping_currency\n",
    "    global date_today\n",
    "    global currency_list\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "    # Define the Azure Blob Storage connection details\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    # Read the Excel file into a Pandas DataFrame\n",
    "    rates_eur = get_rates(date_today, 'EUR')\n",
    "    rates_gbp = get_rates(date_today, 'EUR')\n",
    "    rates_usd = rates_eur\n",
    "    currency_not_found_list = []\n",
    "    currny_not_found = False\n",
    "#     GBP AND USD ARE NOT SUPORTED WITHING THIS CURRENT SUBSRICPTION UPGRADE PLAN\n",
    "#     rates_gbp = get_rates(date_today, 'GBP')\n",
    "\n",
    "#     rates_usd = get_rates(date_today, 'USD')\n",
    "    excel_data = pd.read_excel(local_file_path, sheet_name=None)\n",
    "    output_file_path = \"temp_file.xlsx\"\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            if sheet_name in exclude_sheets:\n",
    "                continue\n",
    "            if sheet_name == 'Mt-Vesuvius':\n",
    "                sheet_name = 'Mount-Vesuvius'\n",
    "                df['Miasto'] = 'Mount-Vesuvius'\n",
    "            # Make changes to the df DataFrame as needed\n",
    "            df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "            df['IloscOpini'] = df['IloscOpini'].fillna(0) \n",
    "            df['Opinia'] = df['Opinia'].fillna('N/A')\n",
    "            df = df[df['Tytul'] != 'Tytul']\n",
    "            df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "            df = df[df['Data zestawienia'].str.len() > 4]\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace('\\\\\"', '', regex=True)\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace('\\\"', '', regex=True)\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace(r'\\\\', '', regex=True)\n",
    "            df['IloscOpini'] = df['IloscOpini'].astype(str).str.replace(',','')\n",
    "            df['Pozycja'] = df.groupby('Kategoria').cumcount() + 1\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                currency = ''\n",
    "                if 'per group' in row['Cena']:\n",
    "                    df.at[index, 'Cena'] = row['Cena'].split('per group')[0]\n",
    "                    row['Cena']= row['Cena'].split('per group')[0]\n",
    "                for i in range(0,10):\n",
    "                    if not row['Cena'][i].isnumeric():\n",
    "                        currency = currency + (row['Cena'][i])\n",
    "                    else:\n",
    "                        if row['Cena'][i] == '¹':\n",
    "                            currency = currency + (row['Cena'][i])\n",
    "                            continue\n",
    "                        price = float(row['Cena'][i:].split()[0].replace(',',''))\n",
    "                        total_price = row['Cena']\n",
    "                        break\n",
    "    #             print(currency)\n",
    "                if sheet_name in EUR_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(\"Currency mapping not found for: \",currency,\" in \", sheet_name)\n",
    "                        currny_not_found = True\n",
    "                        currency_not_found_list.append(currency)\n",
    "                elif sheet_name in GBP_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_gbp[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(\"Currency mapping not found for: \",currency,\" in \", sheet_name)\n",
    "                        currny_not_found = True\n",
    "                        currency_not_found_list.append(currency)\n",
    "                elif sheet_name in USD_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_usd[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(\"Currency mapping not found for: \",currency,\" in \", sheet_name)\n",
    "                        currny_not_found = True\n",
    "                        currency_not_found_list.append(currency)\n",
    "    #             print(f'{mapping_currency[currency[:3]][0:3]} conversion rate: {conversion_rate}')\n",
    "    #             print(f'{total_price}- price: {price} - covnersion: {price/(conversion_rate*1.020)}')\n",
    "                df.at[index, 'Cena'] = round(price/(conversion_rate*1.0185), 2)\n",
    "                currency_list.append(currency)\n",
    "\n",
    "            currency_list = list(set(currency_list))\n",
    "            if currny_not_found:\n",
    "                logger_done.info(currency_not_found_list)\n",
    "                print('Curreny not found: ', currency_not_found_list)\n",
    "    #         display(df)\n",
    "\n",
    "    #         df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "            df.drop(columns=['Przecena', 'Tekst'], inplace=True)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Create a connection to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name_refined)\n",
    "\n",
    "    # Upload the modified Excel file to Azure Blob Storage\n",
    "    with open(output_file_path, \"rb\") as data:\n",
    "        container_client.upload_blob(name=blob_name, data=data)\n",
    "        \n",
    "    print(\"File uploaded successfully to Azure Blob Storage (refined).\")\n",
    "    os.remove(output_file_path)\n",
    "    create_log_done('Refined')\n",
    "    return 'Added to Blob'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bff3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def currency_switcher(currency_code_text, driver):\n",
    "    wait_currency = WebDriverWait(driver,2) \n",
    "#     try:\n",
    "#         currency_code_element = wait_currency.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-automation*=\"EUR\"]')))\n",
    "#         # Click on the EUR element\n",
    "#         currency_code_element.click()\n",
    "#         time.sleep(5)\n",
    "#     except:\n",
    "#         try:\n",
    "#             driver.find_element(By.CSS_SELECTOR, 'div[class*=\"menuContent\"]').find_element(By.CSS_SELECTOR, 'button[data-automation=\"currency\"]').click()\n",
    "#         except:\n",
    "    driver.find_element(By.CSS_SELECTOR, 'div[class*=\"menu-container\"]').find_element(By.CSS_SELECTOR, '[data-action-page-properties*=\"currency\"]').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "#         currency_code = driver.find_elements(By.CSS_SELECTOR, '[data-automation=\"header-code\"]')\n",
    "#     if len(currency_code) == 0:\n",
    "    currency_code = driver.find_elements(By.CSS_SELECTOR, '[data-action-tag=\"select_currency_modal\"]')\n",
    "\n",
    "    for item in currency_code:\n",
    "            if currency_code_text in item.text:\n",
    "                item.click()\n",
    "                time.sleep(15)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aed3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_amount_data():\n",
    "    global date_today\n",
    "    global output_gyg\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    #     date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#     xls = pd.ExcelFile(fr\"{output_viator}/Viator - 2023-05-31.xlsx\")\n",
    "    xls = pd.ExcelFile(fr\"{output_viator}/Viator - {date_today}.xlsx\")\n",
    "    #     link_file = fr'resource/GYG_links.csv'\n",
    "    #     avg_file = fr'resource/avg-gyg.csv'\n",
    "    #     re_run_path = fr'output/GYG/{date_today} - ReRun GYG.csv'\n",
    "    df_links = pd.read_csv(link_file)\n",
    "    df_avg = pd.read_csv(avg_file)\n",
    "    re_run_data = []\n",
    "\n",
    "    city_to_get_data = df_links['City'].drop_duplicates().tolist()\n",
    "    for excel_sheet_name in city_to_get_data:\n",
    "    #     Check if the all excel files which are in df_links are available in created excel file\n",
    "        if excel_sheet_name in xls.sheet_names:\n",
    "    #         for viator to change \n",
    "\n",
    "    #         Data collected it's loaded excel file for selected city\n",
    "            data_collected = xls.parse(sheet_name=excel_sheet_name)\n",
    "            if excel_sheet_name == 'Mt-Vesuvius':\n",
    "                excel_sheet_name = 'Mount-Vesuvius'\n",
    "            amount_of_data_collected = len(data_collected)\n",
    "            print(excel_sheet_name, amount_of_data_collected)\n",
    "            avg_value_city = int(df_avg[df_avg['City'] == excel_sheet_name]['Avg'])\n",
    "            if abs(amount_of_data_collected - avg_value_city)/avg_value_city > 0.15 :\n",
    "                if amount_of_data_collected < avg_value_city:\n",
    "                    print(abs(amount_of_data_collected - avg_value_city), excel_sheet_name, amount_of_data_collected, avg_value_city)\n",
    "    #                     logger_done.info(abs(amount_of_data_collected - avg_value_city), excel_sheet_name, amount_of_data_collected, avg_value_city)\n",
    "                category_to_get = df_links[(df_links['City'] == excel_sheet_name)]['MatchCategory'].tolist()\n",
    "                category_collected = data_collected['Kategoria'].drop_duplicates().tolist()\n",
    "    #             display(data_collected.groupby('Kategoria')['Kategoria'].count())\n",
    "                for category_name in category_to_get:\n",
    "                    if category_name in category_collected:\n",
    "                        pass\n",
    "                    else:\n",
    "    #                     If the category is missing in the excel sheet add it to re-run data\n",
    "                        print(f'Missing {category_name} for {excel_sheet_name}')\n",
    "                        re_run_data.append([excel_sheet_name, category_name])\n",
    "    #                 FOR TESTING\n",
    "    #                 re_run_data.append([excel_sheet_name, category_name])\n",
    "    #                 re_run_data.append([excel_sheet_name, 'all'])\n",
    "    #     If the excel sheet is missing add it to re-run data\n",
    "        else:\n",
    "            print(f'Missing {excel_sheet_name} in data')\n",
    "            re_run_data.append([excel_sheet_name, 'all'])\n",
    "    if len(re_run_data) > 0:\n",
    "        pd.DataFrame(re_run_data).to_csv(re_run_path, index=False, header=['City', 'Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7468f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_avg_data_required():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    # COUNT AVG PER CITY \n",
    "    # Initialize variables\n",
    "    city_counts = []\n",
    "    total_rows = 0\n",
    "    result = []\n",
    "    # Iterate over each text file in the directory\n",
    "    for file_name in os.listdir(folder_path_with_txt_to_count_avg):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path_with_txt_to_count_avg, file_name)\n",
    "\n",
    "            # Open the text file\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Extract the city name using regular expressions\n",
    "                city_list = re.findall(r'\\d+ - ([^\\n]+).', content)\n",
    "                count_list = re.findall(r'\\d+ rows', content)\n",
    "\n",
    "                for item1, item2 in zip(city_list, count_list):\n",
    "                    joined = str(item1) + ' ' + str(item2.split(' ')[0])\n",
    "                    result.append(joined)\n",
    "\n",
    "                for row in result:\n",
    "                    city = row.split(' ')[0]\n",
    "\n",
    "                    # Extract the row count using regular expressions\n",
    "                    count_match = row.split(' ')[1]\n",
    "                    count = int(count_match)\n",
    "                    # Add the city and row count to the list\n",
    "                    city_counts.append((city, count))\n",
    "\n",
    "                    # Update the total row count\n",
    "                    total_rows += count\n",
    "    city_population = {}\n",
    "\n",
    "    # Store population values for each city\n",
    "    for city, row_count in city_counts:\n",
    "        if city in city_population:\n",
    "            city_population[city].append(row_count)\n",
    "        else:\n",
    "            city_population[city] = [row_count]\n",
    "\n",
    "    # Calculate average population for each city\n",
    "    city_avg = {}\n",
    "    for city, population_list in city_population.items():\n",
    "        city_avg[city] = round(sum(population_list) / len(population_list),0)\n",
    "\n",
    "    # Print average population for each city\n",
    "    #     report_str+= f\"{city} - {round(avg, 0)}\"\n",
    "    avg_path_viator = 'resource/avg-viator.csv'\n",
    "    # with open(avg_path_viator, \"w\") as f:\n",
    "    #                 f.write(report_str)\n",
    "    df = pd.DataFrame(city_avg.items(), columns=['City', 'Avg'])\n",
    "    df.to_csv(avg_path_viator, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d326bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### FOR RE-RUN PREPARATION\n",
    "# def re_run_daily():\n",
    "#     global re_run_path\n",
    "#     global link_file\n",
    "#     global archive_folder\n",
    "#     #     re_run_path = fr'output/GYG/2023-05-31-ReRun-GYG.csv'\n",
    "#     if os.path.exists(re_run_path):\n",
    "#         df_re_run = pd.read_csv(re_run_path)\n",
    "#         df_links = pd.read_csv(link_file)\n",
    "#         mergded_df_re_run = pd.merge(df_links,df_re_run, how='right', on=('City'))\n",
    "\n",
    "#         for index, row in mergded_df_re_run.iterrows():\n",
    "#             if row['Category_y'] == 'all':\n",
    "#                 continue\n",
    "#             if row['Category_y'] != row['MatchCategory']:\n",
    "#                 mergded_df_re_run.drop(index=index, inplace=True)\n",
    "\n",
    "#         daily_run_viator(mergded_df_re_run, True)\n",
    "#     else:\n",
    "#         print('No missing categories or cities')\n",
    "\n",
    "    \n",
    "# #     NOT DONE DATA IS NOT BEING INSERTED TO EXCEL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f5c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66c0492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_city_data(df_links):\n",
    "#     cities_to_process = []\n",
    "\n",
    "#     for city in df_links['City'].unique():\n",
    "#         city_path_done = fr'{output_viator}/{date_today}-{city}.csv'\n",
    "#         if os.path.exists(city_path_done):\n",
    "#             print(city)\n",
    "#             city_done_msg = pd.read_csv(city_path_done)\n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "#         # Request the status of all URLs at once\n",
    "#         city_done_msg['Status'] = city_done_msg['UrlResponse'].apply(lambda url: requests.get(url=url).json()['status'])\n",
    "\n",
    "#         # Check if all statuses are finished\n",
    "#         if all(city_done_msg['Status'] == 'finished'):\n",
    "#             print('All finished')\n",
    "#             df_links = df_links[df_links['City'] != city]\n",
    "#             try:\n",
    "#                 position = df_links[df_links['City'] == city]['Page'] * 24\n",
    "#             except:\n",
    "#                 position = 0\n",
    "            \n",
    "#             max_page_for_city = get_max_pages(city_done_msg.iloc[0]['UrlResponse'])\n",
    "#             city_done_msg['MaxPage'] = max_page_for_city\n",
    "#             city_done_msg.to_csv(file_path_done, header=not os.path.exists(file_path_done), index=False, mode='a')\n",
    "#             cities_to_process.append(city)\n",
    "#             process_html_from_response_scraperapi(city_done_msg, position)\n",
    "            \n",
    "# #             os.remove(city_path_done)\n",
    "\n",
    "#     return df_links, cities_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "859fe4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_url_to_process_scraperapi(url_input, city_input, category_input, page = 1, max_pages = 25):\n",
    "#     global date_today\n",
    "#     global output_viator\n",
    "#     global file_path_done\n",
    "#     global file_path_output\n",
    "#     global avg_file\n",
    "#     global re_run_path\n",
    "#     global folder_path_with_txt_to_count_avg\n",
    "#     global archive_folder\n",
    "#     data = []\n",
    "#     city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#     if city_input == 'Capri':\n",
    "#         max_pages = 9\n",
    "#     elif city_input == 'Taormina':\n",
    "#         max_pages = 6\n",
    "#     elif city_input == 'Lisbon' and category_input == 'Global':\n",
    "#         max_pages = 100\n",
    "#     elif city_input == 'Porto' and category_input == 'Global' :\n",
    "#         max_pages = 40\n",
    "        \n",
    "#     if os.path.exists(city_path_done):\n",
    "#         city_done_msg = pd.read_csv(city_path_done)\n",
    "#         page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page']) + 1\n",
    "# #         print(f'City: {city_input} category: {category_input} have page done {page} in file {city_path_done}')\n",
    "        \n",
    "    \n",
    "#     url_time = time.time()\n",
    "#     while page <= max_pages:\n",
    "#         if page == 1:\n",
    "#             url = f'{url_input}'\n",
    "#         else:\n",
    "#             url = f'{url_input}/{page}'\n",
    "#         print(url)\n",
    "\n",
    "#         country_codes = [\n",
    "#             'us',\n",
    "#             'eu'\n",
    "#             ]\n",
    "\n",
    "#         random_country_code = random.choice(country_codes)\n",
    "        \n",
    "# # CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#         print(random_country_code)\n",
    "#         params = {\n",
    "#             'url': url,\n",
    "#             'apikey': API_KEY_ZENROWS,\n",
    "#             'js_render': 'true',\n",
    "#             'json_response': 'true',\n",
    "#             'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "#             'premium_proxy': 'true',\n",
    "#         }\n",
    "#         response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "#         url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "#                                     json={'apiKey': f'{API_KEY_ZENROWS}', \n",
    "#                                           'country_code': random_country_code,\n",
    "#                                           'url': url })\n",
    "# #         time.sleep(5)\n",
    "#         if url_request.status_code == 200:\n",
    "#             try:\n",
    "#                 print(url_request.json()['statusUrl'])\n",
    "#                 status_url = url_request.json()['statusUrl']\n",
    "#                 data_send_df = pd.DataFrame({\n",
    "#                     'UrlRequest': [url],\n",
    "#                     'UrlResponse': [status_url],\n",
    "#                     'City': [city_input],\n",
    "#                     'Page': [page],\n",
    "#                     'Category': category_input\n",
    "#                 }, columns=['UrlRequest', 'UrlResponse', 'City', 'Page', 'Category'])\n",
    "#                 data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"JSON could not be decoded\")\n",
    "#         else:\n",
    "#             print(\"HTTP request returned code: \", url_request.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "#             page -=1\n",
    "\n",
    "\n",
    "# # IN THE TEXT FILE ADD URL AND STATUS AND WHICH PAGE IS IT RELATED TO \n",
    "        \n",
    "#         page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec6927d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_from_response_zenrows(response, city, category, position = 0, DEBUG=False):    \n",
    "    data = []\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')       \n",
    "    tours = soup.select(\"[data-automation*=ttd-product-list-card]\")\n",
    "    if DEBUG:\n",
    "        print(response)\n",
    "    # Filter these elements to find those that exactly match your target attribute value\n",
    "    tour_items = [el for el in tours if el.get('data-automation') == r'\\\"ttd-product-list-card\\\"']\n",
    "    print(f\"Found {len(tour_items)} elements with exact 'data-automation=ttd-product-list-card' match.\")\n",
    "    if len(tour_items) > 0:\n",
    "        for tour_item in tour_items:\n",
    "        #                 page_pos = tour_item['data-action-page-properties']\n",
    "        #                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "        #                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "            position = position + 1\n",
    "            if DEBUG:\n",
    "                print(tour_item.content)\n",
    "            title = tour_item.select_one(\"[data-automation*=ttd-product-list-card-title]\").get_text()\n",
    "            price_container = tour_item.select_one(\"[data-automation*=ttd-product-list-card-price]\")\n",
    "            price = price_container.select_one(\"[class*=currentPrice]\").text.strip().split('from')[-1]\n",
    "            try:\n",
    "                part_url = tour_item.select_one(\"[data-automation*=ttd-product-list-card-link]\").get('href').split('\"')[1].split('\\\\')[0]\n",
    "            except:\n",
    "                try:\n",
    "                    part_url = tour_item['href'].split('\"')[1].split('\\\\')[0]\n",
    "                except:\n",
    "                    logger_err.error(f\"No able to find the HREF for {title}, moving further\")\n",
    "                    part_url = \"\"\n",
    "                    \n",
    "            product_url = f\"https://www.viator.com{part_url}\"\n",
    "            siteuse = 'Viator'\n",
    "            try:\n",
    "                discount = price_container.select_one(\"[class*=discountInfoContainer]\").select_one(\"[class*=originalPrice]\").text.strip()\n",
    "            except:\n",
    "                discount = 'N/A'\n",
    "\n",
    "            amount_reviews = 'N/A'\n",
    "            #NUMBER OF REVIEWS\n",
    "            try:\n",
    "                amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            try:\n",
    "                star_int = 0\n",
    "                stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "                half_star = 'M14'\n",
    "                for st in stars_grouped:\n",
    "                    path_text = str(st.find('path')['d'])\n",
    "                    if half_star in path_text:\n",
    "                        star_int = star_int + 0.5\n",
    "                    else:\n",
    "                        if '0a.77.77' in str(st):\n",
    "                            star_int = star_int + 1\n",
    "                stars = f'star-{str(star_int)}'\n",
    "            except:\n",
    "                stars = 'N/A'\n",
    "\n",
    "            text = tour_item.text.strip()\n",
    "                \n",
    "            data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "    else:\n",
    "        tour_items = soup.select(\"[class*=productListCardWithDebug]\")\n",
    "#             print('Running using debug HTML')\n",
    "        for tour_item in tour_items:\n",
    "            position = position + 1\n",
    "            title = tour_item.select_one(\"[class*=title]\").text.strip()\n",
    "            price = tour_item.select_one(\"[class*=currentPrice]\").text.strip()\n",
    "            if 'from' in price:\n",
    "                price = price.split('from')[1]\n",
    "            splitter = price[0]\n",
    "            product_url = f\"https://www.viator.com{tour_item.find('a')['href']}\"\n",
    "            siteuse = 'Viator'\n",
    "            star =\"M7.5 0a.77.77 0 00-.701.456L5.087 4.083a.785.785 0 01-.588.448l-3.827.582a.828.828 0 00-.433 1.395L3.008 9.33c.185.192.26\"\n",
    "            half =\"M14.761 6.507a.828.828 0 00-.433-1.395L10.5 4.53a.785.785 0 01-.589-.447L8.201.456a.767.767 0 00-1.402 0L5.087 4.083a.785\"\n",
    "            nostar =\"M7.5 1.167l1.565 3.317c.242.52.728.885 1.295.974l3.583.544-2.62 2.673a1.782 1.782 0 00-.48 1.532l.609 3.718L8.315 12.2a1.6\"\n",
    "            try:\n",
    "                discount = tour_item.select_one(\"[class*=savingsLabel]\").text.strip()\n",
    "            except:\n",
    "                discount = 'N/A'\n",
    "            try:\n",
    "                amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "            except:\n",
    "                amount_reviews = 'N/A'\n",
    "            try:\n",
    "                star_int = 0\n",
    "                stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "                half_star = 'M14'\n",
    "                for st in stars_grouped:\n",
    "                    path_text = str(st.find('path')['d'])\n",
    "                    if half_star in path_text:\n",
    "                        star_int = star_int + 0.5\n",
    "                    else:\n",
    "                        if '0a.77.77' in str(st):\n",
    "                            star_int = star_int + 1\n",
    "                stars = f'star-{str(star_int)}'\n",
    "            except:\n",
    "                stars = 'N/A'\n",
    "            text = tour_item.text.strip()\n",
    "\n",
    "            data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "    # print(f'URL: {city} currency: {splitter}')\n",
    "    url_done = time.time()\n",
    "    # message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page Currency: 1-{first_style_curr}, 2-{second_style_curr}, 3-{thirtd_style_curr}'\n",
    "    # print(message)\n",
    "    # logger_info.info(message)\n",
    "    df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto'])\n",
    "    df['Pozycja'] = df.groupby('Kategoria').cumcount() + 1\n",
    "    file_path = fr'{output_viator}/{date_today}-{city}-Viator.csv' \n",
    "    df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e76f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_city(row, thread_name = None):\n",
    "    \n",
    "    global date_today, output_viator, API_KEY_ZENROWS\n",
    "    \n",
    "    if thread_name:\n",
    "        current_thread().name = thread_name\n",
    "    page = 1\n",
    "    url_input = row[\"URL\"]\n",
    "    city_input = row['City']\n",
    "    category_input = row['MatchCategory']\n",
    "    max_pages = calculate_max_pages(city_input, category_input)\n",
    "\n",
    "    city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'\n",
    "    city_path_done_archive = fr'{output_viator}/archive/{date_today}-{city_input}-{category_input}.csv'\n",
    "    \n",
    "    if os.path.exists(city_path_done):\n",
    "        city_done_msg = pd.read_csv(city_path_done)\n",
    "        page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page'].iloc[0]) + 1\n",
    "        logger_info.info(f'Resuming {city_input}-{category_input} from page {page} of {max_pages}')\n",
    "    elif os.path.exists(city_path_done_archive):\n",
    "        logger_done.info('City already in Archive folder moving further')\n",
    "        return\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        url = f'{url_input}' if page == 1 else f'{url_input}/{page}'\n",
    "        logger_info.info(f'Processing: {city_input}, {category_input}, Page: {page} of max {max_pages}, URL: {url}')\n",
    "        response = make_request(url)\n",
    "        logger_info.info(current_thread().name)\n",
    "        if response and response.status_code == 200:\n",
    "            try:\n",
    "                save_data(response, city_input, category_input, url, page, city_path_done)\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger_err.error(f'JSON could not be decoded for URL: {url}, error: {str(e)}')\n",
    "                raise\n",
    "        else:\n",
    "            # Log the error with the status code and response content\n",
    "            logger_err.error(f'HTTP request failed for city: {city_input} category: {category_input} page: {page} with status code {response.status_code}  Decrement the page count. Content: {response.content}')\n",
    "            page -= 1\n",
    "            # Specific handling for 403 and 429 status codes\n",
    "            if response.status_code == 403:\n",
    "                logger_err.error(f'{current_thread().name}: IP Address Blocked, sleeping for 5 minutes before retrying.')\n",
    "                time.sleep(300)  # Wait for 5 minutes before retrying\n",
    "            elif response.status_code == 429:\n",
    "                logger_err.error(f'{current_thread().name}: Concurrency limit exceeded , sleeping for 5 minutes before retrying.')\n",
    "                time.sleep(300)  # Wait for 5 minutes before retrying\n",
    "            else:\n",
    "                logger_err.error(f'Status code did not set for {response.status_code}')\n",
    "        page += 1\n",
    "    \n",
    "    shutil.move(city_path_done, city_path_done_archive)\n",
    "    logger_info.info((f'Archived file to {city_path_done_archive}'))\n",
    "\n",
    "\n",
    "def calculate_max_pages(city_input, category_input):\n",
    "    if city_input == 'Capri':\n",
    "        return 9\n",
    "    if city_input == 'Taormina':\n",
    "        return 6\n",
    "    if city_input == 'Lisbon' and category_input == 'Global':\n",
    "        return 65\n",
    "    if city_input == 'Porto' and category_input == 'Global':\n",
    "        return 30\n",
    "    if city_input == 'Venice' and category_input == 'Global':\n",
    "        return 55\n",
    "    return 25 if category_input == 'Global' else 2\n",
    "\n",
    "def make_request(url):\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'apikey': API_KEY_ZENROWS,\n",
    "        'js_render': 'true',\n",
    "        'json_response': 'true',\n",
    "        'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "        'premium_proxy': 'true',\n",
    "    }\n",
    "    return requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "def save_data(response, city_input, category_input, url, page, city_path_done):\n",
    "    try:\n",
    "        data_send_df = pd.DataFrame({\n",
    "            'UrlRequest': [url],\n",
    "            'City': city_input,\n",
    "            'Page': [page],\n",
    "            'Category': category_input\n",
    "        }, columns=['UrlRequest', 'City', 'Page', 'Category'])\n",
    "        data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "        logger_done.info(f'Data for {city_input}-{category_input}, Page {page} saved on disk')\n",
    "        process_html_from_response_zenrows(response, city_input, category_input)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON could not be decoded\")\n",
    "\n",
    "def send_url_to_process_zenrows(df_links):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {}\n",
    "        for index, row in df_links.iterrows():\n",
    "            thread_name = f\"CityProcessor-{row['City']}-{row['MatchCategory']}-index-{index}\"\n",
    "            futures[executor.submit(process_city, row, thread_name=thread_name)] = row\n",
    "\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "# The rest of your global variables and helper functions should be defined outside of these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4982fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_url_to_process_zenrows(df_links):\n",
    "#     global date_today\n",
    "#     global output_viator\n",
    "#     global file_path_done\n",
    "#     global file_path_output\n",
    "#     global avg_file\n",
    "#     global re_run_path\n",
    "#     global folder_path_with_txt_to_count_avg\n",
    "#     global archive_folder\n",
    "\n",
    "#     for index, row in df_links.iterrows():\n",
    "#         print('Row processing: ', index)\n",
    "#         page = 1\n",
    "#         url_input = row[\"URL\"]\n",
    "#         city_input = row['City']\n",
    "#         category_input = row['MatchCategory']\n",
    "#         max_pages = calculate_max_pages(city_input, category_input)\n",
    "\n",
    "#         city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#         city_path_done_archive = fr'{output_viator}/archive/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#         if os.path.exists(city_path_done):\n",
    "#             city_done_msg = pd.read_csv(city_path_done)\n",
    "#             page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page'].iloc[0]) + 1\n",
    "#         elif os.path.exists(city_path_done_archive):\n",
    "#             logger_done.info('City already in Archive folder moving further')\n",
    "#             df_links = df_links.drop(index)\n",
    "#             page = max_pages + 1\n",
    "#             continue\n",
    "                        \n",
    "\n",
    "# #         print(f'City: {city_input} category: {category_input} have page done {page} in file {city_path_done}')\n",
    "        \n",
    "\n",
    "#         while page <= max_pages:\n",
    "#             if page == 1:\n",
    "#                 url = f'{url_input}'\n",
    "#             else:\n",
    "#                 url = f'{url_input}/{page}'\n",
    "#             print(url)\n",
    "#             page += 1\n",
    "            \n",
    "#     # CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#             print(city_input, category_input, url, 'Processing in ZEN')\n",
    "#             params = {\n",
    "#                 'url': url,\n",
    "#                 'apikey': API_KEY_ZENROWS,\n",
    "#                 'js_render': 'true',\n",
    "#                 'json_response': 'true',\n",
    "#                 'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "#                 'premium_proxy': 'true',\n",
    "#             }\n",
    "#             response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "#             # time.sleep(5)\n",
    "#             if response.status_code == 200:\n",
    "#                     try:\n",
    "#                         data_send_df = pd.DataFrame({\n",
    "#                             'UrlRequest': [url],\n",
    "#                             'City': city_input,\n",
    "#                             'Page': [page],\n",
    "#                             'Category': category_input\n",
    "#                         }, columns=['UrlRequest', 'City', 'Page', 'Category'])\n",
    "#                         data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "#                         print('Data saved on disk, processing to extract data')\n",
    "#                         process_html_from_response_zenrows(response, city_input, category_input)\n",
    "#                     except json.JSONDecodeError:\n",
    "#                         print(\"JSON could not be decoded\")\n",
    "#             else:\n",
    "#                     print(\"HTTP request returned code: \", response.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "#                     page -=1\n",
    "#         shutil.move(city_path_done, city_path_done_archive)\n",
    "#         logger_info.info((f'Archived file to {city_path_done_archive}'))\n",
    "\n",
    "\n",
    "#     # IN THE TEXT FILE ADD URL AND STATUS AND WHICH PAGE IS IT RELATED TO \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983f0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def get_max_pages(session, url, retries=5, backoff_factor=0.6):\n",
    "#     retry_wait = backoff_factor\n",
    "    \n",
    "#     for attempt in range(1, retries + 1):\n",
    "#         try:\n",
    "#             results = session.get(url, timeout=10 + (retry_wait) + 5)  # 10 seconds timeout\n",
    "#             results.raise_for_status()  # Raises an error for 4XX or 5XX status codes\n",
    "#         except requests.exceptions.HTTPError as http_err:\n",
    "#             logging.error(f'HTTP error occurred for {url}: {http_err}')\n",
    "#         except requests.exceptions.ConnectionError as conn_err:\n",
    "#             logging.error(f'Connection error occurred for {url}: {conn_err}')\n",
    "#         except requests.exceptions.Timeout as timeout_err:\n",
    "#             logging.error(f'Timeout error occurred for {url}: {timeout_err}')\n",
    "#         except requests.exceptions.RequestException as err:\n",
    "#             logging.error(f'Unexpected error occurred for {url}: {err}')\n",
    "        \n",
    "#         if attempt < retries:\n",
    "#             logging.info(f'Waiting {retry_wait} seconds before retrying...')\n",
    "#             time.sleep(retry_wait)\n",
    "#             retry_wait *= 2  # Exponential backoff\n",
    "            \n",
    "#     soup = BeautifulSoup(results.content, 'html.parser')\n",
    "#     product_list_count = None\n",
    "\n",
    "#     # Try finding the productListCount label using two different CSS selectors\n",
    "#     selectors = [\"[id*=productListCount]\", \"h3[class*=productListCount]\", \"h2[class*=productListCountLabel]\"]\n",
    "\n",
    "#     for selector in selectors:\n",
    "#         count_element = soup.select_one(selector)\n",
    "#         if count_element:\n",
    "#             product_list_count = int(count_element.text.split()[0].replace(',', ''))\n",
    "#             break\n",
    "\n",
    "#     if product_list_count is None:\n",
    "#         print(\"Product count not found in the HTML content.\")\n",
    "#         return None\n",
    "#     try:\n",
    "#         max_pages = int(round(product_list_count / 24, 0))\n",
    "#         return max_pages\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error while fetching HTML content: {e}\")\n",
    "#         return 25  # Return a default value of 25 pages if there's an error\n",
    "    \n",
    "# # Define the get_status function with a retry mechanism\n",
    "# def get_status(session, url, retries=7, backoff_factor=0.9):\n",
    "#     retry_wait = backoff_factor\n",
    "#     for attempt in range(1, retries + 1):\n",
    "#         try:\n",
    "#             logging.info(f'Attempt {attempt}: Sending request to {url}')\n",
    "#             response = session.get(url, timeout=10 + retry_wait + 5)  # 10 seconds timeout\n",
    "#             response.raise_for_status()  # Raises an error for 4XX or 5XX status codes\n",
    "#             status = response.json().get('status')\n",
    "#             logging.info(f'Response received: {url} - Status: {status}')\n",
    "#             return url, status\n",
    "#         except requests.exceptions.HTTPError as http_err:\n",
    "#             logging.error(f'HTTP error occurred for {url}: {http_err}')\n",
    "#         except requests.exceptions.ConnectionError as conn_err:\n",
    "#             logging.error(f'Connection error occurred for {url}: {conn_err}')\n",
    "#         except requests.exceptions.Timeout as timeout_err:\n",
    "#             logging.error(f'Timeout error occurred for {url}: {timeout_err}')\n",
    "#         except requests.exceptions.RequestException as err:\n",
    "#             logging.error(f'Unexpected error occurred for {url}: {err}')\n",
    "        \n",
    "#         if attempt < retries:\n",
    "#             logging.info(f'Waiting {retry_wait} seconds before retrying...')\n",
    "#             time.sleep(retry_wait)\n",
    "#             retry_wait *= 2  # Exponential backoff\n",
    "\n",
    "#     # If all retries fail, return 'error' status\n",
    "#     logging.error(f'All attempts failed for {url}. Marking status as error.')\n",
    "#     return url, 'error'\n",
    "\n",
    "# def check_status_and_process_city_data(df_links):\n",
    "#     cities_to_process = []\n",
    "#     session = requests.Session()\n",
    "#     statuses = {}\n",
    "    \n",
    "    \n",
    "#     for index, row in df_links.iterrows():\n",
    "#         city = row['City']\n",
    "#         category = row['MatchCategory']\n",
    "#         city_path_done = fr'{output_viator}/{date_today}-{city}-{category}.csv'\n",
    "        \n",
    "        \n",
    "#         if os.path.exists(city_path_done):\n",
    "#             print(city, '-', category)\n",
    "#             city_done_msg = pd.read_csv(city_path_done)\n",
    "#             city_done_msg.drop_duplicates(inplace=True)\n",
    "#         else:\n",
    "#             continue\n",
    "            \n",
    "        \n",
    "#         start_time_get_resposne = time.time()\n",
    "#         # Using ThreadPoolExecutor to fetch statuses\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#             future_to_url = {executor.submit(get_status, session, url): url for url in city_done_msg['UrlResponse']}\n",
    "#             for future in concurrent.futures.as_completed(future_to_url):\n",
    "#                 url = future_to_url[future]\n",
    "#                 try:\n",
    "#                     _, status = future.result()\n",
    "#                     statuses[url] = status  # Store the status with the URL as the key\n",
    "#                 except Exception as exc:\n",
    "#                     logging.error(f'{url} generated an exception: {exc}')\n",
    "                    \n",
    "#          # Update the DataFrame outside of the loop\n",
    "#         for url, status in statuses.items():\n",
    "#             city_done_msg.loc[city_done_msg['UrlResponse'] == url, 'Status'] = status\n",
    "#         end_time_get_resposne = time.time()\n",
    "#         print(f'First option with concurrent: {round(end_time_get_resposne-start_time_get_resposne,2)}s')\n",
    "#         print(f\"For {city} finished {len(city_done_msg[city_done_msg['Status'] == 'finished'])} from {len(city_done_msg)}\")\n",
    "#         # Check if all statuses are finished\n",
    "#         if len(city_done_msg[city_done_msg['Status'] == 'finished']) == len(city_done_msg):\n",
    "#             try:\n",
    "#                 position = df_links[df_links['City'] == city]['Page'] * 24\n",
    "#             except:\n",
    "#                 position = 0\n",
    "\n",
    "#             max_page_for_city = get_max_pages(session, city_done_msg.iloc[0]['UrlResponse'])\n",
    "#             city_done_msg['MaxPage'] = max_page_for_city\n",
    "#             process_html_from_response_scraperapi(city_done_msg, city_path_done,  position)\n",
    "#             df_links = df_links[(df_links['City'] != city) & (df_links['MatchCategory'] != category)]\n",
    "#             print(f'In check_status_and_process_city_data finished process removing: {city} - {category} ')\n",
    "            \n",
    "#     return df_links, cities_to_process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "143c2d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def process_html_from_response_scraperapi(data_city_df, city_path_done, position = 0):\n",
    "# # data_city_df = pd.read_csv(city_path_done)\n",
    "#     data = []\n",
    "# ### OPTION IF THE BELOW WILL NOT WORK PROPELRY CHECK BELOW   \n",
    "#     session = requests.Session()  # Using a Session object for connection pooling\n",
    "    \n",
    "#     # Set up retry strategy with backoff factor\n",
    "#     retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
    "#     session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "#     session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    \n",
    "#     for index, row in data_city_df.iterrows():\n",
    "#         url = row['UrlResponse'].replace(',', '')\n",
    "        \n",
    "#         try:\n",
    "#             logging.info(f'{index} - Starting request for URL: {url}')\n",
    "#             start_time = time.time()\n",
    "#             results = session.get(url, timeout=10)  # Set a reasonable timeout\n",
    "#             logging.info(F\"Time: {time.time() - start_time}\")\n",
    "#             results.raise_for_status()  # This will raise an exception for HTTP error codes\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             # Handle all requests-related exceptions\n",
    "#             logging.error(f'Request exception for URL {url}: {e}')        \n",
    "        \n",
    "#         soup = BeautifulSoup(results.content, 'html.parser')       \n",
    "#         tour_items = soup.select(\"[id*=productName]\")\n",
    "\n",
    "#         if len(tour_items) > 0:\n",
    "#             for tour_item in tour_items:\n",
    "# #                 page_pos = tour_item['data-action-page-properties']\n",
    "# #                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "# #                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "#                 position = position + 1\n",
    "#                 title = tour_item.find('h2').text.strip()\n",
    "#                 splitter = tour_item.text.split('From')[-1][0]\n",
    "#                 price = splitter + tour_item.text.split('From')[-1].split(splitter)[1]\n",
    "#                 if len(price) > 9:\n",
    "#                     price = price.split('Price')[0]\n",
    "#                 part_url = tour_item['data-url'].split('\"')[1].split('\\\\')[0]\n",
    "#                 product_url = f\"https://www.viator.com{part_url}\"\n",
    "#                 siteuse = 'Viator'\n",
    "#                 city = row['City']\n",
    "#                 category = row['Category']\n",
    "#                 try:\n",
    "#                     discount = tour_item.find('div', {'class': 'text-special product-list-card-savings-label'}).text.strip()\n",
    "#                 except:\n",
    "#                     discount = 'N/A'\n",
    "\n",
    "#                 amount_reviews = 'N/A'\n",
    "#                 #NUMBER OF REVIEWS\n",
    "#                 spans = tour_item.select('span')\n",
    "#                 for span in spans:\n",
    "#         #             print('________________________')\n",
    "#         #             print(span.attrs)\n",
    "#                     try:\n",
    "#                         span['reviewlink']\n",
    "#                         amount_reviews = span.text\n",
    "#                         break\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 try:\n",
    "#                     stars = tour_item.find('svg').text.strip()\n",
    "#                 except:\n",
    "#                     stars = 'N/A'\n",
    "\n",
    "#                 text = tour_item.text.strip()\n",
    "\n",
    "\n",
    "#                 data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "#         else:\n",
    "#             tour_items = soup.select(\"[class*=productListCardWithDebug]\")\n",
    "# #             print('Running using debug HTML')\n",
    "#             for tour_item in tour_items:\n",
    "#                 position = position + 1\n",
    "#                 title = tour_item.select_one(\"[class*=title]\").text.strip()\n",
    "#                 price = tour_item.select_one(\"[class*=currentPrice]\").text.strip()\n",
    "#                 if 'from' in price:\n",
    "#                     price = price.split('from')[1]\n",
    "#                 splitter = price[0]\n",
    "#                 product_url = f\"https://www.viator.com{tour_item.find('a')['href']}\"\n",
    "#                 siteuse = 'Viator'\n",
    "#                 city = row['City']\n",
    "#                 category = row['Category']\n",
    "\n",
    "#                 star =\"M7.5 0a.77.77 0 00-.701.456L5.087 4.083a.785.785 0 01-.588.448l-3.827.582a.828.828 0 00-.433 1.395L3.008 9.33c.185.192.26\"\n",
    "#                 half =\"M14.761 6.507a.828.828 0 00-.433-1.395L10.5 4.53a.785.785 0 01-.589-.447L8.201.456a.767.767 0 00-1.402 0L5.087 4.083a.785\"\n",
    "#                 nostar =\"M7.5 1.167l1.565 3.317c.242.52.728.885 1.295.974l3.583.544-2.62 2.673a1.782 1.782 0 00-.48 1.532l.609 3.718L8.315 12.2a1.6\"\n",
    "#                 try:\n",
    "#                     discount = tour_item.select_one(\"[class*=savingsLabel]\").text.strip()\n",
    "#                 except:\n",
    "#                     discount = 'N/A'\n",
    "#                 try:\n",
    "#                     amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "#                 except:\n",
    "#                     amount_reviews = 'N/A'\n",
    "#                 try:\n",
    "#                     star_int = 0\n",
    "#                     stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "#                     half_star = 'M14'\n",
    "#                     for st in stars_grouped:\n",
    "#                         path_text = str(st.find('path')['d'])\n",
    "#                         if half_star in path_text:\n",
    "#                             star_int = star_int + 0.5\n",
    "#                         else:\n",
    "#                             if '0a.77.77' in str(st):\n",
    "#                                 star_int = star_int + 1\n",
    "#                     stars = f'star-{str(star_int)}'\n",
    "#                 except:\n",
    "#                     stars = 'N/A'\n",
    "#                 text = tour_item.text.strip()\n",
    "\n",
    "#                 data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "#         print(f'URL: {city} currency: {splitter}')\n",
    "#     url_done = time.time()\n",
    "#     # message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page Currency: 1-{first_style_curr}, 2-{second_style_curr}, 3-{thirtd_style_curr}'\n",
    "#     # print(message)\n",
    "#     # logger_info.info(message)\n",
    "#     df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto'])\n",
    "#     df['Pozycja'] = df.groupby('Kategoria').cumcount() + 1\n",
    "#     file_path = fr'{output_viator}/{date_today}-{city}-Viator.csv' \n",
    "#     df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')\n",
    "#     data_city_df.to_csv(file_path_done, header=not os.path.exists(file_path_done), index=False, mode='a')\n",
    "#     os.remove(city_path_done)\n",
    "# #     row.to_csv(file_path_done, header=True, index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22b407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d12a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_run_viator(df_links=pd.DataFrame(), re_run=False):\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    if len(df_links) == 0:\n",
    "        df_links = pd.read_csv(link_file)\n",
    "    EUR_City = [\n",
    "        \"Amsterdam\", \"Athens\", \"Barcelona\", \"Berlin\", \"Dublin\", \"Dubrovnik\", \"Florence\", \"Istanbul\",\n",
    "        \"Krakow\", \"Lisbon\", \"Madrid\", \"Milan\", \"Naples\", \"Paris\", \"Porto\", \"Rome\", \"Palermo\", \"Venice\",\n",
    "        \"Taormina\", \"Capri\", \"Sorrento\", \"Mount-Etna\", \"Mount-Vesuvius\", \"Herculaneum\", \"Amalfi-Coast\",\n",
    "        \"Pompeii\"\n",
    "    ]\n",
    "\n",
    "    USD_City = [\n",
    "        \"Las-Vegas\", \"New-York-City\", \"Cancun\", \"Dubai\"\n",
    "    ]\n",
    "\n",
    "    GBP_City = [\n",
    "        \"Edinburgh\", \"London\"\n",
    "    ]\n",
    "\n",
    "#     date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_path_done =fr'output/Viator/{date_today}-DONE-Viator.csv'  \n",
    "#     file_path_output = f\"output/Viator - {date_today}.xlsx\"\n",
    "    if os.path.exists(file_path_output) and re_run == False:\n",
    "        print(f'Today ({date_today}) Viator done')\n",
    "        return 'Done'\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.exists(file_path_done) and re_run == False:\n",
    "        \n",
    "        done_msg = pd.read_csv(file_path_done).drop_duplicates(subset=['City', 'Category'], keep='last').reset_index()\n",
    "#         display(df_links)\n",
    "#         df_links = df_links[~(df_links['City'].isin(done_msg['City']) & df_links['MatchCategory'].isin(done_msg['Category']))]\n",
    "        merged = df_links.merge(done_msg, left_on=['City', 'MatchCategory'], right_on=['City', 'Category'], how='left', indicator=True)\n",
    "        # Filter rows where '_merge' is 'left_only', which means the combination is not present in done_msg\n",
    "        filtered = merged[merged['_merge'] == 'left_only']\n",
    "        # Drop the _merge column and reset index\n",
    "        filtered = filtered.drop(columns='_merge').reset_index(drop=True)\n",
    "        df_links = filtered\n",
    "#         df_links = df_links[~df_links['City'].isin(done_msg['City'].values)]\n",
    "        df_links_with_page_maxpage = df_links[df_links['City'].isin(done_msg['City'].values)]\n",
    "        df_links_with_page_maxpage = pd.merge(df_links_with_page_maxpage, done_msg[['City', 'Page', 'MaxPage']], on='City', how='left')\n",
    "    elif re_run == True:\n",
    "        print(f'Lenght of links: {len(df_links)}')\n",
    "    else:\n",
    "        logger_info.info(\"Nothing done yet\")\n",
    "\n",
    "    # Define the URL of the website we want to scrape\n",
    "    start_time = time.time()\n",
    "    if len(df_links) == 0:\n",
    "        print('Df_links empty')\n",
    "        return 'Done'\n",
    "    df_links = df_links[df_links['Priority'] > 0]\n",
    "    send_url_to_process_zenrows(df_links)\n",
    "    # print('Finished sending data to scraperapi')\n",
    "        \n",
    "#     display(df_links)\n",
    "#     while not df_links.empty:\n",
    "# #         display(df_links)\n",
    "#         df_links, processed_cities = check_status_and_process_city_data(df_links)\n",
    "#         print(f'After processing one row in df_links the df_links is {len(df_links)}')\n",
    "# #         display(df_links)\n",
    "        \n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59c382fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_all_csv_processed():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    # Get all CSV files with the specified date prefix    \n",
    "    csv_files = [file for file in os.listdir(f'{output_viator}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "    csv_files_not_finished = []\n",
    "    for csv in csv_files:\n",
    "        if 'viator' not in csv.lower():\n",
    "            csv_files_not_finished.append(csv)\n",
    "\n",
    "\n",
    "    if len(csv_files_not_finished) == 0:\n",
    "        return 'brake'\n",
    "    else:\n",
    "        return f\"Files to process: {len(csv_files_not_finished)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e292ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 11:06:26,942 - Done_logger - INFO - There are 733 pages to collect daily which is 18325 credits daily\n",
      "2024-04-25 11:06:26,943 - Done_logger - INFO - Requried credits per month for current setup 549750\n"
     ]
    }
   ],
   "source": [
    "def calculate_max_pages_specualtions(city_input, category_input):\n",
    "    if city_input == 'Capri':\n",
    "        return 9\n",
    "    if city_input == 'Taormina':\n",
    "        return 6\n",
    "    if city_input == 'Lisbon' and category_input == 'Global':\n",
    "        return 65\n",
    "    if city_input == 'Porto' and category_input == 'Global':\n",
    "        return 30\n",
    "    if city_input == 'Venice' and category_input == 'Global':\n",
    "        return 55\n",
    "    return 25 if category_input == 'Global' else 2\n",
    "\n",
    "def count_credits_use():\n",
    "    df_links = pd.read_csv(link_file)\n",
    "    total_pages_per_day = sum(calculate_max_pages_specualtions(row['City'], row['MatchCategory']) for index, row in df_links.iterrows())\n",
    "    credit_per_page = 25\n",
    "    avg_days_in_month = 30\n",
    "    logger_done.info(f'There are {total_pages_per_day} pages to collect daily which is {total_pages_per_day*credit_per_page} credits daily')\n",
    "    logger_done.info(f'Requried credits per month for current setup {total_pages_per_day*credit_per_page*avg_days_in_month}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d586cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80e60ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:46:27,381 - Info_logger - INFO - Nothing done yet\n",
      "2024-04-25 10:46:27,388 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,390 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,390 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,391 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,394 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,395 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,395 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,396 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,398 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,399 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,400 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,412 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,414 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,415 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,420 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,420 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,421 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,422 - Info_logger - INFO - Resuming Palermo from page 19\n",
      "2024-04-25 10:46:27,423 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,425 - Info_logger - INFO - Resuming Pompeii from page 19\n",
      "2024-04-25 10:46:27,427 - Info_logger - INFO - Resuming Venice from page 52\n",
      "2024-04-25 10:46:27,430 - Info_logger - INFO - Resuming Sorrento from page 9\n",
      "2024-04-25 10:46:27,431 - Info_logger - INFO - Resuming Amalfi-Coast from page 8\n",
      "2024-04-25 10:46:27,432 - Info_logger - INFO - Resuming Capri from page 8\n",
      "2024-04-25 10:46:27,432 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,432 - Info_logger - INFO - Resuming London from page 9\n",
      "2024-04-25 10:46:27,435 - Info_logger - INFO - Resuming Krakow from page 17\n",
      "2024-04-25 10:46:27,435 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,440 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,444 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,444 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,445 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,449 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,450 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,454 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,454 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,455 - Info_logger - INFO - Processing: Palermo, Global, Page: 19 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/19\n",
      "2024-04-25 10:46:27,456 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,457 - Info_logger - INFO - Processing: Pompeii, Global, Page: 19 of max 25, URL: https://www.viator.com/Pompeii/d24336-ttd/19\n",
      "2024-04-25 10:46:27,457 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,457 - Info_logger - INFO - Processing: Venice, Global, Page: 52 of max 55, URL: https://www.viator.com/Venice/d522-ttd/52\n",
      "2024-04-25 10:46:27,461 - Info_logger - INFO - Processing: Sorrento, Global, Page: 9 of max 25, URL: https://www.viator.com/Sorrento/d947-ttd/9\n",
      "2024-04-25 10:46:27,463 - Info_logger - INFO - Processing: Amalfi-Coast, Global, Page: 8 of max 25, URL: https://www.viator.com/Amalfi-Coast/d946-ttd/8\n",
      "2024-04-25 10:46:27,463 - Info_logger - INFO - Processing: Capri, Global, Page: 8 of max 9, URL: https://www.viator.com/Capri/d4223-ttd/8\n",
      "2024-04-25 10:46:27,465 - Info_logger - INFO - Processing: London, Global, Page: 9 of max 25, URL: https://www.viator.com/London/d737-ttd/9\n",
      "2024-04-25 10:46:27,465 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,466 - Info_logger - INFO - Processing: Krakow, Global, Page: 17 of max 25, URL: https://www.viator.com/Krakow/d529-ttd/17\n",
      "2024-04-25 10:46:27,467 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,468 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,469 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,470 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,471 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,472 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,472 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,473 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,474 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,478 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,479 - Info_logger - INFO - Resuming Munich from page 6\n",
      "2024-04-25 10:46:27,488 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,491 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,491 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,492 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,493 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,494 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,494 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,495 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,496 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,497 - Info_logger - INFO - Processing: Munich, Global, Page: 6 of max 25, URL: https://www.viator.com/Munich/d487-ttd/6\n",
      "2024-04-25 10:46:27,497 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,497 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,498 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,499 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,499 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,500 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,501 - Done_logger - INFO - City already in Archive folder moving further\n",
      "2024-04-25 10:46:27,514 - Info_logger - INFO - Resuming Porto from page 3\n",
      "2024-04-25 10:46:27,515 - Info_logger - INFO - Resuming Porto from page 3\n",
      "2024-04-25 10:46:27,517 - Info_logger - INFO - Processing: Porto, Transfers-and-Ground-Transport, Page: 1 of max 2, URL: https://www.viator.com/Porto-tours/Transfers-and-Ground-Transport/d26879-g15\n",
      "2024-04-25 10:46:27,517 - Info_logger - INFO - Resuming Porto from page 3\n",
      "2024-04-25 10:46:27,521 - Info_logger - INFO - Processing: Las-Vegas, Global, Page: 1 of max 25, URL: https://www.viator.com/Las-Vegas/d684-ttd\n",
      "2024-04-25 10:46:27,521 - Info_logger - INFO - Resuming Porto from page 2\n",
      "2024-04-25 10:46:27,522 - Info_logger - INFO - Resuming Porto from page 2\n",
      "2024-04-25 10:46:27,523 - Info_logger - INFO - Resuming Porto from page 3\n",
      "2024-04-25 10:46:27,524 - Info_logger - INFO - Resuming Porto from page 2\n",
      "2024-04-25 10:46:27,526 - Info_logger - INFO - Resuming Porto from page 2\n",
      "2024-04-25 10:46:27,528 - Info_logger - INFO - Resuming Porto from page 2\n",
      "2024-04-25 10:46:27,528 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Port-Transfers.csv\n",
      "2024-04-25 10:46:27,531 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Art-and-Culture.csv\n",
      "2024-04-25 10:46:27,532 - Info_logger - INFO - Processing: Porto, Seasonal-and-Special-Occasions, Page: 2 of max 2, URL: https://www.viator.com/Porto-tours/Seasonal-and-Special-Occasions/d26879-tag21916/2\n",
      "2024-04-25 10:46:27,533 - Info_logger - INFO - Processing: Porto, Food-Wine-and-Nightlife, Page: 2 of max 2, URL: https://www.viator.com/Porto-tours/Food-Wine-and-Nightlife/d26879-g6/2\n",
      "2024-04-25 10:46:27,533 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Hop-on-Hop-off-Tours.csv\n",
      "2024-04-25 10:46:27,534 - Info_logger - INFO - Processing: Porto, Outdoor-Activities, Page: 2 of max 2, URL: https://www.viator.com/Porto-tours/Outdoor-Activities/d26879-g9/2\n",
      "2024-04-25 10:46:27,534 - Info_logger - INFO - Processing: Porto, Sightseeing-Tickets-and-Passes, Page: 2 of max 2, URL: https://www.viator.com/Porto-tours/Sightseeing-Tickets-and-Passes/d26879-g8/2\n",
      "2024-04-25 10:46:27,536 - Info_logger - INFO - Processing: Porto, Tours-and-Sightseeing, Page: 2 of max 2, URL: https://www.viator.com/Porto-tours/Tours-and-Sightseeing/d26879-g12/2\n",
      "2024-04-25 10:46:27,537 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Classes-and-Workshops.csv\n",
      "2024-04-25 10:46:48,578 - Info_logger - INFO - CityProcessor-Porto-Transfers-and-Ground-Transport-index-75\n",
      "2024-04-25 10:46:48,592 - Done_logger - INFO - Data for Porto-Transfers-and-Ground-Transport, Page 1 saved on disk\n",
      "2024-04-25 10:46:48,884 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n",
      "2024-04-25 10:46:49,106 - Done_logger - INFO - Data for Palermo-Global, Page 19 saved on disk\n",
      "2024-04-25 10:46:49,248 - Info_logger - INFO - CityProcessor-Porto-Sightseeing-Tickets-and-Passes-index-73\n",
      "2024-04-25 10:46:49,703 - Done_logger - INFO - Data for Porto-Sightseeing-Tickets-and-Passes, Page 2 saved on disk\n",
      "2024-04-25 10:46:49,862 - Info_logger - INFO - CityProcessor-Capri-Global-index-14\n",
      "2024-04-25 10:46:50,514 - Info_logger - INFO - CityProcessor-Pompeii-Global-index-13\n",
      "2024-04-25 10:46:50,686 - Info_logger - INFO - CityProcessor-Amalfi-Coast-Global-index-17\n",
      "2024-04-25 10:46:50,922 - Done_logger - INFO - Data for Capri-Global, Page 8 saved on disk\n",
      "2024-04-25 10:46:51,208 - Info_logger - INFO - CityProcessor-Porto-Seasonal-and-Special-Occasions-index-72\n",
      "2024-04-25 10:46:51,444 - Done_logger - INFO - Data for Pompeii-Global, Page 19 saved on disk\n",
      "2024-04-25 10:46:52,111 - Info_logger - INFO - CityProcessor-Porto-Outdoor-Activities-index-71\n",
      "2024-04-25 10:46:52,174 - Info_logger - INFO - CityProcessor-Krakow-Global-index-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:46:52,286 - Done_logger - INFO - Data for Amalfi-Coast-Global, Page 8 saved on disk\n",
      "2024-04-25 10:46:52,777 - Done_logger - INFO - Data for Porto-Seasonal-and-Special-Occasions, Page 2 saved on disk\n",
      "2024-04-25 10:46:53,220 - Done_logger - INFO - Data for Porto-Outdoor-Activities, Page 2 saved on disk\n",
      "2024-04-25 10:46:54,427 - Done_logger - INFO - Data for Krakow-Global, Page 17 saved on disk\n",
      "2024-04-25 10:46:54,615 - Info_logger - INFO - CityProcessor-Porto-Tours-and-Sightseeing-index-74\n",
      "2024-04-25 10:46:54,915 - Info_logger - INFO - Processing: Porto, Transfers-and-Ground-Transport, Page: 2 of max 2, URL: https://www.viator.com/Porto-tours/Transfers-and-Ground-Transport/d26879-g15/2\n",
      "2024-04-25 10:46:57,764 - Done_logger - INFO - Data for Porto-Tours-and-Sightseeing, Page 2 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:46:58,745 - Info_logger - INFO - Processing: Palermo, Global, Page: 20 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:00,981 - Info_logger - INFO - CityProcessor-Porto-Food-Wine-and-Nightlife-index-70\n",
      "2024-04-25 10:47:02,309 - Done_logger - INFO - Data for Porto-Food-Wine-and-Nightlife, Page 2 saved on disk\n",
      "2024-04-25 10:47:02,535 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Sightseeing-Tickets-and-Passes.csv\n",
      "2024-04-25 10:47:04,097 - Info_logger - INFO - CityProcessor-Las-Vegas-Global-index-76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:04,412 - Info_logger - INFO - CityProcessor-Munich-Global-index-41\n",
      "2024-04-25 10:47:07,454 - Info_logger - INFO - Processing: Pompeii, Global, Page: 20 of max 25, URL: https://www.viator.com/Pompeii/d24336-ttd/20\n",
      "2024-04-25 10:47:07,897 - Done_logger - INFO - Data for Las-Vegas-Global, Page 1 saved on disk\n",
      "2024-04-25 10:47:08,575 - Done_logger - INFO - Data for Munich-Global, Page 6 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n",
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n",
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:10,164 - Info_logger - INFO - Processing: Capri, Global, Page: 9 of max 9, URL: https://www.viator.com/Capri/d4223-ttd/9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:11,356 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Seasonal-and-Special-Occasions.csv\n",
      "2024-04-25 10:47:11,719 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Outdoor-Activities.csv\n",
      "2024-04-25 10:47:12,509 - Info_logger - INFO - Processing: Krakow, Global, Page: 18 of max 25, URL: https://www.viator.com/Krakow/d529-ttd/18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:14,184 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Tours-and-Sightseeing.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:15,351 - Info_logger - INFO - CityProcessor-Venice-Global-index-10\n",
      "2024-04-25 10:47:15,700 - Info_logger - INFO - Processing: Amalfi-Coast, Global, Page: 9 of max 25, URL: https://www.viator.com/Amalfi-Coast/d946-ttd/9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:16,046 - Done_logger - INFO - Data for Venice-Global, Page 52 saved on disk\n",
      "2024-04-25 10:47:16,078 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Food-Wine-and-Nightlife.csv\n",
      "2024-04-25 10:47:17,106 - Info_logger - INFO - CityProcessor-London-Global-index-16\n",
      "2024-04-25 10:47:17,945 - Done_logger - INFO - Data for London-Global, Page 9 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:23,236 - Info_logger - INFO - Processing: Las-Vegas, Global, Page: 2 of max 25, URL: https://www.viator.com/Las-Vegas/d684-ttd/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:23,619 - Info_logger - INFO - Processing: Munich, Global, Page: 7 of max 25, URL: https://www.viator.com/Munich/d487-ttd/7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:25,437 - Info_logger - INFO - Processing: Venice, Global, Page: 53 of max 55, URL: https://www.viator.com/Venice/d522-ttd/53\n",
      "2024-04-25 10:47:25,673 - Info_logger - INFO - CityProcessor-Sorrento-Global-index-15\n",
      "2024-04-25 10:47:25,896 - Done_logger - INFO - Data for Sorrento-Global, Page 9 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:26,246 - Info_logger - INFO - Processing: London, Global, Page: 10 of max 25, URL: https://www.viator.com/London/d737-ttd/10\n",
      "2024-04-25 10:47:26,691 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n",
      "2024-04-25 10:47:26,976 - Done_logger - INFO - Data for Palermo-Global, Page 20 saved on disk\n",
      "2024-04-25 10:47:29,404 - Info_logger - INFO - CityProcessor-Porto-Transfers-and-Ground-Transport-index-75\n",
      "2024-04-25 10:47:30,073 - Done_logger - INFO - Data for Porto-Transfers-and-Ground-Transport, Page 2 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:31,104 - Info_logger - INFO - Processing: Sorrento, Global, Page: 10 of max 25, URL: https://www.viator.com/Sorrento/d947-ttd/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:32,116 - Info_logger - INFO - Processing: Palermo, Global, Page: 21 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/21\n",
      "2024-04-25 10:47:32,768 - Info_logger - INFO - CityProcessor-Pompeii-Global-index-13\n",
      "2024-04-25 10:47:33,004 - Done_logger - INFO - Data for Pompeii-Global, Page 20 saved on disk\n",
      "2024-04-25 10:47:33,099 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Porto-Transfers-and-Ground-Transport.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:34,826 - Info_logger - INFO - CityProcessor-Capri-Global-index-14\n",
      "2024-04-25 10:47:35,123 - Done_logger - INFO - Data for Capri-Global, Page 9 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:35,375 - Info_logger - INFO - Processing: Pompeii, Global, Page: 21 of max 25, URL: https://www.viator.com/Pompeii/d24336-ttd/21\n",
      "2024-04-25 10:47:37,189 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Capri-Global.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:44,688 - Info_logger - INFO - CityProcessor-Krakow-Global-index-18\n",
      "2024-04-25 10:47:44,696 - Done_logger - INFO - Data for Krakow-Global, Page 18 saved on disk\n",
      "2024-04-25 10:47:46,174 - Info_logger - INFO - CityProcessor-Amalfi-Coast-Global-index-17\n",
      "2024-04-25 10:47:46,743 - Done_logger - INFO - Data for Amalfi-Coast-Global, Page 9 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:47,721 - Info_logger - INFO - Processing: Krakow, Global, Page: 19 of max 25, URL: https://www.viator.com/Krakow/d529-ttd/19\n",
      "2024-04-25 10:47:49,557 - Info_logger - INFO - CityProcessor-London-Global-index-16\n",
      "2024-04-25 10:47:49,777 - Done_logger - INFO - Data for London-Global, Page 10 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:50,060 - Info_logger - INFO - Processing: Amalfi-Coast, Global, Page: 10 of max 25, URL: https://www.viator.com/Amalfi-Coast/d946-ttd/10\n",
      "2024-04-25 10:47:52,634 - Info_logger - INFO - CityProcessor-Venice-Global-index-10\n",
      "2024-04-25 10:47:52,907 - Done_logger - INFO - Data for Venice-Global, Page 53 saved on disk\n",
      "2024-04-25 10:47:53,141 - Info_logger - INFO - Processing: London, Global, Page: 11 of max 25, URL: https://www.viator.com/London/d737-ttd/11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:53,914 - Info_logger - INFO - CityProcessor-Las-Vegas-Global-index-76\n",
      "2024-04-25 10:47:54,197 - Done_logger - INFO - Data for Las-Vegas-Global, Page 2 saved on disk\n",
      "2024-04-25 10:47:54,658 - Info_logger - INFO - CityProcessor-Sorrento-Global-index-15\n",
      "2024-04-25 10:47:55,223 - Done_logger - INFO - Data for Sorrento-Global, Page 10 saved on disk\n",
      "2024-04-25 10:47:55,494 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:47:56,171 - Info_logger - INFO - Processing: Venice, Global, Page: 54 of max 55, URL: https://www.viator.com/Venice/d522-ttd/54\n",
      "2024-04-25 10:47:56,234 - Done_logger - INFO - Data for Palermo-Global, Page 21 saved on disk\n",
      "2024-04-25 10:47:58,580 - Info_logger - INFO - CityProcessor-Pompeii-Global-index-13\n",
      "2024-04-25 10:47:59,544 - Done_logger - INFO - Data for Pompeii-Global, Page 21 saved on disk\n",
      "2024-04-25 10:48:00,783 - Info_logger - INFO - CityProcessor-Munich-Global-index-41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:01,778 - Done_logger - INFO - Data for Munich-Global, Page 7 saved on disk\n",
      "2024-04-25 10:48:02,429 - Info_logger - INFO - Processing: Las-Vegas, Global, Page: 3 of max 25, URL: https://www.viator.com/Las-Vegas/d684-ttd/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n",
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:05,555 - Info_logger - INFO - Processing: Sorrento, Global, Page: 11 of max 25, URL: https://www.viator.com/Sorrento/d947-ttd/11\n",
      "2024-04-25 10:48:05,824 - Info_logger - INFO - Processing: Palermo, Global, Page: 22 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:06,202 - Info_logger - INFO - Processing: Pompeii, Global, Page: 22 of max 25, URL: https://www.viator.com/Pompeii/d24336-ttd/22\n",
      "2024-04-25 10:48:07,535 - Info_logger - INFO - Processing: Munich, Global, Page: 8 of max 25, URL: https://www.viator.com/Munich/d487-ttd/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:10,857 - Info_logger - INFO - CityProcessor-Krakow-Global-index-18\n",
      "2024-04-25 10:48:10,863 - Done_logger - INFO - Data for Krakow-Global, Page 19 saved on disk\n",
      "2024-04-25 10:48:13,729 - Info_logger - INFO - Processing: Krakow, Global, Page: 20 of max 25, URL: https://www.viator.com/Krakow/d529-ttd/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:24,287 - Info_logger - INFO - CityProcessor-Venice-Global-index-10\n",
      "2024-04-25 10:48:24,295 - Done_logger - INFO - Data for Venice-Global, Page 54 saved on disk\n",
      "2024-04-25 10:48:27,482 - Info_logger - INFO - Processing: Venice, Global, Page: 55 of max 55, URL: https://www.viator.com/Venice/d522-ttd/55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:28,237 - Info_logger - INFO - CityProcessor-Sorrento-Global-index-15\n",
      "2024-04-25 10:48:28,245 - Done_logger - INFO - Data for Sorrento-Global, Page 11 saved on disk\n",
      "2024-04-25 10:48:30,945 - Info_logger - INFO - Processing: Sorrento, Global, Page: 12 of max 25, URL: https://www.viator.com/Sorrento/d947-ttd/12\n",
      "2024-04-25 10:48:31,022 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n",
      "2024-04-25 10:48:31,031 - Done_logger - INFO - Data for Palermo-Global, Page 22 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:31,727 - Info_logger - INFO - CityProcessor-Las-Vegas-Global-index-76\n",
      "2024-04-25 10:48:31,902 - Done_logger - INFO - Data for Las-Vegas-Global, Page 3 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:34,645 - Info_logger - INFO - Processing: Palermo, Global, Page: 23 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/23\n",
      "2024-04-25 10:48:34,679 - Info_logger - INFO - CityProcessor-Amalfi-Coast-Global-index-17\n",
      "2024-04-25 10:48:34,915 - Done_logger - INFO - Data for Amalfi-Coast-Global, Page 10 saved on disk\n",
      "2024-04-25 10:48:35,090 - Info_logger - INFO - CityProcessor-Pompeii-Global-index-13\n",
      "2024-04-25 10:48:35,628 - Done_logger - INFO - Data for Pompeii-Global, Page 22 saved on disk\n",
      "2024-04-25 10:48:36,622 - Info_logger - INFO - CityProcessor-Munich-Global-index-41\n",
      "2024-04-25 10:48:37,070 - Info_logger - INFO - CityProcessor-London-Global-index-16\n",
      "2024-04-25 10:48:37,652 - Done_logger - INFO - Data for Munich-Global, Page 8 saved on disk\n",
      "2024-04-25 10:48:37,859 - Done_logger - INFO - Data for London-Global, Page 11 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:38,908 - Info_logger - INFO - Processing: Las-Vegas, Global, Page: 4 of max 25, URL: https://www.viator.com/Las-Vegas/d684-ttd/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:44,439 - Info_logger - INFO - Processing: Pompeii, Global, Page: 23 of max 25, URL: https://www.viator.com/Pompeii/d24336-ttd/23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:46,341 - Info_logger - INFO - Processing: Amalfi-Coast, Global, Page: 11 of max 25, URL: https://www.viator.com/Amalfi-Coast/d946-ttd/11\n",
      "2024-04-25 10:48:47,834 - Info_logger - INFO - CityProcessor-Krakow-Global-index-18\n",
      "2024-04-25 10:48:47,839 - Info_logger - INFO - Processing: London, Global, Page: 12 of max 25, URL: https://www.viator.com/London/d737-ttd/12\n",
      "2024-04-25 10:48:47,844 - Info_logger - INFO - Processing: Munich, Global, Page: 9 of max 25, URL: https://www.viator.com/Munich/d487-ttd/9\n",
      "2024-04-25 10:48:47,844 - Done_logger - INFO - Data for Krakow-Global, Page 20 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:48,609 - Info_logger - INFO - CityProcessor-Venice-Global-index-10\n",
      "2024-04-25 10:48:48,800 - Done_logger - INFO - Data for Venice-Global, Page 55 saved on disk\n",
      "2024-04-25 10:48:51,478 - Info_logger - INFO - Processing: Krakow, Global, Page: 21 of max 25, URL: https://www.viator.com/Krakow/d529-ttd/21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:53,363 - Info_logger - INFO - Archived file to G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily/archive/2024-04-25-Venice-Global.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:48:57,700 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n",
      "2024-04-25 10:48:57,707 - Done_logger - INFO - Data for Palermo-Global, Page 23 saved on disk\n",
      "2024-04-25 10:49:00,265 - Info_logger - INFO - Processing: Palermo, Global, Page: 24 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:06,626 - Info_logger - INFO - CityProcessor-Las-Vegas-Global-index-76\n",
      "2024-04-25 10:49:06,634 - Done_logger - INFO - Data for Las-Vegas-Global, Page 4 saved on disk\n",
      "2024-04-25 10:49:09,303 - Info_logger - INFO - CityProcessor-Munich-Global-index-41\n",
      "2024-04-25 10:49:09,311 - Info_logger - INFO - Processing: Las-Vegas, Global, Page: 5 of max 25, URL: https://www.viator.com/Las-Vegas/d684-ttd/5\n",
      "2024-04-25 10:49:09,311 - Done_logger - INFO - Data for Munich-Global, Page 9 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:09,737 - Info_logger - INFO - CityProcessor-Amalfi-Coast-Global-index-17\n",
      "2024-04-25 10:49:09,909 - Done_logger - INFO - Data for Amalfi-Coast-Global, Page 11 saved on disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:14,949 - Info_logger - INFO - Processing: Munich, Global, Page: 10 of max 25, URL: https://www.viator.com/Munich/d487-ttd/10\n",
      "2024-04-25 10:49:15,368 - Info_logger - INFO - Processing: Amalfi-Coast, Global, Page: 12 of max 25, URL: https://www.viator.com/Amalfi-Coast/d946-ttd/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:15,658 - Info_logger - INFO - CityProcessor-Pompeii-Global-index-13\n",
      "2024-04-25 10:49:15,669 - Done_logger - INFO - Data for Pompeii-Global, Page 23 saved on disk\n",
      "2024-04-25 10:49:16,511 - Info_logger - INFO - CityProcessor-Sorrento-Global-index-15\n",
      "2024-04-25 10:49:16,715 - Done_logger - INFO - Data for Sorrento-Global, Page 12 saved on disk\n",
      "2024-04-25 10:49:16,996 - Info_logger - INFO - Processing: Sorrento, Global, Page: 13 of max 25, URL: https://www.viator.com/Sorrento/d947-ttd/13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:17,853 - Info_logger - INFO - Processing: Pompeii, Global, Page: 24 of max 25, URL: https://www.viator.com/Pompeii/d24336-ttd/24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:19,606 - Info_logger - INFO - CityProcessor-London-Global-index-16\n",
      "2024-04-25 10:49:19,616 - Done_logger - INFO - Data for London-Global, Page 12 saved on disk\n",
      "2024-04-25 10:49:22,645 - Info_logger - INFO - Processing: London, Global, Page: 13 of max 25, URL: https://www.viator.com/London/d737-ttd/13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:23,362 - Info_logger - INFO - CityProcessor-Krakow-Global-index-18\n",
      "2024-04-25 10:49:23,372 - Done_logger - INFO - Data for Krakow-Global, Page 21 saved on disk\n",
      "2024-04-25 10:49:26,452 - Info_logger - INFO - Processing: Krakow, Global, Page: 22 of max 25, URL: https://www.viator.com/Krakow/d529-ttd/22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:26,795 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n",
      "2024-04-25 10:49:26,804 - Done_logger - INFO - Data for Palermo-Global, Page 24 saved on disk\n",
      "2024-04-25 10:49:29,209 - Info_logger - INFO - Processing: Palermo, Global, Page: 25 of max 25, URL: https://www.viator.com/Palermo/d4815-ttd/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:35,960 - Info_logger - INFO - CityProcessor-Las-Vegas-Global-index-76\n",
      "2024-04-25 10:49:35,968 - Done_logger - INFO - Data for Las-Vegas-Global, Page 5 saved on disk\n",
      "2024-04-25 10:49:38,992 - Info_logger - INFO - Processing: Las-Vegas, Global, Page: 6 of max 25, URL: https://www.viator.com/Las-Vegas/d684-ttd/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 elements with exact 'data-automation=ttd-product-list-card' match.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:49:46,516 - Info_logger - INFO - CityProcessor-Sorrento-Global-index-15\n",
      "2024-04-25 10:49:46,525 - Done_logger - INFO - Data for Sorrento-Global, Page 13 saved on disk\n",
      "2024-04-25 10:49:47,516 - Info_logger - INFO - CityProcessor-Pompeii-Global-index-13\n",
      "2024-04-25 10:49:47,705 - Done_logger - INFO - Data for Pompeii-Global, Page 24 saved on disk\n",
      "2024-04-25 10:49:48,797 - Info_logger - INFO - CityProcessor-Krakow-Global-index-18\n",
      "2024-04-25 10:49:49,004 - Info_logger - INFO - CityProcessor-London-Global-index-16\n",
      "2024-04-25 10:49:49,145 - Done_logger - INFO - Data for Krakow-Global, Page 22 saved on disk\n",
      "2024-04-25 10:49:49,335 - Done_logger - INFO - Data for London-Global, Page 13 saved on disk\n",
      "2024-04-25 10:49:52,011 - Info_logger - INFO - CityProcessor-Palermo-Global-index-11\n",
      "2024-04-25 10:49:52,170 - Info_logger - INFO - CityProcessor-Munich-Global-index-41\n",
      "2024-04-25 10:49:52,960 - Done_logger - INFO - Data for Palermo-Global, Page 25 saved on disk\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        viator_day = daily_run_viator()\n",
    "        check_brake_option = check_if_all_csv_processed()\n",
    "        logger_info.info(check_brake_option)\n",
    "        if check_brake_option == 'brake':\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print('re-run not done yet')\n",
    "    except Exception as e:\n",
    "        handle_error_and_rerun(e)\n",
    "\n",
    "try:\n",
    "    combine_csv_to_xlsx()\n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)   \n",
    "    tb = traceback.format_exc()\n",
    "    logger_err.error('An error occurred: {} on {}'.format(str(e), tb))\n",
    "# # Call the function to upload the file to Azure Blob Storage\n",
    "try:\n",
    "    upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name)\n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)\n",
    "\n",
    "try:\n",
    "    transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)    \n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Call the function to upload the file to Azure Blob Storage\n",
    "# try:\n",
    "#     upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name)\n",
    "# except Exception as e:\n",
    "#     handle_error_and_rerun(e)\n",
    "\n",
    "# try:\n",
    "#     transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)    \n",
    "# except Exception as e:\n",
    "#     handle_error_and_rerun(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5313bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_links = pd.read_csv(link_file)\n",
    "# for index, row in df_links.iterrows():\n",
    "#     city = row['City']\n",
    "#     category = row['MatchCategory']\n",
    "#     city_path_done = fr'{output_viator}/{date_today}-{city}-{category}.csv'\n",
    "#     if os.path.exists(city_path_done):\n",
    "#         print(city, '-', category)\n",
    "#         city_done_msg = pd.read_csv(city_path_done)\n",
    "#         display(city_done_msg)\n",
    "#         for i, r in city_done_msg.iterrows():\n",
    "#             url = r['UrlResponse'].replace(',', '')\n",
    "#             print(url)\n",
    "#             as_start = time.time()\n",
    "#             results = requests.get(url)    \n",
    "#             print('Time: ', time.time() - as_start)\n",
    "#             print(results)\n",
    "#             print('_______________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c69bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# DEBUG error in output from ZEN\n",
    "\n",
    "# \"\"\"\n",
    "# df_links = pd.read_csv(link_file)\n",
    "# df_links = df_links.head(1)\n",
    "# for index, row in df_links.iterrows():\n",
    "#     print('Row processing: ', index)\n",
    "#     page = 1\n",
    "#     url_input = row[\"URL\"]\n",
    "#     city_input = row['City']\n",
    "#     category_input = row['MatchCategory']\n",
    "\n",
    "#     if category_input == 'Global':\n",
    "#         max_pages = 20\n",
    "#     else:\n",
    "#         max_pages = 2\n",
    "\n",
    "#     if city_input == 'Capri':\n",
    "#         max_pages = 9\n",
    "#     elif city_input == 'Taormina':\n",
    "#         max_pages = 6\n",
    "#     elif city_input == 'Lisbon' and category_input == 'Global':\n",
    "#         max_pages = 65\n",
    "#     elif city_input == 'Porto' and category_input == 'Global' :\n",
    "#         max_pages = 30\n",
    "\n",
    "\n",
    "#     # max_pages = 2\n",
    "\n",
    "#     city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#     city_path_done_archive = fr'{output_viator}/archive/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#     if os.path.exists(city_path_done):\n",
    "#         city_done_msg = pd.read_csv(city_path_done)\n",
    "#         page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page'].iloc[0]) + 1\n",
    "#     elif os.path.exists(city_path_done_archive):\n",
    "#         logger_done.info('City already in Archive folder moving further')\n",
    "#         df_links = df_links.drop(index)\n",
    "#         page = max_pages + 1\n",
    "#         continue\n",
    "                    \n",
    "\n",
    "# #         print(f'City: {city_input} category: {category_input} have page done {page} in file {city_path_done}')\n",
    "    \n",
    "\n",
    "#     while page <= max_pages:\n",
    "#         if page == 1:\n",
    "#             url = f'{url_input}'\n",
    "#         else:\n",
    "#             url = f'{url_input}/{page}'\n",
    "#         print(url)\n",
    "#         page += 1\n",
    "        \n",
    "# # CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#         print(city_input, category_input, url, 'Processing in ZEN')\n",
    "#         params = {\n",
    "#             'url': url,\n",
    "#             'apikey': API_KEY_ZENROWS,\n",
    "#             'js_render': 'true',\n",
    "#             'json_response': 'true',\n",
    "#             'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "#             'premium_proxy': 'true',\n",
    "#         }\n",
    "#         response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "#         # time.sleep(5)\n",
    "#         if response.status_code == 200:\n",
    "#                 try:\n",
    "#                     data_send_df = pd.DataFrame({\n",
    "#                         'UrlRequest': [url],\n",
    "#                         'City': city_input,\n",
    "#                         'Page': [page],\n",
    "#                         'Category': category_input\n",
    "#                     }, columns=['UrlRequest', 'City', 'Page', 'Category'])\n",
    "#                     display(data_send_df)\n",
    "#                     t = process_html_from_response_zenrows(response, city_input, category_input)\n",
    "#                     print('Data saved on disk')\n",
    "#                     data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     print(\"JSON could not be decoded\")\n",
    "#         else:\n",
    "#                 print(\"HTTP request returned code: \", response.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "#                 page +=1\n",
    "#     # shutil.move(city_path_done, city_path_done_archive)\n",
    "#     # logger_info.info((f'Archived file to {city_path_done_archive}'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6206d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = []\n",
    "# soup = BeautifulSoup(t.content, 'html.parser')       \n",
    "# tours = soup.select(\"[data-automation*=ttd-product-list-card]\")\n",
    "# print(response)\n",
    "# print(\"@@@@@@@@@@@@@@\\n\", response.content)\n",
    "# # Filter these elements to find those that exactly match your target attribute value\n",
    "# tour_items = [el for el in tours if el.get('data-automation') == r'\\\"ttd-product-list-card\\\"']\n",
    "# print(f\"Found {len(tour_items)} elements with exact 'data-automation=ttd-product-list-card' match.\")\n",
    "# if len(tour_items) > 0:\n",
    "#     for tour_item in tour_items:\n",
    "#     #                 page_pos = tour_item['data-action-page-properties']\n",
    "#     #                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "#     #                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "#         # position = position + 1\n",
    "#         title = tour_item.select_one(\"[data-automation*=ttd-product-list-card-title]\").get_text()\n",
    "#         price_container = tour_item.select_one(\"[data-automation*=ttd-product-list-card-price]\")\n",
    "#         price = price_container.select_one(\"[class*=currentPrice]\").text.strip().split('from')[-1]\n",
    "#         part_url = tour_item.select_one(\"[data-automation*=ttd-product-list-card-link]\").get('href').split('\"')[1].split('\\\\')[0]\n",
    "#         product_url = f\"https://www.viator.com{part_url}\"\n",
    "#         siteuse = 'Viator'\n",
    "\n",
    "# for i in tours:\n",
    "#     if i.get('data-automation') == r'\\\"ttd-product-list-card\\\"':\n",
    "#         print(i.select_one(\"[data-automation*=ttd-product-list-card-title]\").get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: \"Slow Loading Times Challenges in Efficiently Retrieving HTML Content\"\n",
    "# Description:\n",
    "# This issue revolves around the prolonged loading times experienced when using ScraperAPI to access websites. The process begins with sending a request to ScraperAPI, which in turn provides a URL response containing the HTML content of the desired website. However, the main challenge arises in the subsequent step, where the loading of this HTML content takes an excessively long time. This delay significantly hinders the efficiency of the data retrieval process, affecting the overall performance of applications reliant on timely data scraping. The goal is to identify and resolve the factors contributing to these slow loading times, ensuring a more streamlined and rapid data extraction experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79861bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e49339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
