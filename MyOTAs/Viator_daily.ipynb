{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from threading import Lock, current_thread\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3YmFiYWFiYTEwNWFkZDZiM2ZiYjlmZjNmZjVmZTNkY2E0Y2VkYTEiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiV29qdGVrIEJhbG9uIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGZCODM1WVhSalRJeEl4WmxyTnBaRXpWQk9hZmUyMUFmU1dZZXNnUGc9czk2LWMiLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vZXhhMi1mYjE3MCIsImF1ZCI6ImV4YTItZmIxNzAiLCJhdXRoX3RpbWUiOjE2ODY2NTg5MDYsInVzZXJfaWQiOiJEcWRXRDhRdloyUTkzcTR4WFhWWlFWUk8wSEMyIiwic3ViIjoiRHFkV0Q4UXZaMlE5M3E0eFhYVlpRVlJPMEhDMiIsImlhdCI6MTY4NjY1OTA2MSwiZXhwIjoxNjg2NjYyNjYxLCJlbWFpbCI6IndvamJhbDNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZ29vZ2xlLmNvbSI6WyIxMTUwNTc1NjgzNzI4NjQ1MzA0NTciXSwiZW1haWwiOlsid29qYmFsM0BnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJnb29nbGUuY29tIn19.IAOh_U2LXNXGk1jqG3q6m9utI79QVMDtCuUcDBSH5TEKPmMCEdW962qOZN6J8wfMzexHX1cWoqGcXYBmjLcjQKBhhQoAUAdYjxEivrLHe8Hi37bIwXrEX9mvAKD1wE71Sq1sbB3B9xU51lTsH88l7P0pq9LDgbaKkJCljvvzJ186BTbX9Qw0CF4gma1XjJ1W3Nmd0BK2pE9y0b3arF_V8bSME6BeR4Ls1yKLM9da-MCN5y-IkwGVB6j78Qrt-4_emtAhxjkcYlzauOtEM8dZ0NzblgSxY-hdG_sG-Clg0gM6fxXRQSQJYjqHNgwY7sjAP885JUWbtjWjoXKvdJn_iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb46920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "\n",
    "\n",
    "# date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# output_viator = r'output/Viator'\n",
    "# archive_folder = fr'{output_viator}/Archive'\n",
    "# file_path_done =fr'output/Viator/{date_today}-DONE-Viator.csv'  \n",
    "# file_path_output = fr\"output/Viator - {date_today}.xlsx\"\n",
    "# link_file = fr'resource/Viator_links.csv'\n",
    "# avg_file = fr'resource/avg-Viator.csv'\n",
    "# re_run_path = fr'output/Viator/{date_today}-ReRun-Viator.csv'\n",
    "# folder_path_with_txt_to_count_avg = 'Avg/Viator'\n",
    "# logs_path = fr'Logs/Viator'\n",
    "\n",
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2024-05-30'\n",
    "output_viator = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/Daily'\n",
    "archive_folder = fr'{output_viator}/Archive'\n",
    "file_path_done =fr'{output_viator}/{date_today}-DONE-Viator.csv'  \n",
    "file_path_output = fr\"{output_viator}/Viator - {date_today}.xlsx\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "max_page_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_max_page.csv'\n",
    "avg_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/avg-viator.csv'\n",
    "re_run_path = fr'{output_viator}/{date_today}-ReRun-Viator.csv'\n",
    "logs_path = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/Viator'\n",
    "# FOR ONE TIME USED NOT SYNCHORNIEZD WITH RUNING APPLCIATION\n",
    "folder_path_with_txt_to_count_avg = 'Avg/Viator'\n",
    "\n",
    "# Set the path of the local file\n",
    "local_file_path = f\"{output_viator}/Viator - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/daily/viator\"\n",
    "container_name_refined = \"refined/daily/viator\"\n",
    "\n",
    "blob_name = fr'Viator - {date_today}.xlsx'\n",
    "file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'ARS\\xa0': 'ARS (Argentine Peso)', 'ARS': 'ARS (Argentine Peso)',\n",
    "                    'A$': 'AUD (Australian Dollar)', 'SGD': 'SGD (Singapur Dolar)'}\n",
    "\n",
    "currency_list = []\n",
    "API_KEY_SCRAPERAPI = '8c36bc42cd11c738c1baad3e2000b40c' # https://dashboard.scraperapi.com/\n",
    "API_KEY_ZENROWS = '56ed5b7f827aa5c258b3f6d3f57d36999aa949e8' # https://app.zenrows.com/buildera\n",
    "file_write_lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62665a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger object\n",
    "logger_err = logging.getLogger('Error_logger')\n",
    "logger_err.setLevel(logging.DEBUG)\n",
    "logger_info = logging.getLogger('Info_logger')\n",
    "logger_info.setLevel(logging.DEBUG)\n",
    "logger_done = logging.getLogger('Done_logger')\n",
    "logger_done.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create file handler for error logs and set level to debug\n",
    "fh_error = logging.FileHandler(fr'{logs_path}/error_logs.log')\n",
    "fh_error.setLevel(logging.DEBUG)\n",
    "\n",
    "# create file handler for info logs and set level to info\n",
    "fh_info = logging.FileHandler(fr'{logs_path}/info_logs.log')\n",
    "fh_info.setLevel(logging.INFO)\n",
    "\n",
    "# create file handler for info logs and set level to info\n",
    "fh_done = logging.FileHandler(fr'{logs_path}/done_logs.log')\n",
    "fh_done.setLevel(logging.INFO)\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to handlers\n",
    "ch.setFormatter(formatter)\n",
    "fh_error.setFormatter(formatter)\n",
    "fh_info.setFormatter(formatter)\n",
    "fh_done.setFormatter(formatter)\n",
    "\n",
    "# add handlers to logger\n",
    "logger_err.addHandler(ch)\n",
    "logger_err.addHandler(fh_error)\n",
    "logger_info.addHandler(ch)\n",
    "logger_info.addHandler(fh_info)\n",
    "logger_done.addHandler(ch)\n",
    "logger_done.addHandler(fh_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63807628",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "    ''\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c25470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_error_and_rerun(error):\n",
    "#     recipient_error = 'wojbal3@gmail.com'\n",
    "    tb = traceback.format_exc()\n",
    "    logger_err.error('An error occurred: {} on {}'.format(str(error), tb))\n",
    "#     subject = f'Error occurred - {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}'\n",
    "#     message = f'<html><body><p>Error occurred: {str(error)} on {tb}</p></body></html>'\n",
    "#     send_email(subject, message, recipient_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c377fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rates(of_date, currency_code='EUR'):\n",
    "# USING API TO GET RATES FROM SITE https://fixer.io/documentation\n",
    "    res = requests.get(fr'http://data.fixer.io/api/{of_date}?access_key=acfed48df1159d37fa4305e5e95c234f&base={currency_code}')\n",
    "    rates = res.json()['rates']\n",
    "    return rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_to_xlsx():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    # Get all CSV files with the specified date prefix\n",
    "    csv_files = [file for file in os.listdir(f'{output_viator}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found with the date prefix '{date_today}'\")\n",
    "        return\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    output_file = f\"{output_viator}/Viator - {date_today}.xlsx\"\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_viator}', csv_file)\n",
    "        if 'Viator' not in csv_file:\n",
    "            continue\n",
    "        sheet_name = os.path.splitext(csv_file)[0]\n",
    "        sheet_name = sheet_name.split(date_today + '-')[1].split('-Viator')[0]\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Write the DataFrame to the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save the Excel file\n",
    "    # writer.save()\n",
    "    writer.close()\n",
    "\n",
    "    print(f\"Combined CSV files with date prefix '{date_today}' into '{output_file}'\")\n",
    "\n",
    "    # Remove the CSV files\n",
    "#     for csv_file in csv_files:\n",
    "#         os.remove(csv_file)\n",
    "    # Move the CSV files to the Archive folder\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_viator}', csv_file)\n",
    "        if 'DONE' in csv_file:\n",
    "            df_done = pd.read_csv(csv_path)\n",
    "            df_done = df_done.drop_duplicates(subset=['City', 'Category'])\n",
    "            df_done = df_done.drop(columns=['UrlRequest', 'UrlResponse', 'Status', 'Page'])\n",
    "            df_done['Date'] = date_today\n",
    "            df_done.to_csv(max_page_file, mode='a', index=False, header=False)\n",
    "        destination_path = os.path.join(archive_folder, csv_file)\n",
    "        shutil.move(csv_path, destination_path)\n",
    "        \n",
    "\n",
    "    print(f\"Moved {len(csv_files)} CSV file(s) to the '{archive_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_done(log_type):\n",
    "    global file_path_logs_processed\n",
    "    if log_type == 'Raw':\n",
    "        with open(f'{file_path_logs_processed}-raw.txt', 'w') as file:\n",
    "            file.write('Done')\n",
    "    elif log_type == 'Refined':\n",
    "        with open(f'{file_path_logs_processed}-refined.txt', 'w') as file:\n",
    "            file.write('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbd5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name):\n",
    "    try:\n",
    "        # Create a connection string to the Azure Storage account\n",
    "        connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "        # Create a BlobServiceClient object using the connection string\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "        # Get a reference to the container\n",
    "        container_client = blob_service_client.get_container_client(container_name_raw)\n",
    "\n",
    "        # Upload the file to Azure Blob Storage\n",
    "        with open(local_file_path, \"rb\") as file:\n",
    "            container_client.upload_blob(name=blob_name, data=file, )\n",
    "        create_log_done('Raw')\n",
    "        print(\"File uploaded successfully to Azure Blob Storage (raw).\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730dfcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name):\n",
    "    global mapping_currency\n",
    "    global date_today\n",
    "    global currency_list\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "    # Define the Azure Blob Storage connection details\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    # Read the Excel file into a Pandas DataFrame\n",
    "    rates_eur = get_rates(date_today, 'EUR')\n",
    "    rates_gbp = get_rates(date_today, 'EUR')\n",
    "    rates_usd = rates_eur\n",
    "    currency_not_found_list = []\n",
    "    currny_not_found = False\n",
    "#     GBP AND USD ARE NOT SUPORTED WITHING THIS CURRENT SUBSRICPTION UPGRADE PLAN\n",
    "#     rates_gbp = get_rates(date_today, 'GBP')\n",
    "\n",
    "#     rates_usd = get_rates(date_today, 'USD')\n",
    "    excel_data = pd.read_excel(local_file_path, sheet_name=None)\n",
    "    output_file_path = \"temp_file.xlsx\"\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            if sheet_name in exclude_sheets:\n",
    "                continue\n",
    "            if sheet_name == 'Mt-Vesuvius':\n",
    "                sheet_name = 'Mount-Vesuvius'\n",
    "                df['Miasto'] = 'Mount-Vesuvius'\n",
    "            # Make changes to the df DataFrame as needed\n",
    "            df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "            df['IloscOpini'] = df['IloscOpini'].fillna(0) \n",
    "            df['Opinia'] = df['Opinia'].fillna('N/A')\n",
    "            df = df[df['Tytul'] != 'Tytul']\n",
    "            df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "            df = df[df['Data zestawienia'].str.len() > 4]\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace('\\\\\"', '', regex=True)\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace('\\\"', '', regex=True)\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace(r'\\\\', '', regex=True)\n",
    "            df['IloscOpini'] = df['IloscOpini'].astype(str).str.replace(',','')\n",
    "            df['Pozycja'] = df.groupby('Kategoria').cumcount() + 1\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                currency = ''\n",
    "                if 'per group' in row['Cena']:\n",
    "                    df.at[index, 'Cena'] = row['Cena'].split('per group')[0]\n",
    "                    row['Cena']= row['Cena'].split('per group')[0]\n",
    "                for i in range(0,10):\n",
    "                    if not row['Cena'][i].isnumeric():\n",
    "                        currency = currency + (row['Cena'][i])\n",
    "                    else:\n",
    "                        if row['Cena'][i] == '¹':\n",
    "                            currency = currency + (row['Cena'][i])\n",
    "                            continue\n",
    "                        price = float(row['Cena'][i:].split()[0].replace(',',''))\n",
    "                        total_price = row['Cena']\n",
    "                        break\n",
    "    #             print(currency)\n",
    "                if sheet_name in EUR_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(\"Currency mapping not found for: \",currency,\" in \", sheet_name)\n",
    "                        currny_not_found = True\n",
    "                        currency_not_found_list.append(currency)\n",
    "                elif sheet_name in GBP_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_gbp[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(\"Currency mapping not found for: \",currency,\" in \", sheet_name)\n",
    "                        currny_not_found = True\n",
    "                        currency_not_found_list.append(currency)\n",
    "                elif sheet_name in USD_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_usd[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(\"Currency mapping not found for: \",currency,\" in \", sheet_name)\n",
    "                        currny_not_found = True\n",
    "                        currency_not_found_list.append(currency)\n",
    "    #             print(f'{mapping_currency[currency[:3]][0:3]} conversion rate: {conversion_rate}')\n",
    "    #             print(f'{total_price}- price: {price} - covnersion: {price/(conversion_rate*1.020)}')\n",
    "                df.at[index, 'Cena'] = round(price/(conversion_rate*1.0185), 2)\n",
    "                currency_list.append(currency)\n",
    "\n",
    "            currency_list = list(set(currency_list))\n",
    "            if currny_not_found:\n",
    "                logger_done.info(currency_not_found_list)\n",
    "                print('Curreny not found: ', currency_not_found_list)\n",
    "    #         display(df)\n",
    "\n",
    "    #         df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "            df.drop(columns=['Przecena', 'Tekst'], inplace=True)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Create a connection to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name_refined)\n",
    "\n",
    "    # Upload the modified Excel file to Azure Blob Storage\n",
    "    with open(output_file_path, \"rb\") as data:\n",
    "        container_client.upload_blob(name=blob_name, data=data)\n",
    "        \n",
    "    print(\"File uploaded successfully to Azure Blob Storage (refined).\")\n",
    "    os.remove(output_file_path)\n",
    "    create_log_done('Refined')\n",
    "    return 'Added to Blob'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def currency_switcher(currency_code_text, driver):\n",
    "    wait_currency = WebDriverWait(driver,2) \n",
    "#     try:\n",
    "#         currency_code_element = wait_currency.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '[data-automation*=\"EUR\"]')))\n",
    "#         # Click on the EUR element\n",
    "#         currency_code_element.click()\n",
    "#         time.sleep(5)\n",
    "#     except:\n",
    "#         try:\n",
    "#             driver.find_element(By.CSS_SELECTOR, 'div[class*=\"menuContent\"]').find_element(By.CSS_SELECTOR, 'button[data-automation=\"currency\"]').click()\n",
    "#         except:\n",
    "    driver.find_element(By.CSS_SELECTOR, 'div[class*=\"menu-container\"]').find_element(By.CSS_SELECTOR, '[data-action-page-properties*=\"currency\"]').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "#         currency_code = driver.find_elements(By.CSS_SELECTOR, '[data-automation=\"header-code\"]')\n",
    "#     if len(currency_code) == 0:\n",
    "    currency_code = driver.find_elements(By.CSS_SELECTOR, '[data-action-tag=\"select_currency_modal\"]')\n",
    "\n",
    "    for item in currency_code:\n",
    "            if currency_code_text in item.text:\n",
    "                item.click()\n",
    "                time.sleep(15)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_amount_data():\n",
    "    global date_today\n",
    "    global output_gyg\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    #     date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#     xls = pd.ExcelFile(fr\"{output_viator}/Viator - 2023-05-31.xlsx\")\n",
    "    xls = pd.ExcelFile(fr\"{output_viator}/Viator - {date_today}.xlsx\")\n",
    "    #     link_file = fr'resource/GYG_links.csv'\n",
    "    #     avg_file = fr'resource/avg-gyg.csv'\n",
    "    #     re_run_path = fr'output/GYG/{date_today} - ReRun GYG.csv'\n",
    "    df_links = pd.read_csv(link_file)\n",
    "    df_avg = pd.read_csv(avg_file)\n",
    "    re_run_data = []\n",
    "\n",
    "    city_to_get_data = df_links['City'].drop_duplicates().tolist()\n",
    "    for excel_sheet_name in city_to_get_data:\n",
    "    #     Check if the all excel files which are in df_links are available in created excel file\n",
    "        if excel_sheet_name in xls.sheet_names:\n",
    "    #         for viator to change \n",
    "\n",
    "    #         Data collected it's loaded excel file for selected city\n",
    "            data_collected = xls.parse(sheet_name=excel_sheet_name)\n",
    "            if excel_sheet_name == 'Mt-Vesuvius':\n",
    "                excel_sheet_name = 'Mount-Vesuvius'\n",
    "            amount_of_data_collected = len(data_collected)\n",
    "            print(excel_sheet_name, amount_of_data_collected)\n",
    "            avg_value_city = int(df_avg[df_avg['City'] == excel_sheet_name]['Avg'])\n",
    "            if abs(amount_of_data_collected - avg_value_city)/avg_value_city > 0.15 :\n",
    "                if amount_of_data_collected < avg_value_city:\n",
    "                    print(abs(amount_of_data_collected - avg_value_city), excel_sheet_name, amount_of_data_collected, avg_value_city)\n",
    "    #                     logger_done.info(abs(amount_of_data_collected - avg_value_city), excel_sheet_name, amount_of_data_collected, avg_value_city)\n",
    "                category_to_get = df_links[(df_links['City'] == excel_sheet_name)]['MatchCategory'].tolist()\n",
    "                category_collected = data_collected['Kategoria'].drop_duplicates().tolist()\n",
    "    #             display(data_collected.groupby('Kategoria')['Kategoria'].count())\n",
    "                for category_name in category_to_get:\n",
    "                    if category_name in category_collected:\n",
    "                        pass\n",
    "                    else:\n",
    "    #                     If the category is missing in the excel sheet add it to re-run data\n",
    "                        print(f'Missing {category_name} for {excel_sheet_name}')\n",
    "                        re_run_data.append([excel_sheet_name, category_name])\n",
    "    #                 FOR TESTING\n",
    "    #                 re_run_data.append([excel_sheet_name, category_name])\n",
    "    #                 re_run_data.append([excel_sheet_name, 'all'])\n",
    "    #     If the excel sheet is missing add it to re-run data\n",
    "        else:\n",
    "            print(f'Missing {excel_sheet_name} in data')\n",
    "            re_run_data.append([excel_sheet_name, 'all'])\n",
    "    if len(re_run_data) > 0:\n",
    "        pd.DataFrame(re_run_data).to_csv(re_run_path, index=False, header=['City', 'Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_avg_data_required():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    # COUNT AVG PER CITY \n",
    "    # Initialize variables\n",
    "    city_counts = []\n",
    "    total_rows = 0\n",
    "    result = []\n",
    "    # Iterate over each text file in the directory\n",
    "    for file_name in os.listdir(folder_path_with_txt_to_count_avg):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path_with_txt_to_count_avg, file_name)\n",
    "\n",
    "            # Open the text file\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Extract the city name using regular expressions\n",
    "                city_list = re.findall(r'\\d+ - ([^\\n]+).', content)\n",
    "                count_list = re.findall(r'\\d+ rows', content)\n",
    "\n",
    "                for item1, item2 in zip(city_list, count_list):\n",
    "                    joined = str(item1) + ' ' + str(item2.split(' ')[0])\n",
    "                    result.append(joined)\n",
    "\n",
    "                for row in result:\n",
    "                    city = row.split(' ')[0]\n",
    "\n",
    "                    # Extract the row count using regular expressions\n",
    "                    count_match = row.split(' ')[1]\n",
    "                    count = int(count_match)\n",
    "                    # Add the city and row count to the list\n",
    "                    city_counts.append((city, count))\n",
    "\n",
    "                    # Update the total row count\n",
    "                    total_rows += count\n",
    "    city_population = {}\n",
    "\n",
    "    # Store population values for each city\n",
    "    for city, row_count in city_counts:\n",
    "        if city in city_population:\n",
    "            city_population[city].append(row_count)\n",
    "        else:\n",
    "            city_population[city] = [row_count]\n",
    "\n",
    "    # Calculate average population for each city\n",
    "    city_avg = {}\n",
    "    for city, population_list in city_population.items():\n",
    "        city_avg[city] = round(sum(population_list) / len(population_list),0)\n",
    "\n",
    "    # Print average population for each city\n",
    "    #     report_str+= f\"{city} - {round(avg, 0)}\"\n",
    "    avg_path_viator = 'resource/avg-viator.csv'\n",
    "    # with open(avg_path_viator, \"w\") as f:\n",
    "    #                 f.write(report_str)\n",
    "    df = pd.DataFrame(city_avg.items(), columns=['City', 'Avg'])\n",
    "    df.to_csv(avg_path_viator, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### FOR RE-RUN PREPARATION\n",
    "# def re_run_daily():\n",
    "#     global re_run_path\n",
    "#     global link_file\n",
    "#     global archive_folder\n",
    "#     #     re_run_path = fr'output/GYG/2023-05-31-ReRun-GYG.csv'\n",
    "#     if os.path.exists(re_run_path):\n",
    "#         df_re_run = pd.read_csv(re_run_path)\n",
    "#         df_links = pd.read_csv(link_file)\n",
    "#         mergded_df_re_run = pd.merge(df_links,df_re_run, how='right', on=('City'))\n",
    "\n",
    "#         for index, row in mergded_df_re_run.iterrows():\n",
    "#             if row['Category_y'] == 'all':\n",
    "#                 continue\n",
    "#             if row['Category_y'] != row['MatchCategory']:\n",
    "#                 mergded_df_re_run.drop(index=index, inplace=True)\n",
    "\n",
    "#         daily_run_viator(mergded_df_re_run, True)\n",
    "#     else:\n",
    "#         print('No missing categories or cities')\n",
    "\n",
    "    \n",
    "# #     NOT DONE DATA IS NOT BEING INSERTED TO EXCEL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f5c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_city_data(df_links):\n",
    "#     cities_to_process = []\n",
    "\n",
    "#     for city in df_links['City'].unique():\n",
    "#         city_path_done = fr'{output_viator}/{date_today}-{city}.csv'\n",
    "#         if os.path.exists(city_path_done):\n",
    "#             print(city)\n",
    "#             city_done_msg = pd.read_csv(city_path_done)\n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "#         # Request the status of all URLs at once\n",
    "#         city_done_msg['Status'] = city_done_msg['UrlResponse'].apply(lambda url: requests.get(url=url).json()['status'])\n",
    "\n",
    "#         # Check if all statuses are finished\n",
    "#         if all(city_done_msg['Status'] == 'finished'):\n",
    "#             print('All finished')\n",
    "#             df_links = df_links[df_links['City'] != city]\n",
    "#             try:\n",
    "#                 position = df_links[df_links['City'] == city]['Page'] * 24\n",
    "#             except:\n",
    "#                 position = 0\n",
    "            \n",
    "#             max_page_for_city = get_max_pages(city_done_msg.iloc[0]['UrlResponse'])\n",
    "#             city_done_msg['MaxPage'] = max_page_for_city\n",
    "#             city_done_msg.to_csv(file_path_done, header=not os.path.exists(file_path_done), index=False, mode='a')\n",
    "#             cities_to_process.append(city)\n",
    "#             process_html_from_response_scraperapi(city_done_msg, position)\n",
    "            \n",
    "# #             os.remove(city_path_done)\n",
    "\n",
    "#     return df_links, cities_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fe4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_url_to_process_scraperapi(url_input, city_input, category_input, page = 1, max_pages = 25):\n",
    "#     global date_today\n",
    "#     global output_viator\n",
    "#     global file_path_done\n",
    "#     global file_path_output\n",
    "#     global avg_file\n",
    "#     global re_run_path\n",
    "#     global folder_path_with_txt_to_count_avg\n",
    "#     global archive_folder\n",
    "#     data = []\n",
    "#     city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#     if city_input == 'Capri':\n",
    "#         max_pages = 9\n",
    "#     elif city_input == 'Taormina':\n",
    "#         max_pages = 6\n",
    "#     elif city_input == 'Lisbon' and category_input == 'Global':\n",
    "#         max_pages = 100\n",
    "#     elif city_input == 'Porto' and category_input == 'Global' :\n",
    "#         max_pages = 40\n",
    "        \n",
    "#     if os.path.exists(city_path_done):\n",
    "#         city_done_msg = pd.read_csv(city_path_done)\n",
    "#         page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page']) + 1\n",
    "# #         print(f'City: {city_input} category: {category_input} have page done {page} in file {city_path_done}')\n",
    "        \n",
    "    \n",
    "#     url_time = time.time()\n",
    "#     while page <= max_pages:\n",
    "#         if page == 1:\n",
    "#             url = f'{url_input}'\n",
    "#         else:\n",
    "#             url = f'{url_input}/{page}'\n",
    "#         print(url)\n",
    "\n",
    "#         country_codes = [\n",
    "#             'us',\n",
    "#             'eu'\n",
    "#             ]\n",
    "\n",
    "#         random_country_code = random.choice(country_codes)\n",
    "        \n",
    "# # CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#         print(random_country_code)\n",
    "#         params = {\n",
    "#             'url': url,\n",
    "#             'apikey': API_KEY_ZENROWS,\n",
    "#             'js_render': 'true',\n",
    "#             'json_response': 'true',\n",
    "#             'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "#             'premium_proxy': 'true',\n",
    "#         }\n",
    "#         response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "#         url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "#                                     json={'apiKey': f'{API_KEY_ZENROWS}', \n",
    "#                                           'country_code': random_country_code,\n",
    "#                                           'url': url })\n",
    "# #         time.sleep(5)\n",
    "#         if url_request.status_code == 200:\n",
    "#             try:\n",
    "#                 print(url_request.json()['statusUrl'])\n",
    "#                 status_url = url_request.json()['statusUrl']\n",
    "#                 data_send_df = pd.DataFrame({\n",
    "#                     'UrlRequest': [url],\n",
    "#                     'UrlResponse': [status_url],\n",
    "#                     'City': [city_input],\n",
    "#                     'Page': [page],\n",
    "#                     'Category': category_input\n",
    "#                 }, columns=['UrlRequest', 'UrlResponse', 'City', 'Page', 'Category'])\n",
    "#                 data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(\"JSON could not be decoded\")\n",
    "#         else:\n",
    "#             print(\"HTTP request returned code: \", url_request.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "#             page -=1\n",
    "\n",
    "\n",
    "# # IN THE TEXT FILE ADD URL AND STATUS AND WHICH PAGE IS IT RELATED TO \n",
    "        \n",
    "#         page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6927d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_from_response_zenrows(response, city, category, position = 0, DEBUG=False):    \n",
    "    data = []\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')       \n",
    "    tours = soup.select(\"[data-automation*=ttd-product-list-card]\")\n",
    "    if DEBUG:\n",
    "        print(response)\n",
    "    # Filter these elements to find those that exactly match your target attribute value\n",
    "    tour_items = [el for el in tours if el.get('data-automation') == r'\\\"ttd-product-list-card\\\"']\n",
    "    print(f\"Found {len(tour_items)} elements with exact 'data-automation=ttd-product-list-card' match.\")\n",
    "    if len(tour_items) > 0:\n",
    "        for tour_item in tour_items:\n",
    "        #                 page_pos = tour_item['data-action-page-properties']\n",
    "        #                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "        #                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "            position = position + 1\n",
    "            if DEBUG:\n",
    "                print(tour_item.content)\n",
    "            title = tour_item.select_one(\"[data-automation*=ttd-product-list-card-title]\").get_text()\n",
    "            price_container = tour_item.select_one(\"[data-automation*=ttd-product-list-card-price]\")\n",
    "            price = price_container.select_one(\"[class*=currentPrice]\").text.strip().split('from')[-1]\n",
    "            try:\n",
    "                part_url = tour_item.select_one(\"[data-automation*=ttd-product-list-card-link]\").get('href').split('\"')[1].split('\\\\')[0]\n",
    "            except:\n",
    "                try:\n",
    "                    part_url = tour_item['href'].split('\"')[1].split('\\\\')[0]\n",
    "                except:\n",
    "                    logger_err.error(f\"No able to find the HREF for {title}, moving further\")\n",
    "                    part_url = \"\"\n",
    "                    \n",
    "            product_url = f\"https://www.viator.com{part_url}\"\n",
    "            siteuse = 'Viator'\n",
    "            try:\n",
    "                discount = price_container.select_one(\"[class*=discountInfoContainer]\").select_one(\"[class*=originalPrice]\").text.strip()\n",
    "            except:\n",
    "                discount = 'N/A'\n",
    "\n",
    "            amount_reviews = 'N/A'\n",
    "            #NUMBER OF REVIEWS\n",
    "            try:\n",
    "                amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            try:\n",
    "                star_int = 0\n",
    "                stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "                half_star = 'M14'\n",
    "                for st in stars_grouped:\n",
    "                    path_text = str(st.find('path')['d'])\n",
    "                    if half_star in path_text:\n",
    "                        star_int = star_int + 0.5\n",
    "                    else:\n",
    "                        if '0a.77.77' in str(st):\n",
    "                            star_int = star_int + 1\n",
    "                stars = f'star-{str(star_int)}'\n",
    "            except:\n",
    "                stars = 'N/A'\n",
    "\n",
    "            text = tour_item.text.strip()\n",
    "                \n",
    "            data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "    else:\n",
    "        tour_items = soup.select(\"[class*=productListCardWithDebug]\")\n",
    "#             print('Running using debug HTML')\n",
    "        for tour_item in tour_items:\n",
    "            position = position + 1\n",
    "            title = tour_item.select_one(\"[class*=title]\").text.strip()\n",
    "            price = tour_item.select_one(\"[class*=currentPrice]\").text.strip()\n",
    "            if 'from' in price:\n",
    "                price = price.split('from')[1]\n",
    "            splitter = price[0]\n",
    "            product_url = f\"https://www.viator.com{tour_item.find('a')['href']}\"\n",
    "            siteuse = 'Viator'\n",
    "            star =\"M7.5 0a.77.77 0 00-.701.456L5.087 4.083a.785.785 0 01-.588.448l-3.827.582a.828.828 0 00-.433 1.395L3.008 9.33c.185.192.26\"\n",
    "            half =\"M14.761 6.507a.828.828 0 00-.433-1.395L10.5 4.53a.785.785 0 01-.589-.447L8.201.456a.767.767 0 00-1.402 0L5.087 4.083a.785\"\n",
    "            nostar =\"M7.5 1.167l1.565 3.317c.242.52.728.885 1.295.974l3.583.544-2.62 2.673a1.782 1.782 0 00-.48 1.532l.609 3.718L8.315 12.2a1.6\"\n",
    "            try:\n",
    "                discount = tour_item.select_one(\"[class*=savingsLabel]\").text.strip()\n",
    "            except:\n",
    "                discount = 'N/A'\n",
    "            try:\n",
    "                amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "            except:\n",
    "                amount_reviews = 'N/A'\n",
    "            try:\n",
    "                star_int = 0\n",
    "                stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "                half_star = 'M14'\n",
    "                for st in stars_grouped:\n",
    "                    path_text = str(st.find('path')['d'])\n",
    "                    if half_star in path_text:\n",
    "                        star_int = star_int + 0.5\n",
    "                    else:\n",
    "                        if '0a.77.77' in str(st):\n",
    "                            star_int = star_int + 1\n",
    "                stars = f'star-{str(star_int)}'\n",
    "            except:\n",
    "                stars = 'N/A'\n",
    "            text = tour_item.text.strip()\n",
    "\n",
    "            data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "    # print(f'URL: {city} currency: {splitter}')\n",
    "    url_done = time.time()\n",
    "    # message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page Currency: 1-{first_style_curr}, 2-{second_style_curr}, 3-{thirtd_style_curr}'\n",
    "    # print(message)\n",
    "    # logger_info.info(message)\n",
    "    df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto'])\n",
    "    df['Pozycja'] = df.groupby('Kategoria').cumcount() + 1\n",
    "    file_path = fr'{output_viator}/{date_today}-{city}-Viator.csv' \n",
    "    df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_city(row, thread_name = None):\n",
    "    \n",
    "    global date_today, output_viator, API_KEY_ZENROWS\n",
    "    \n",
    "    if thread_name:\n",
    "        current_thread().name = thread_name\n",
    "    page = 1\n",
    "    url_input = row[\"URL\"]\n",
    "    city_input = row['City']\n",
    "    category_input = row['MatchCategory']\n",
    "    max_pages = calculate_max_pages(city_input, category_input)\n",
    "\n",
    "    city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'\n",
    "    city_path_done_archive = fr'{output_viator}/archive/{date_today}-{city_input}-{category_input}.csv'\n",
    "    \n",
    "    if os.path.exists(city_path_done):\n",
    "        city_done_msg = pd.read_csv(city_path_done)\n",
    "        page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page'].iloc[0]) + 1\n",
    "        logger_info.info(f'Resuming {city_input} from page {page}')\n",
    "    elif os.path.exists(city_path_done_archive):\n",
    "        logger_done.info('City already in Archive folder moving further')\n",
    "        return\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        url = f'{url_input}' if page == 1 else f'{url_input}/{page}'\n",
    "        logger_info.info(f'Processing: {city_input}, {category_input}, Page: {page} of max {max_pages}, URL: {url}')\n",
    "        response = make_request(url)\n",
    "        logger_info.info(current_thread().name)\n",
    "        if response and response.status_code == 200:\n",
    "            try:\n",
    "                save_data(response, city_input, category_input, url, page, city_path_done)\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger_err.error(f'JSON could not be decoded for URL: {url}, error: {str(e)}')\n",
    "                raise\n",
    "        else:\n",
    "            # Log the error with the status code and response content\n",
    "            logger_err.error(f'HTTP request failed for city: {city_input} category: {category_input} page: {page} with status code {response.status_code}  Decrement the page count. Content: {response.content}')\n",
    "            page -= 1\n",
    "            # Specific handling for 403 and 429 status codes\n",
    "            if response.status_code == 403:\n",
    "                logger_err.error(f'{current_thread().name}: IP Address Blocked, sleeping for 5 minutes before retrying.')\n",
    "                time.sleep(300)  # Wait for 5 minutes before retrying\n",
    "            elif response.status_code == 429:\n",
    "                logger_err.error(f'{current_thread().name}: Concurrency limit exceeded , sleeping for 5 minutes before retrying.')\n",
    "                time.sleep(300)  # Wait for 5 minutes before retrying\n",
    "            else:\n",
    "                logger_err.error(f'Status code did not set for {response.status_code}')\n",
    "        page += 1\n",
    "    \n",
    "    shutil.move(city_path_done, city_path_done_archive)\n",
    "    logger_info.info((f'Archived file to {city_path_done_archive}'))\n",
    "\n",
    "\n",
    "def calculate_max_pages(city_input, category_input):\n",
    "    if city_input == 'Capri':\n",
    "        return 9\n",
    "    if city_input == 'Taormina':\n",
    "        return 6\n",
    "    if city_input == 'Lisbon' and category_input == 'Global':\n",
    "        return 65\n",
    "    if city_input == 'Porto' and category_input == 'Global':\n",
    "        return 30\n",
    "    if city_input == 'Venice' and category_input == 'Global':\n",
    "        return 55\n",
    "    return 25 if category_input == 'Global' else 2\n",
    "\n",
    "def make_request(url):\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'apikey': API_KEY_ZENROWS,\n",
    "        'js_render': 'true',\n",
    "        'json_response': 'true',\n",
    "        'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "        'premium_proxy': 'true',\n",
    "    }\n",
    "    return requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "def save_data(response, city_input, category_input, url, page, city_path_done):\n",
    "    try:\n",
    "        data_send_df = pd.DataFrame({\n",
    "            'UrlRequest': [url],\n",
    "            'City': city_input,\n",
    "            'Page': [page],\n",
    "            'Category': category_input\n",
    "        }, columns=['UrlRequest', 'City', 'Page', 'Category'])\n",
    "        data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "        logger_done.info(f'Data for {city_input}-{category_input}, Page {page} saved on disk')\n",
    "        process_html_from_response_zenrows(response, city_input, category_input)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON could not be decoded\")\n",
    "\n",
    "def send_url_to_process_zenrows(df_links):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = {}\n",
    "        for index, row in df_links.iterrows():\n",
    "            thread_name = f\"CityProcessor-{row['City']}-{index}\"\n",
    "            futures[executor.submit(process_city, row, thread_name=thread_name)] = row\n",
    "\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "# The rest of your global variables and helper functions should be defined outside of these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4982fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_url_to_process_zenrows(df_links):\n",
    "#     global date_today\n",
    "#     global output_viator\n",
    "#     global file_path_done\n",
    "#     global file_path_output\n",
    "#     global avg_file\n",
    "#     global re_run_path\n",
    "#     global folder_path_with_txt_to_count_avg\n",
    "#     global archive_folder\n",
    "\n",
    "#     for index, row in df_links.iterrows():\n",
    "#         print('Row processing: ', index)\n",
    "#         page = 1\n",
    "#         url_input = row[\"URL\"]\n",
    "#         city_input = row['City']\n",
    "#         category_input = row['MatchCategory']\n",
    "#         max_pages = calculate_max_pages(city_input, category_input)\n",
    "\n",
    "#         city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#         city_path_done_archive = fr'{output_viator}/archive/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#         if os.path.exists(city_path_done):\n",
    "#             city_done_msg = pd.read_csv(city_path_done)\n",
    "#             page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page'].iloc[0]) + 1\n",
    "#         elif os.path.exists(city_path_done_archive):\n",
    "#             logger_done.info('City already in Archive folder moving further')\n",
    "#             df_links = df_links.drop(index)\n",
    "#             page = max_pages + 1\n",
    "#             continue\n",
    "                        \n",
    "\n",
    "# #         print(f'City: {city_input} category: {category_input} have page done {page} in file {city_path_done}')\n",
    "        \n",
    "\n",
    "#         while page <= max_pages:\n",
    "#             if page == 1:\n",
    "#                 url = f'{url_input}'\n",
    "#             else:\n",
    "#                 url = f'{url_input}/{page}'\n",
    "#             print(url)\n",
    "#             page += 1\n",
    "            \n",
    "#     # CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#             print(city_input, category_input, url, 'Processing in ZEN')\n",
    "#             params = {\n",
    "#                 'url': url,\n",
    "#                 'apikey': API_KEY_ZENROWS,\n",
    "#                 'js_render': 'true',\n",
    "#                 'json_response': 'true',\n",
    "#                 'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "#                 'premium_proxy': 'true',\n",
    "#             }\n",
    "#             response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "#             # time.sleep(5)\n",
    "#             if response.status_code == 200:\n",
    "#                     try:\n",
    "#                         data_send_df = pd.DataFrame({\n",
    "#                             'UrlRequest': [url],\n",
    "#                             'City': city_input,\n",
    "#                             'Page': [page],\n",
    "#                             'Category': category_input\n",
    "#                         }, columns=['UrlRequest', 'City', 'Page', 'Category'])\n",
    "#                         data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "#                         print('Data saved on disk, processing to extract data')\n",
    "#                         process_html_from_response_zenrows(response, city_input, category_input)\n",
    "#                     except json.JSONDecodeError:\n",
    "#                         print(\"JSON could not be decoded\")\n",
    "#             else:\n",
    "#                     print(\"HTTP request returned code: \", response.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "#                     page -=1\n",
    "#         shutil.move(city_path_done, city_path_done_archive)\n",
    "#         logger_info.info((f'Archived file to {city_path_done_archive}'))\n",
    "\n",
    "\n",
    "#     # IN THE TEXT FILE ADD URL AND STATUS AND WHICH PAGE IS IT RELATED TO \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# def get_max_pages(session, url, retries=5, backoff_factor=0.6):\n",
    "#     retry_wait = backoff_factor\n",
    "    \n",
    "#     for attempt in range(1, retries + 1):\n",
    "#         try:\n",
    "#             results = session.get(url, timeout=10 + (retry_wait) + 5)  # 10 seconds timeout\n",
    "#             results.raise_for_status()  # Raises an error for 4XX or 5XX status codes\n",
    "#         except requests.exceptions.HTTPError as http_err:\n",
    "#             logging.error(f'HTTP error occurred for {url}: {http_err}')\n",
    "#         except requests.exceptions.ConnectionError as conn_err:\n",
    "#             logging.error(f'Connection error occurred for {url}: {conn_err}')\n",
    "#         except requests.exceptions.Timeout as timeout_err:\n",
    "#             logging.error(f'Timeout error occurred for {url}: {timeout_err}')\n",
    "#         except requests.exceptions.RequestException as err:\n",
    "#             logging.error(f'Unexpected error occurred for {url}: {err}')\n",
    "        \n",
    "#         if attempt < retries:\n",
    "#             logging.info(f'Waiting {retry_wait} seconds before retrying...')\n",
    "#             time.sleep(retry_wait)\n",
    "#             retry_wait *= 2  # Exponential backoff\n",
    "            \n",
    "#     soup = BeautifulSoup(results.content, 'html.parser')\n",
    "#     product_list_count = None\n",
    "\n",
    "#     # Try finding the productListCount label using two different CSS selectors\n",
    "#     selectors = [\"[id*=productListCount]\", \"h3[class*=productListCount]\", \"h2[class*=productListCountLabel]\"]\n",
    "\n",
    "#     for selector in selectors:\n",
    "#         count_element = soup.select_one(selector)\n",
    "#         if count_element:\n",
    "#             product_list_count = int(count_element.text.split()[0].replace(',', ''))\n",
    "#             break\n",
    "\n",
    "#     if product_list_count is None:\n",
    "#         print(\"Product count not found in the HTML content.\")\n",
    "#         return None\n",
    "#     try:\n",
    "#         max_pages = int(round(product_list_count / 24, 0))\n",
    "#         return max_pages\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error while fetching HTML content: {e}\")\n",
    "#         return 25  # Return a default value of 25 pages if there's an error\n",
    "    \n",
    "# # Define the get_status function with a retry mechanism\n",
    "# def get_status(session, url, retries=7, backoff_factor=0.9):\n",
    "#     retry_wait = backoff_factor\n",
    "#     for attempt in range(1, retries + 1):\n",
    "#         try:\n",
    "#             logging.info(f'Attempt {attempt}: Sending request to {url}')\n",
    "#             response = session.get(url, timeout=10 + retry_wait + 5)  # 10 seconds timeout\n",
    "#             response.raise_for_status()  # Raises an error for 4XX or 5XX status codes\n",
    "#             status = response.json().get('status')\n",
    "#             logging.info(f'Response received: {url} - Status: {status}')\n",
    "#             return url, status\n",
    "#         except requests.exceptions.HTTPError as http_err:\n",
    "#             logging.error(f'HTTP error occurred for {url}: {http_err}')\n",
    "#         except requests.exceptions.ConnectionError as conn_err:\n",
    "#             logging.error(f'Connection error occurred for {url}: {conn_err}')\n",
    "#         except requests.exceptions.Timeout as timeout_err:\n",
    "#             logging.error(f'Timeout error occurred for {url}: {timeout_err}')\n",
    "#         except requests.exceptions.RequestException as err:\n",
    "#             logging.error(f'Unexpected error occurred for {url}: {err}')\n",
    "        \n",
    "#         if attempt < retries:\n",
    "#             logging.info(f'Waiting {retry_wait} seconds before retrying...')\n",
    "#             time.sleep(retry_wait)\n",
    "#             retry_wait *= 2  # Exponential backoff\n",
    "\n",
    "#     # If all retries fail, return 'error' status\n",
    "#     logging.error(f'All attempts failed for {url}. Marking status as error.')\n",
    "#     return url, 'error'\n",
    "\n",
    "# def check_status_and_process_city_data(df_links):\n",
    "#     cities_to_process = []\n",
    "#     session = requests.Session()\n",
    "#     statuses = {}\n",
    "    \n",
    "    \n",
    "#     for index, row in df_links.iterrows():\n",
    "#         city = row['City']\n",
    "#         category = row['MatchCategory']\n",
    "#         city_path_done = fr'{output_viator}/{date_today}-{city}-{category}.csv'\n",
    "        \n",
    "        \n",
    "#         if os.path.exists(city_path_done):\n",
    "#             print(city, '-', category)\n",
    "#             city_done_msg = pd.read_csv(city_path_done)\n",
    "#             city_done_msg.drop_duplicates(inplace=True)\n",
    "#         else:\n",
    "#             continue\n",
    "            \n",
    "        \n",
    "#         start_time_get_resposne = time.time()\n",
    "#         # Using ThreadPoolExecutor to fetch statuses\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#             future_to_url = {executor.submit(get_status, session, url): url for url in city_done_msg['UrlResponse']}\n",
    "#             for future in concurrent.futures.as_completed(future_to_url):\n",
    "#                 url = future_to_url[future]\n",
    "#                 try:\n",
    "#                     _, status = future.result()\n",
    "#                     statuses[url] = status  # Store the status with the URL as the key\n",
    "#                 except Exception as exc:\n",
    "#                     logging.error(f'{url} generated an exception: {exc}')\n",
    "                    \n",
    "#          # Update the DataFrame outside of the loop\n",
    "#         for url, status in statuses.items():\n",
    "#             city_done_msg.loc[city_done_msg['UrlResponse'] == url, 'Status'] = status\n",
    "#         end_time_get_resposne = time.time()\n",
    "#         print(f'First option with concurrent: {round(end_time_get_resposne-start_time_get_resposne,2)}s')\n",
    "#         print(f\"For {city} finished {len(city_done_msg[city_done_msg['Status'] == 'finished'])} from {len(city_done_msg)}\")\n",
    "#         # Check if all statuses are finished\n",
    "#         if len(city_done_msg[city_done_msg['Status'] == 'finished']) == len(city_done_msg):\n",
    "#             try:\n",
    "#                 position = df_links[df_links['City'] == city]['Page'] * 24\n",
    "#             except:\n",
    "#                 position = 0\n",
    "\n",
    "#             max_page_for_city = get_max_pages(session, city_done_msg.iloc[0]['UrlResponse'])\n",
    "#             city_done_msg['MaxPage'] = max_page_for_city\n",
    "#             process_html_from_response_scraperapi(city_done_msg, city_path_done,  position)\n",
    "#             df_links = df_links[(df_links['City'] != city) & (df_links['MatchCategory'] != category)]\n",
    "#             print(f'In check_status_and_process_city_data finished process removing: {city} - {category} ')\n",
    "            \n",
    "#     return df_links, cities_to_process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c2d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def process_html_from_response_scraperapi(data_city_df, city_path_done, position = 0):\n",
    "# # data_city_df = pd.read_csv(city_path_done)\n",
    "#     data = []\n",
    "# ### OPTION IF THE BELOW WILL NOT WORK PROPELRY CHECK BELOW   \n",
    "#     session = requests.Session()  # Using a Session object for connection pooling\n",
    "    \n",
    "#     # Set up retry strategy with backoff factor\n",
    "#     retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
    "#     session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "#     session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    \n",
    "#     for index, row in data_city_df.iterrows():\n",
    "#         url = row['UrlResponse'].replace(',', '')\n",
    "        \n",
    "#         try:\n",
    "#             logging.info(f'{index} - Starting request for URL: {url}')\n",
    "#             start_time = time.time()\n",
    "#             results = session.get(url, timeout=10)  # Set a reasonable timeout\n",
    "#             logging.info(F\"Time: {time.time() - start_time}\")\n",
    "#             results.raise_for_status()  # This will raise an exception for HTTP error codes\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             # Handle all requests-related exceptions\n",
    "#             logging.error(f'Request exception for URL {url}: {e}')        \n",
    "        \n",
    "#         soup = BeautifulSoup(results.content, 'html.parser')       \n",
    "#         tour_items = soup.select(\"[id*=productName]\")\n",
    "\n",
    "#         if len(tour_items) > 0:\n",
    "#             for tour_item in tour_items:\n",
    "# #                 page_pos = tour_item['data-action-page-properties']\n",
    "# #                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "# #                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "#                 position = position + 1\n",
    "#                 title = tour_item.find('h2').text.strip()\n",
    "#                 splitter = tour_item.text.split('From')[-1][0]\n",
    "#                 price = splitter + tour_item.text.split('From')[-1].split(splitter)[1]\n",
    "#                 if len(price) > 9:\n",
    "#                     price = price.split('Price')[0]\n",
    "#                 part_url = tour_item['data-url'].split('\"')[1].split('\\\\')[0]\n",
    "#                 product_url = f\"https://www.viator.com{part_url}\"\n",
    "#                 siteuse = 'Viator'\n",
    "#                 city = row['City']\n",
    "#                 category = row['Category']\n",
    "#                 try:\n",
    "#                     discount = tour_item.find('div', {'class': 'text-special product-list-card-savings-label'}).text.strip()\n",
    "#                 except:\n",
    "#                     discount = 'N/A'\n",
    "\n",
    "#                 amount_reviews = 'N/A'\n",
    "#                 #NUMBER OF REVIEWS\n",
    "#                 spans = tour_item.select('span')\n",
    "#                 for span in spans:\n",
    "#         #             print('________________________')\n",
    "#         #             print(span.attrs)\n",
    "#                     try:\n",
    "#                         span['reviewlink']\n",
    "#                         amount_reviews = span.text\n",
    "#                         break\n",
    "#                     except:\n",
    "#                         pass\n",
    "\n",
    "#                 try:\n",
    "#                     stars = tour_item.find('svg').text.strip()\n",
    "#                 except:\n",
    "#                     stars = 'N/A'\n",
    "\n",
    "#                 text = tour_item.text.strip()\n",
    "\n",
    "\n",
    "#                 data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "#         else:\n",
    "#             tour_items = soup.select(\"[class*=productListCardWithDebug]\")\n",
    "# #             print('Running using debug HTML')\n",
    "#             for tour_item in tour_items:\n",
    "#                 position = position + 1\n",
    "#                 title = tour_item.select_one(\"[class*=title]\").text.strip()\n",
    "#                 price = tour_item.select_one(\"[class*=currentPrice]\").text.strip()\n",
    "#                 if 'from' in price:\n",
    "#                     price = price.split('from')[1]\n",
    "#                 splitter = price[0]\n",
    "#                 product_url = f\"https://www.viator.com{tour_item.find('a')['href']}\"\n",
    "#                 siteuse = 'Viator'\n",
    "#                 city = row['City']\n",
    "#                 category = row['Category']\n",
    "\n",
    "#                 star =\"M7.5 0a.77.77 0 00-.701.456L5.087 4.083a.785.785 0 01-.588.448l-3.827.582a.828.828 0 00-.433 1.395L3.008 9.33c.185.192.26\"\n",
    "#                 half =\"M14.761 6.507a.828.828 0 00-.433-1.395L10.5 4.53a.785.785 0 01-.589-.447L8.201.456a.767.767 0 00-1.402 0L5.087 4.083a.785\"\n",
    "#                 nostar =\"M7.5 1.167l1.565 3.317c.242.52.728.885 1.295.974l3.583.544-2.62 2.673a1.782 1.782 0 00-.48 1.532l.609 3.718L8.315 12.2a1.6\"\n",
    "#                 try:\n",
    "#                     discount = tour_item.select_one(\"[class*=savingsLabel]\").text.strip()\n",
    "#                 except:\n",
    "#                     discount = 'N/A'\n",
    "#                 try:\n",
    "#                     amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "#                 except:\n",
    "#                     amount_reviews = 'N/A'\n",
    "#                 try:\n",
    "#                     star_int = 0\n",
    "#                     stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "#                     half_star = 'M14'\n",
    "#                     for st in stars_grouped:\n",
    "#                         path_text = str(st.find('path')['d'])\n",
    "#                         if half_star in path_text:\n",
    "#                             star_int = star_int + 0.5\n",
    "#                         else:\n",
    "#                             if '0a.77.77' in str(st):\n",
    "#                                 star_int = star_int + 1\n",
    "#                     stars = f'star-{str(star_int)}'\n",
    "#                 except:\n",
    "#                     stars = 'N/A'\n",
    "#                 text = tour_item.text.strip()\n",
    "\n",
    "#                 data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "#         print(f'URL: {city} currency: {splitter}')\n",
    "#     url_done = time.time()\n",
    "#     # message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page Currency: 1-{first_style_curr}, 2-{second_style_curr}, 3-{thirtd_style_curr}'\n",
    "#     # print(message)\n",
    "#     # logger_info.info(message)\n",
    "#     df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto'])\n",
    "#     df['Pozycja'] = df.groupby('Kategoria').cumcount() + 1\n",
    "#     file_path = fr'{output_viator}/{date_today}-{city}-Viator.csv' \n",
    "#     df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')\n",
    "#     data_city_df.to_csv(file_path_done, header=not os.path.exists(file_path_done), index=False, mode='a')\n",
    "#     os.remove(city_path_done)\n",
    "# #     row.to_csv(file_path_done, header=True, index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22b407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_run_viator(df_links=pd.DataFrame(), re_run=False):\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    if len(df_links) == 0:\n",
    "        df_links = pd.read_csv(link_file)\n",
    "    EUR_City = [\n",
    "        \"Amsterdam\", \"Athens\", \"Barcelona\", \"Berlin\", \"Dublin\", \"Dubrovnik\", \"Florence\", \"Istanbul\",\n",
    "        \"Krakow\", \"Lisbon\", \"Madrid\", \"Milan\", \"Naples\", \"Paris\", \"Porto\", \"Rome\", \"Palermo\", \"Venice\",\n",
    "        \"Taormina\", \"Capri\", \"Sorrento\", \"Mount-Etna\", \"Mount-Vesuvius\", \"Herculaneum\", \"Amalfi-Coast\",\n",
    "        \"Pompeii\"\n",
    "    ]\n",
    "\n",
    "    USD_City = [\n",
    "        \"Las-Vegas\", \"New-York-City\", \"Cancun\", \"Dubai\"\n",
    "    ]\n",
    "\n",
    "    GBP_City = [\n",
    "        \"Edinburgh\", \"London\"\n",
    "    ]\n",
    "\n",
    "#     date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_path_done =fr'output/Viator/{date_today}-DONE-Viator.csv'  \n",
    "#     file_path_output = f\"output/Viator - {date_today}.xlsx\"\n",
    "    if os.path.exists(file_path_output) and re_run == False:\n",
    "        print(f'Today ({date_today}) Viator done')\n",
    "        return 'Done'\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.exists(file_path_done) and re_run == False:\n",
    "        \n",
    "        done_msg = pd.read_csv(file_path_done).drop_duplicates(subset=['City', 'Category'], keep='last').reset_index()\n",
    "#         display(df_links)\n",
    "#         df_links = df_links[~(df_links['City'].isin(done_msg['City']) & df_links['MatchCategory'].isin(done_msg['Category']))]\n",
    "        merged = df_links.merge(done_msg, left_on=['City', 'MatchCategory'], right_on=['City', 'Category'], how='left', indicator=True)\n",
    "        # Filter rows where '_merge' is 'left_only', which means the combination is not present in done_msg\n",
    "        filtered = merged[merged['_merge'] == 'left_only']\n",
    "        # Drop the _merge column and reset index\n",
    "        filtered = filtered.drop(columns='_merge').reset_index(drop=True)\n",
    "        df_links = filtered\n",
    "#         df_links = df_links[~df_links['City'].isin(done_msg['City'].values)]\n",
    "        df_links_with_page_maxpage = df_links[df_links['City'].isin(done_msg['City'].values)]\n",
    "        df_links_with_page_maxpage = pd.merge(df_links_with_page_maxpage, done_msg[['City', 'Page', 'MaxPage']], on='City', how='left')\n",
    "    elif re_run == True:\n",
    "        print(f'Lenght of links: {len(df_links)}')\n",
    "    else:\n",
    "        logger_info.info(\"Nothing done yet\")\n",
    "\n",
    "    # Define the URL of the website we want to scrape\n",
    "    start_time = time.time()\n",
    "    if len(df_links) == 0:\n",
    "        print('Df_links empty')\n",
    "        return 'Done'\n",
    "    df_links = df_links[df_links['Priority'] > 0]\n",
    "    send_url_to_process_zenrows(df_links)\n",
    "    # print('Finished sending data to scraperapi')\n",
    "        \n",
    "#     display(df_links)\n",
    "#     while not df_links.empty:\n",
    "# #         display(df_links)\n",
    "#         df_links, processed_cities = check_status_and_process_city_data(df_links)\n",
    "#         print(f'After processing one row in df_links the df_links is {len(df_links)}')\n",
    "# #         display(df_links)\n",
    "        \n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c382fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_all_csv_processed():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    # Get all CSV files with the specified date prefix    \n",
    "    csv_files = [file for file in os.listdir(f'{output_viator}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "    csv_files_not_finished = []\n",
    "    for csv in csv_files:\n",
    "        if 'viator' not in csv.lower():\n",
    "            csv_files_not_finished.append(csv)\n",
    "\n",
    "\n",
    "    if len(csv_files_not_finished) == 0:\n",
    "        return 'brake'\n",
    "    else:\n",
    "        return f\"Files to process: {len(csv_files_not_finished)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e60ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        viator_day = daily_run_viator()\n",
    "        check_brake_option = check_if_all_csv_processed()\n",
    "        logger_info.info(check_brake_option)\n",
    "        if check_brake_option == 'brake':\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print('re-run not done yet')\n",
    "    except Exception as e:\n",
    "        handle_error_and_rerun(e)\n",
    "\n",
    "try:\n",
    "    combine_csv_to_xlsx()\n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)   \n",
    "\n",
    "    tb = traceback.format_exc()\n",
    "    logger_err.error('An error occurred: {} on {}'.format(str(e), tb))\n",
    "# # Call the function to upload the file to Azure Blob Storage\n",
    "try:\n",
    "    upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name)\n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)\n",
    "\n",
    "try:\n",
    "    transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)    \n",
    "except Exception as e:\n",
    "    handle_error_and_rerun(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Call the function to upload the file to Azure Blob Storage\n",
    "# try:\n",
    "#     upload_excel_to_azure_storage_account(local_file_path, storage_account_name, storage_account_key, container_name_raw, blob_name)\n",
    "# except Exception as e:\n",
    "#     handle_error_and_rerun(e)\n",
    "\n",
    "# try:\n",
    "#     transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)    \n",
    "# except Exception as e:\n",
    "#     handle_error_and_rerun(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5313bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_links = pd.read_csv(link_file)\n",
    "# for index, row in df_links.iterrows():\n",
    "#     city = row['City']\n",
    "#     category = row['MatchCategory']\n",
    "#     city_path_done = fr'{output_viator}/{date_today}-{city}-{category}.csv'\n",
    "#     if os.path.exists(city_path_done):\n",
    "#         print(city, '-', category)\n",
    "#         city_done_msg = pd.read_csv(city_path_done)\n",
    "#         display(city_done_msg)\n",
    "#         for i, r in city_done_msg.iterrows():\n",
    "#             url = r['UrlResponse'].replace(',', '')\n",
    "#             print(url)\n",
    "#             as_start = time.time()\n",
    "#             results = requests.get(url)    \n",
    "#             print('Time: ', time.time() - as_start)\n",
    "#             print(results)\n",
    "#             print('_______________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c69bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# DEBUG error in output from ZEN\n",
    "\n",
    "# \"\"\"\n",
    "# df_links = pd.read_csv(link_file)\n",
    "# df_links = df_links.head(1)\n",
    "# for index, row in df_links.iterrows():\n",
    "#     print('Row processing: ', index)\n",
    "#     page = 1\n",
    "#     url_input = row[\"URL\"]\n",
    "#     city_input = row['City']\n",
    "#     category_input = row['MatchCategory']\n",
    "\n",
    "#     if category_input == 'Global':\n",
    "#         max_pages = 20\n",
    "#     else:\n",
    "#         max_pages = 2\n",
    "\n",
    "#     if city_input == 'Capri':\n",
    "#         max_pages = 9\n",
    "#     elif city_input == 'Taormina':\n",
    "#         max_pages = 6\n",
    "#     elif city_input == 'Lisbon' and category_input == 'Global':\n",
    "#         max_pages = 65\n",
    "#     elif city_input == 'Porto' and category_input == 'Global' :\n",
    "#         max_pages = 30\n",
    "\n",
    "\n",
    "#     # max_pages = 2\n",
    "\n",
    "#     city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#     city_path_done_archive = fr'{output_viator}/archive/{date_today}-{city_input}-{category_input}.csv'  \n",
    "#     if os.path.exists(city_path_done):\n",
    "#         city_done_msg = pd.read_csv(city_path_done)\n",
    "#         page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page'].iloc[0]) + 1\n",
    "#     elif os.path.exists(city_path_done_archive):\n",
    "#         logger_done.info('City already in Archive folder moving further')\n",
    "#         df_links = df_links.drop(index)\n",
    "#         page = max_pages + 1\n",
    "#         continue\n",
    "                    \n",
    "\n",
    "# #         print(f'City: {city_input} category: {category_input} have page done {page} in file {city_path_done}')\n",
    "    \n",
    "\n",
    "#     while page <= max_pages:\n",
    "#         if page == 1:\n",
    "#             url = f'{url_input}'\n",
    "#         else:\n",
    "#             url = f'{url_input}/{page}'\n",
    "#         print(url)\n",
    "#         page += 1\n",
    "        \n",
    "# # CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#         print(city_input, category_input, url, 'Processing in ZEN')\n",
    "#         params = {\n",
    "#             'url': url,\n",
    "#             'apikey': API_KEY_ZENROWS,\n",
    "#             'js_render': 'true',\n",
    "#             'json_response': 'true',\n",
    "#             'js_instructions': \"\"\"[{\"click\":\".selector\"},{\"wait\":500},{\"fill\":[\".input\",\"value\"]},{\"wait_for\":\".slow_selector\"}]\"\"\",\n",
    "#             'premium_proxy': 'true',\n",
    "#         }\n",
    "#         response = requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "#         # time.sleep(5)\n",
    "#         if response.status_code == 200:\n",
    "#                 try:\n",
    "#                     data_send_df = pd.DataFrame({\n",
    "#                         'UrlRequest': [url],\n",
    "#                         'City': city_input,\n",
    "#                         'Page': [page],\n",
    "#                         'Category': category_input\n",
    "#                     }, columns=['UrlRequest', 'City', 'Page', 'Category'])\n",
    "#                     display(data_send_df)\n",
    "#                     t = process_html_from_response_zenrows(response, city_input, category_input)\n",
    "#                     print('Data saved on disk')\n",
    "#                     data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     print(\"JSON could not be decoded\")\n",
    "#         else:\n",
    "#                 print(\"HTTP request returned code: \", response.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "#                 page +=1\n",
    "#     # shutil.move(city_path_done, city_path_done_archive)\n",
    "#     # logger_info.info((f'Archived file to {city_path_done_archive}'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6206d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = []\n",
    "# soup = BeautifulSoup(t.content, 'html.parser')       \n",
    "# tours = soup.select(\"[data-automation*=ttd-product-list-card]\")\n",
    "# print(response)\n",
    "# print(\"@@@@@@@@@@@@@@\\n\", response.content)\n",
    "# # Filter these elements to find those that exactly match your target attribute value\n",
    "# tour_items = [el for el in tours if el.get('data-automation') == r'\\\"ttd-product-list-card\\\"']\n",
    "# print(f\"Found {len(tour_items)} elements with exact 'data-automation=ttd-product-list-card' match.\")\n",
    "# if len(tour_items) > 0:\n",
    "#     for tour_item in tour_items:\n",
    "#     #                 page_pos = tour_item['data-action-page-properties']\n",
    "#     #                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "#     #                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "#         # position = position + 1\n",
    "#         title = tour_item.select_one(\"[data-automation*=ttd-product-list-card-title]\").get_text()\n",
    "#         price_container = tour_item.select_one(\"[data-automation*=ttd-product-list-card-price]\")\n",
    "#         price = price_container.select_one(\"[class*=currentPrice]\").text.strip().split('from')[-1]\n",
    "#         part_url = tour_item.select_one(\"[data-automation*=ttd-product-list-card-link]\").get('href').split('\"')[1].split('\\\\')[0]\n",
    "#         product_url = f\"https://www.viator.com{part_url}\"\n",
    "#         siteuse = 'Viator'\n",
    "\n",
    "# for i in tours:\n",
    "#     if i.get('data-automation') == r'\\\"ttd-product-list-card\\\"':\n",
    "#         print(i.select_one(\"[data-automation*=ttd-product-list-card-title]\").get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: \"Slow Loading Times Challenges in Efficiently Retrieving HTML Content\"\n",
    "# Description:\n",
    "# This issue revolves around the prolonged loading times experienced when using ScraperAPI to access websites. The process begins with sending a request to ScraperAPI, which in turn provides a URL response containing the HTML content of the desired website. However, the main challenge arises in the subsequent step, where the loading of this HTML content takes an excessively long time. This delay significantly hinders the efficiency of the data retrieval process, affecting the overall performance of applications reliant on timely data scraping. The goal is to identify and resolve the factors contributing to these slow loading times, ensuring a more streamlined and rapid data extraction experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79861bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e49339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
