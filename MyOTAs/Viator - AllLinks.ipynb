{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fc006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import xlsxwriter\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "# eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3YmFiYWFiYTEwNWFkZDZiM2ZiYjlmZjNmZjVmZTNkY2E0Y2VkYTEiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiV29qdGVrIEJhbG9uIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGZCODM1WVhSalRJeEl4WmxyTnBaRXpWQk9hZmUyMUFmU1dZZXNnUGc9czk2LWMiLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vZXhhMi1mYjE3MCIsImF1ZCI6ImV4YTItZmIxNzAiLCJhdXRoX3RpbWUiOjE2ODY2NTg5MDYsInVzZXJfaWQiOiJEcWRXRDhRdloyUTkzcTR4WFhWWlFWUk8wSEMyIiwic3ViIjoiRHFkV0Q4UXZaMlE5M3E0eFhYVlpRVlJPMEhDMiIsImlhdCI6MTY4NjY1OTA2MSwiZXhwIjoxNjg2NjYyNjYxLCJlbWFpbCI6IndvamJhbDNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZ29vZ2xlLmNvbSI6WyIxMTUwNTc1NjgzNzI4NjQ1MzA0NTciXSwiZW1haWwiOlsid29qYmFsM0BnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJnb29nbGUuY29tIn19.IAOh_U2LXNXGk1jqG3q6m9utI79QVMDtCuUcDBSH5TEKPmMCEdW962qOZN6J8wfMzexHX1cWoqGcXYBmjLcjQKBhhQoAUAdYjxEivrLHe8Hi37bIwXrEX9mvAKD1wE71Sq1sbB3B9xU51lTsH88l7P0pq9LDgbaKkJCljvvzJ186BTbX9Qw0CF4gma1XjJ1W3Nmd0BK2pE9y0b3arF_V8bSME6BeR4Ls1yKLM9da-MCN5y-IkwGVB6j78Qrt-4_emtAhxjkcYlzauOtEM8dZ0NzblgSxY-hdG_sG-Clg0gM6fxXRQSQJYjqHNgwY7sjAP885JUWbtjWjoXKvdJn_iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2023-10-19'\n",
    "date_yesterday = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "output_viator = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/All Links'\n",
    "file_path_done =fr'{output_viator}/{date_today}-DONE-Viator.csv'  \n",
    "archive_folder = fr'{output_viator}/Archive'\n",
    "\n",
    "file_path_done_archive =fr'{archive_folder}/{date_yesterday}-DONE-Viator.csv'  \n",
    "file_path_output = fr\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "file_path_output_processed = fr\"{output_viator}/All Links Viator - {date_today}.xlsx\"\n",
    "file_path_output_processed_csv = fr\"{output_viator}/All Links Viator - {date_today}.csv\"\n",
    "file_path_csv_operator = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv\"\n",
    "file_path_all_links_send_to_scraper = fr\"{output_viator}\\SupplierExtract - {date_today}.csv\"\n",
    "file_path_all_links_send_to_scraper_finished = fr\"{output_viator}\\SupplierExtractFinished - {date_today}.csv\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "all_links_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/AllViator_links.csv'\n",
    "# Set the path of the local file\n",
    "local_file_path = file_path_output\n",
    "# local_file_path = f\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/all_links/viator\"\n",
    "container_name_refined = \"refined/all_links/viator\"\n",
    "\n",
    "blob_name = fr'Viator - {date_today}.xlsx'\n",
    "# file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'â‚´': 'UAH (Ukrainian Hryvnia)', 'zÅ‚': 'PLN (Polish Zloty)',\n",
    "                    'Ð»Ð²': 'BGN Bulgarian Lev', 'US$': 'USD (United States Dollar)', 'lei': 'RON (Romanian Leu)',\n",
    "                    'zł': 'PLN (Polish Zloty)','$U': 'UYU (Uruguayan Peso)', 'COL$': 'COP (Colombian Peso)', \n",
    "                    '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'zł': 'PLN (Polish Zloty)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    'CL$': 'CLP (Chilean Peso)', 'Rp': 'IDR (Indonesian Rupiah)', 'AR$': 'ARS (Argentine Peso)',\n",
    "                    '฿': 'THB (Thai Baht)', 'Kč': 'CZK (Czech Koruna)', 'lei': 'RON (Romanian Leu)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'A$': 'AUD (Australian Dollar)', 'Ft': 'HUF (Hungarian Forint)',\n",
    "                    '€': 'EUR (Euro)', '£': 'GBP (British Pound Sterling)', '₹': 'INR (Indian Rupee)',\n",
    "                    'US$': 'USD (United States Dollar)', 'лв': 'BGN (Bulgarian Lev)',\n",
    "                    'COL$': 'COP (Colombian Peso)', 'lei': 'RON (Romanian Leu)', 'C$': 'NIO (Nicaraguan Cordoba)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'AR$': 'ARS (Argentine Peso)', 'A$': 'AUD (Australian Dollar)',\n",
    "                    'лв': 'BGN (Bulgarian Lev)', 'Ft': 'HUF (Hungarian Forint)', 'DKK': 'DKK (Danish Krone)',\n",
    "                    '₪': 'ILS (Israeli Shekel)', '€.': 'EUR (Euro)', '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'R$': 'BRL (Brazilian Real)', '₹': 'INR (Indian Rupee)', 'zł': 'PLN (Polish Zloty)',\n",
    "                    'US$': 'USD (United States Dollar)', '€': 'EUR (Euro)', '$U': 'UYU (Uruguayan Peso)',\n",
    "                    'Kč': 'CZK (Czech Koruna)', 'SEK': 'SEK (Swedish Krona)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'E£': 'EGP (Egyptian Pound)', 'CL$': 'CLP (Chilean Peso)'}\n",
    "\n",
    "\n",
    "currency_list = []\n",
    "API_KEY = '8c36bc42cd11c738c1baad3e2000b40c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d149ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rates(of_date, currency_code='EUR'):\n",
    "# USING API TO GET RATES FROM SITE https://fixer.io/documentation\n",
    "    res = requests.get(fr'http://data.fixer.io/api/{of_date}?access_key=acfed48df1159d37fa4305e5e95c234f&base={currency_code}')\n",
    "    rates = res.json()['rates']\n",
    "    return rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85212e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_url_to_process_scraperapi(url_input, city_input, category_input, page = 1, max_pages = 25):\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    data = []\n",
    "    city_path_done = fr'{output_viator}/{date_today}-{city_input}-{category_input}.csv'          \n",
    "    if os.path.exists(city_path_done):\n",
    "        city_done_msg = pd.read_csv(city_path_done)\n",
    "        page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page']) + 1\n",
    "        \n",
    "    \n",
    "    url_time = time.time()\n",
    "    while page <= max_pages:\n",
    "        if page == 1:\n",
    "            url = f'{url_input}'\n",
    "        else:\n",
    "            url = f'{url_input}/{page}'\n",
    "        print(url)\n",
    "\n",
    "        country_codes = [\"eu\", \"us\"]\n",
    "\n",
    "        random_country_code = random.choice(country_codes)\n",
    "        \n",
    "# CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#         print(random_country_code)\n",
    "    \n",
    "        url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "                                    json={'apiKey': f'{API_KEY}', \n",
    "                                          'country_code': random_country_code,\n",
    "                                          'url': url })\n",
    "#         time.sleep(random.uniform(1, 10))\n",
    "        if url_request.status_code == 200:\n",
    "            try:\n",
    "                print(url_request.json()['statusUrl'])\n",
    "                status_url = url_request.json()['statusUrl']\n",
    "                data_send_df = pd.DataFrame({\n",
    "                    'UrlRequest': [url],\n",
    "                    'UrlResponse': [status_url],\n",
    "                    'City': [city_input],\n",
    "                    'Page': [page],\n",
    "                    'Category': category_input\n",
    "                }, columns=['UrlRequest', 'UrlResponse', 'City', 'Page', 'Category'])\n",
    "                data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"JSON could not be decoded\")\n",
    "        else:\n",
    "            print(\"HTTP request returned code: \", url_request.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "            page -=1\n",
    "\n",
    "\n",
    "# IN THE TEXT FILE ADD URL AND STATUS AND WHICH PAGE IS IT RELATED TO \n",
    "        \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66d0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_pages(url):\n",
    "    try:\n",
    "        results = requests.get(url)\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        product_list_count = None\n",
    "\n",
    "        # Try finding the productListCount label using two different CSS selectors\n",
    "        selectors = [\"[id*=productListCount]\", \"h3[class*=productListCount]\", \"h2[class*=productListCountLabel]\"]\n",
    "        for selector in selectors:\n",
    "            count_element = soup.select_one(selector)\n",
    "            if count_element:\n",
    "                product_list_count = int(count_element.text.split()[0].replace(',', ''))\n",
    "                break\n",
    "\n",
    "        if product_list_count is None:\n",
    "            print(\"Product count not found in the HTML content.\")\n",
    "            return None\n",
    "\n",
    "        max_pages = int(round(product_list_count / 24, 0))\n",
    "        return max_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error while fetching HTML content: {e}\")\n",
    "        return 25  # Return a default value of 25 pages if there's an error\n",
    "    \n",
    "def get_status(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return response.json()['status']\n",
    "    except Exception as e:\n",
    "        print(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "        return 'error'\n",
    "\n",
    "def check_status_and_process_city_data(df_links):\n",
    "    cities_to_process = []\n",
    "       \n",
    "        \n",
    "    for index, row in df_links.iterrows():\n",
    "        city = row['City']\n",
    "        category = row['MatchCategory']\n",
    "        city_path_done = fr'{output_viator}/{date_today}-{city}-{category}.csv'\n",
    "        if os.path.exists(city_path_done):\n",
    "            print(city, '-', category, city_path_done)\n",
    "            city_done_msg = pd.read_csv(city_path_done)\n",
    "            city_done_msg.drop_duplicates(inplace=True)\n",
    "        else:\n",
    "#             MAYBE REMOVE VALUE FROM DF_LINKS WHEN THERE IS NO FILE\n",
    "            df_links = df_links[(df_links['City'] != city) & (df_links['MatchCategory'] != category)]\n",
    "            continue\n",
    "        start_time_get_resposne = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {executor.submit(get_status, url): url for url in city_done_msg['UrlResponse']}\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                url = futures[future]\n",
    "#                 print(url)\n",
    "                status = future.result()\n",
    "                city_done_msg.loc[city_done_msg['UrlResponse'] == url, 'Status'] = status\n",
    "        end_time_get_resposne = time.time()\n",
    "#         print(f'First option with concurrent: {round(end_time_get_resposne-start_time_get_resposne,2)}s')\n",
    "        print(f\"For {city} finished {len(city_done_msg[city_done_msg['Status'] == 'finished'])} from {len(city_done_msg)}\")\n",
    "        # Check if all statuses are finished\n",
    "        if len(city_done_msg[city_done_msg['Status'] == 'finished']) == len(city_done_msg):\n",
    "            df_links = df_links[(df_links['City'] != city) & (df_links['MatchCategory'] != category)]\n",
    "            try:\n",
    "                position = df_links[df_links['City'] == city]['Page'] * 24\n",
    "            except:\n",
    "                position = 0\n",
    "\n",
    "            max_page_for_city = get_max_pages(city_done_msg.iloc[0]['UrlResponse'])\n",
    "            city_done_msg['MaxPage'] = max_page_for_city\n",
    "            process_html_from_response_scraperapi(city_done_msg, city_path_done,  position)\n",
    "            \n",
    "    return df_links, cities_to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67257d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_from_response_scraperapi(data_city_df, city_path_done, position = 0):\n",
    "# data_city_df = pd.read_csv(city_path_done)\n",
    "    data = []\n",
    "    for index, row in data_city_df.iterrows():\n",
    "#         print(index)\n",
    "        results = requests.get(row['UrlResponse'])            \n",
    "        soup = BeautifulSoup(results.content, 'html.parser')       \n",
    "        \n",
    "\n",
    "        tour_items = soup.select(\"[id*=productName]\")\n",
    "\n",
    "        if len(tour_items) > 0:\n",
    "            for tour_item in tour_items:\n",
    "#                 page_pos = tour_item['data-action-page-properties']\n",
    "#                 page_list = page_pos.split('|')[0].split(':')[1]\n",
    "#                 position = int(page_pos.split('|')[1].split(':')[1]) + (page - 1) * 24\n",
    "                position = position + 1\n",
    "                title = tour_item.find('h2').text.strip()\n",
    "                splitter = tour_item.text.split('From')[-1][0]\n",
    "                price = splitter + tour_item.text.split('From')[-1].split(splitter)[1]\n",
    "                if len(price) > 9:\n",
    "                    price = price.split('Price')[0]\n",
    "                part_url = tour_item['data-url'].split('\"')[1].split('\\\\')[0]\n",
    "                product_url = f\"https://www.viator.com{part_url}\"\n",
    "                siteuse = 'Viator'\n",
    "                city = row['City']\n",
    "                category = row['Category']\n",
    "#                 category = 'Global'\n",
    "                try:\n",
    "                    discount = tour_item.find('div', {'class': 'text-special product-list-card-savings-label'}).text.strip()\n",
    "                except:\n",
    "                    discount = 'N/A'\n",
    "\n",
    "                amount_reviews = 'N/A'\n",
    "                #NUMBER OF REVIEWS\n",
    "                spans = tour_item.select('span')\n",
    "                for span in spans:\n",
    "        #             print('________________________')\n",
    "        #             print(span.attrs)\n",
    "                    try:\n",
    "                        span['reviewlink']\n",
    "                        amount_reviews = span.text\n",
    "                        break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                try:\n",
    "                    stars = tour_item.find('svg').text.strip()\n",
    "                except:\n",
    "                    stars = 'N/A'\n",
    "\n",
    "                text = tour_item.text.strip()\n",
    "\n",
    "\n",
    "                data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "        else:\n",
    "            tour_items = soup.select(\"[class*=productListCardWithDebug]\")\n",
    "            print('Running using debug HTML')\n",
    "            for tour_item in tour_items:\n",
    "                position = position + 1\n",
    "                title = tour_item.select_one(\"[class*=title]\").text.strip()\n",
    "                price = tour_item.select_one(\"[class*=currentPrice]\").text.strip()\n",
    "                if 'from' in price:\n",
    "                    price = price.split('from')[1]\n",
    "                splitter = price[0]\n",
    "                product_url = f\"https://www.viator.com{tour_item.find('a')['href']}\"\n",
    "                siteuse = 'Viator'\n",
    "                city = row['City']\n",
    "    #             category = row['Category']\n",
    "                category = 'Global'\n",
    "\n",
    "                star =\"M7.5 0a.77.77 0 00-.701.456L5.087 4.083a.785.785 0 01-.588.448l-3.827.582a.828.828 0 00-.433 1.395L3.008 9.33c.185.192.26\"\n",
    "                half =\"M14.761 6.507a.828.828 0 00-.433-1.395L10.5 4.53a.785.785 0 01-.589-.447L8.201.456a.767.767 0 00-1.402 0L5.087 4.083a.785\"\n",
    "                nostar =\"M7.5 1.167l1.565 3.317c.242.52.728.885 1.295.974l3.583.544-2.62 2.673a1.782 1.782 0 00-.48 1.532l.609 3.718L8.315 12.2a1.6\"\n",
    "                try:\n",
    "                    discount = tour_item.select_one(\"[class*=savingsLabel]\").text.strip()\n",
    "                except:\n",
    "                    discount = 'N/A'\n",
    "                try:\n",
    "                    amount_reviews = tour_item.select_one(\"[class*=reviewCount]\").text.strip()\n",
    "                except:\n",
    "                    amount_reviews = 'N/A'\n",
    "                try:\n",
    "                    star_int = 0\n",
    "                    stars_grouped = tour_item.select_one(\"[class*=stars]\").find_all('svg')\n",
    "                    half_star = 'M14'\n",
    "                    for st in stars_grouped:\n",
    "                        path_text = str(st.find('path')['d'])\n",
    "                        if half_star in path_text:\n",
    "                            star_int = star_int + 0.5\n",
    "                        else:\n",
    "                            if '0a.77.77' in str(st):\n",
    "                                star_int = star_int + 1\n",
    "                    stars = f'star-{str(star_int)}'\n",
    "                except:\n",
    "                    stars = 'N/A'\n",
    "                text = tour_item.text.strip()\n",
    "\n",
    "                data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, siteuse, city ])\n",
    "        print(f'URL: {city} currency: {splitter}')\n",
    "    url_done = time.time()\n",
    "    # message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page Currency: 1-{first_style_curr}, 2-{second_style_curr}, 3-{thirtd_style_curr}'\n",
    "    # print(message)\n",
    "    # logger_info.info(message)\n",
    "    df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto'])\n",
    "    file_path = fr'{output_viator}/{date_today}-{city}-Viator.csv' \n",
    "    df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a')\n",
    "    data_city_df.to_csv(file_path_done, header=not os.path.exists(file_path_done), index=False, mode='a')\n",
    "    os.remove(city_path_done)\n",
    "#     row.to_csv(file_path_done, header=True, index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d49875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_to_xlsx():\n",
    "    # Get all CSV files with the specified date prefix\n",
    "    csv_files = [file for file in os.listdir(f'{output_viator}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found with the date prefix '{date_today}'\")\n",
    "        return\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    output_file = file_path_output\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_viator}', csv_file)\n",
    "        sheet_name = os.path.splitext(csv_file)[0]\n",
    "        sheet_name = sheet_name.split(date_today + '-')[1].split('-Viator')[0]\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Write the DataFrame to the Excel file\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Save the Excel file\n",
    "    # writer.save()\n",
    "    writer.close()\n",
    "\n",
    "    print(f\"Combined CSV files with date prefix '{date_today}' into '{output_file}'\")\n",
    "\n",
    "    # Remove the CSV files\n",
    "#     for csv_file in csv_files:\n",
    "#         os.remove(csv_file)\n",
    "    # Move the CSV files to the Archive folder\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_viator}', csv_file)\n",
    "        destination_path = os.path.join(archive_folder, csv_file)\n",
    "        shutil.move(csv_path, destination_path)\n",
    "\n",
    "    print(f\"Moved {len(csv_files)} CSV file(s) to the '{archive_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ccbcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_1st_page_in_dataframe(df_links):\n",
    "    if os.path.exists(file_path_done):\n",
    "        done_msg = pd.read_csv(file_path_done).drop_duplicates(subset=['City', 'Category'], keep='last').reset_index()\n",
    "        df_links = df_links[~(df_links['City'].isin(done_msg['City']) & df_links['MatchCategory'].isin(done_msg['Category']))]\n",
    "#         df_links = df_links[~df_links['City'].isin(done_msg['City'].values)]\n",
    "        \n",
    "    else:\n",
    "        print(\"Nothing done yet\")\n",
    "# ################# THE BELWO CODE PROCESSED ONLY ONE PAGE TO GET MAXIMUM AMOUNT OF PAGES ON THE WEBSITE   \n",
    "    for index, row in df_links.iterrows():\n",
    "        url = row[\"URL\"]\n",
    "        city = row['City']\n",
    "        category = row['MatchCategory']\n",
    "        print(city, category, url )\n",
    "        send_url_to_process_scraperapi(url, city, category, max_pages=1)\n",
    "        \n",
    "    while not df_links.empty:\n",
    "        print(len(df_links))\n",
    "        df_links, processed_cities = check_status_and_process_city_data(df_links)\n",
    "# ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334015c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pages_in_dataframe(df_links):\n",
    "    if os.path.exists(file_path_done):\n",
    "        done_msg = pd.read_csv(file_path_done).drop_duplicates(subset=['City', 'Category'], keep='last').reset_index()\n",
    "        df_links_with_page_maxpage = df_links[df_links['City'].isin(done_msg['City'].values)]\n",
    "        df_links_with_page_maxpage = pd.merge(df_links_with_page_maxpage, done_msg[['City', 'Page', 'MaxPage']], on='City', how='left')\n",
    "#         df_links = df_links[~df_links['City'].isin(done_msg['City'].values)]\n",
    "        \n",
    "# #################### GET DATA FOR ALL PAGES \n",
    "    for index, row in df_links_with_page_maxpage.iterrows():\n",
    "        url = row[\"URL\"]\n",
    "        city = row['City']\n",
    "        category = row['MatchCategory']\n",
    "        page = row['Page'] + 1\n",
    "        max_page = round(row['MaxPage']*0.7, 0)\n",
    "        send_url_to_process_scraperapi(url, city, category, page, max_page)\n",
    "        \n",
    "    while not df_links_with_page_maxpage.empty:\n",
    "        print(len(df_links))\n",
    "        df_links_with_page_maxpage, processed_cities = check_status_and_process_city_data(df_links_with_page_maxpage)\n",
    "        print(f'Processed cities: {processed_cities}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83aac548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name):\n",
    "    global mapping_currency\n",
    "    global date_today\n",
    "    global currency_list\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "    # Define the Azure Blob Storage connection details\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    # Read the Excel file into a Pandas DataFrame\n",
    "    rates_eur = get_rates(date_today, 'EUR')\n",
    "#     rates_gbp = get_rates(date_today, 'EUR')\n",
    "#     GBP AND USD ARE NOT SUPORTED WITHING THIS CURRENT SUBSRICPTION UPGRADE PLAN\n",
    "#     rates_gbp = get_rates(date_today, 'GBP')\n",
    "#     rates_usd = get_rates(date_today, 'USD')\n",
    "    excel_data = pd.read_excel(local_file_path, sheet_name=None)  # for .xlsx files\n",
    "    output_file_path = file_path_output_processed\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            position = 1\n",
    "            if sheet_name in exclude_sheets:\n",
    "                continue\n",
    "            if sheet_name == 'Mt-Vesuvius':\n",
    "                sheet_name = 'Mount-Vesuvius'\n",
    "                df['Miasto'] = 'Mount-Vesuvius'\n",
    "            # Make changes to the df DataFrame as needed\n",
    "            df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "            df['IloscOpini'].fillna(0, inplace= True)\n",
    "            df['Opinia'].fillna('N/A', inplace=True)\n",
    "            df = df[df['Tytul'] != 'Tytul']\n",
    "            df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "            df = df[df['Data zestawienia'].str.len() > 4]\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace(r'\\\\\"', '', regex=True)\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace(r'\\\"', '', regex=True)\n",
    "            df['Tytul URL'] = df['Tytul URL'].str.replace(r'\\\\', '', regex=True)\n",
    "            df['IloscOpini'] = df['IloscOpini'].astype(str).str.replace(',','',regex=True)\n",
    "            df = df.drop_duplicates(subset=['Tytul URL'], keep='first')\n",
    "            for index, row in df.iterrows():\n",
    "\n",
    "                df.at[index, 'Pozycja'] = position\n",
    "                position += 1\n",
    "                currency = ''\n",
    "                if 'per group' in row['Cena']:\n",
    "                    df.at[index, 'Cena'] = row['Cena'].split('per group')[0]\n",
    "                    row['Cena']= row['Cena'].split('per group')[0]\n",
    "                for i in range(0,10):\n",
    "                    if not row['Cena'][i].isnumeric():\n",
    "                        currency = currency + (row['Cena'][i])\n",
    "                    else:\n",
    "                        if row['Cena'][i] == '¹':\n",
    "                            currency = currency + (row['Cena'][i])\n",
    "                            continue\n",
    "                        price = float(row['Cena'][i:].split()[0].replace(',',''))\n",
    "                        total_price = row['Cena']\n",
    "                        break\n",
    "    #             print(currency)\n",
    "                if sheet_name in EUR_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(currency, sheet_name)\n",
    "                elif sheet_name in GBP_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(currency, sheet_name)\n",
    "                elif sheet_name in USD_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(currency, sheet_name)\n",
    "                else:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency[:3]][0:3]])\n",
    "                    except:\n",
    "                        print(currency, sheet_name)\n",
    "    #             print(f'{mapping_currency[currency[:3]][0:3]} conversion rate: {conversion_rate}')\n",
    "    #             print(f'{total_price}- price: {price} - covnersion: {price/(conversion_rate*1.020)}')\n",
    "                df.at[index, 'Cena'] = round(price/(conversion_rate*1.0185), 2)\n",
    "                currency_list.append(currency)\n",
    "\n",
    "            currency_list = list(set(currency_list))\n",
    "    #         display(df)\n",
    "\n",
    "    #         df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "            df.drop(columns=['Przecena', 'Tekst'], inplace=True)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Create a connection to Azure Blob Storage\n",
    "#     blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "#     container_client = blob_service_client.get_container_client(container_name_refined)\n",
    "\n",
    "#     # Upload the modified Excel file to Azure Blob Storage\n",
    "#     with open(output_file_path, \"rb\") as data:\n",
    "#         container_client.upload_blob(name=blob_name, data=data)\n",
    "        \n",
    "#     print(\"File uploaded successfully to Azure Blob Storage (refined).\")\n",
    "#     os.remove(output_file_path)\n",
    "#     create_log_done('Refined')\n",
    "    return 'Added to Blob'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac26d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_run_viator_all_links(df_links=pd.DataFrame()):\n",
    "    \n",
    "    df_links = pd.read_csv(all_links_file)\n",
    "    df_links = df_links[(df_links['Category'] == 'Global') & (df_links['Run'] == 1)]\n",
    "    if os.path.exists(file_path_output):\n",
    "        print(f'Today ({date_today}) Viator done')\n",
    "        return 'Done'  \n",
    "    run_1st_page_in_dataframe(df_links)\n",
    "    run_all_pages_in_dataframe(df_links)\n",
    "    return 'Done'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ebd864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excel_to_single_sheet(excel_file, output_excel):\n",
    "    # Read all sheets of the Excel file into a dictionary of DataFrames\n",
    "    all_sheets = pd.read_excel(excel_file, sheet_name=None)\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "    dataframes_to_combine = []\n",
    "\n",
    "    for sheet_name, data in all_sheets.items():\n",
    "        if sheet_name not in exclude_sheets:\n",
    "            dataframes_to_combine.append(data)\n",
    "\n",
    "    # Combine data using concat\n",
    "    combined_data = pd.concat(dataframes_to_combine, ignore_index=True)\n",
    "    combined_data['Tytul URL'] = combined_data['Tytul URL'].str.lower()\n",
    "    combined_data.drop_duplicates(subset=['Tytul URL'], inplace=True)\n",
    "    # If you wish to convert URLs to text to bypass Excel's limitation\n",
    "    # Comment out the following line if you don't want this\n",
    "#     combined_data = combined_data.applymap(lambda x: 'URL:' + x if isinstance(x, str) and x.startswith('http') else x)\n",
    "    combined_data['Tytul URL'] = \"'\" + combined_data['Tytul URL'].astype(str)\n",
    "    combined_data.to_excel(output_excel, sheet_name='AllLinks', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b974f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_all_csv_processed():\n",
    "    global date_today\n",
    "    global output_viator\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    # Get all CSV files with the specified date prefix    \n",
    "    csv_files = [file for file in os.listdir(f'{output_viator}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "    csv_files_not_finished = []\n",
    "    for csv in csv_files:\n",
    "        if 'viator' not in csv.lower():\n",
    "            csv_files_not_finished.append(csv)\n",
    "\n",
    "\n",
    "    if len(csv_files_not_finished) == 0:\n",
    "        return 'brake'\n",
    "    else:\n",
    "        return f\"Files to process: {len(csv_files_not_finished)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448a319",
   "metadata": {},
   "source": [
    "Execute below cell to get the all link from Viator page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfb47ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    msg_output = ''\n",
    "    while True:\n",
    "        try:\n",
    "            msg_output = weekly_run_viator_all_links()\n",
    "            check_brake_option = check_if_all_csv_processed()\n",
    "            if check_brake_option == 'brake':\n",
    "                break\n",
    "            else:\n",
    "                print(f'CSV file available in {output_viator}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "    try:\n",
    "        combine_csv_to_xlsx()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "            \n",
    "    transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)    \n",
    "    convert_excel_to_single_sheet(file_path_output_processed, file_path_output_processed)\n",
    "\n",
    "    return msg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3803a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today (2024-02-08) Viator done\n",
      "No CSV files found with the date prefix '2024-02-08'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['IloscOpini'].fillna(0, inplace= True)\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_7548\\1407340858.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Opinia'].fillna('N/A', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Wroclaw\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Poznan\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Zakopane\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Warsaw\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n",
      "‚Çπ Gdanks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77e921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
