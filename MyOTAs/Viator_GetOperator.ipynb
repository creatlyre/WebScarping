{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29fc006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import xlsxwriter\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "# eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3YmFiYWFiYTEwNWFkZDZiM2ZiYjlmZjNmZjVmZTNkY2E0Y2VkYTEiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiV29qdGVrIEJhbG9uIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGZCODM1WVhSalRJeEl4WmxyTnBaRXpWQk9hZmUyMUFmU1dZZXNnUGc9czk2LWMiLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vZXhhMi1mYjE3MCIsImF1ZCI6ImV4YTItZmIxNzAiLCJhdXRoX3RpbWUiOjE2ODY2NTg5MDYsInVzZXJfaWQiOiJEcWRXRDhRdloyUTkzcTR4WFhWWlFWUk8wSEMyIiwic3ViIjoiRHFkV0Q4UXZaMlE5M3E0eFhYVlpRVlJPMEhDMiIsImlhdCI6MTY4NjY1OTA2MSwiZXhwIjoxNjg2NjYyNjYxLCJlbWFpbCI6IndvamJhbDNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZ29vZ2xlLmNvbSI6WyIxMTUwNTc1NjgzNzI4NjQ1MzA0NTciXSwiZW1haWwiOlsid29qYmFsM0BnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJnb29nbGUuY29tIn19.IAOh_U2LXNXGk1jqG3q6m9utI79QVMDtCuUcDBSH5TEKPmMCEdW962qOZN6J8wfMzexHX1cWoqGcXYBmjLcjQKBhhQoAUAdYjxEivrLHe8Hi37bIwXrEX9mvAKD1wE71Sq1sbB3B9xU51lTsH88l7P0pq9LDgbaKkJCljvvzJ186BTbX9Qw0CF4gma1XjJ1W3Nmd0BK2pE9y0b3arF_V8bSME6BeR4Ls1yKLM9da-MCN5y-IkwGVB6j78Qrt-4_emtAhxjkcYlzauOtEM8dZ0NzblgSxY-hdG_sG-Clg0gM6fxXRQSQJYjqHNgwY7sjAP885JUWbtjWjoXKvdJn_iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6c40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2023-10-19'\n",
    "date_yesterday = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "output_viator = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/All Links'\n",
    "file_path_done =fr'{output_viator}/{date_today}-DONE-Viator.csv'  \n",
    "archive_folder = fr'{output_viator}/Archive'\n",
    "\n",
    "file_path_done_archive =fr'{archive_folder}/{date_yesterday}-DONE-Viator.csv'  \n",
    "file_path_output = fr\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "file_path_output_processed = fr\"{output_viator}/All Links Viator - {date_today}.xlsx\"\n",
    "file_path_output_processed_csv = fr\"{output_viator}/All Links Viator - {date_today}.csv\"\n",
    "file_path_csv_operator = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv\"\n",
    "file_path_all_links_send_to_scraper = fr\"{output_viator}\\SupplierExtract - {date_today}.csv\"\n",
    "file_path_all_links_send_to_scraper_finished = fr\"{output_viator}\\SupplierExtractFinished - {date_today}.csv\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "all_links_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/AllViator_links.csv'\n",
    "# Set the path of the local file\n",
    "local_file_path = file_path_output\n",
    "# local_file_path = f\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/all_links/viator\"\n",
    "container_name_refined = \"refined/all_links/viator\"\n",
    "\n",
    "blob_name = fr'Viator - {date_today}.xlsx'\n",
    "# file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'â‚´': 'UAH (Ukrainian Hryvnia)', 'zÅ‚': 'PLN (Polish Zloty)',\n",
    "                    'Ð»Ð²': 'BGN Bulgarian Lev', 'US$': 'USD (United States Dollar)', 'lei': 'RON (Romanian Leu)',\n",
    "                    'zł': 'PLN (Polish Zloty)','$U': 'UYU (Uruguayan Peso)', 'COL$': 'COP (Colombian Peso)', \n",
    "                    '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'zł': 'PLN (Polish Zloty)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    'CL$': 'CLP (Chilean Peso)', 'Rp': 'IDR (Indonesian Rupiah)', 'AR$': 'ARS (Argentine Peso)',\n",
    "                    '฿': 'THB (Thai Baht)', 'Kč': 'CZK (Czech Koruna)', 'lei': 'RON (Romanian Leu)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'A$': 'AUD (Australian Dollar)', 'Ft': 'HUF (Hungarian Forint)',\n",
    "                    '€': 'EUR (Euro)', '£': 'GBP (British Pound Sterling)', '₹': 'INR (Indian Rupee)',\n",
    "                    'US$': 'USD (United States Dollar)', 'лв': 'BGN (Bulgarian Lev)',\n",
    "                    'COL$': 'COP (Colombian Peso)', 'lei': 'RON (Romanian Leu)', 'C$': 'NIO (Nicaraguan Cordoba)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'AR$': 'ARS (Argentine Peso)', 'A$': 'AUD (Australian Dollar)',\n",
    "                    'лв': 'BGN (Bulgarian Lev)', 'Ft': 'HUF (Hungarian Forint)', 'DKK': 'DKK (Danish Krone)',\n",
    "                    '₪': 'ILS (Israeli Shekel)', '€.': 'EUR (Euro)', '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'R$': 'BRL (Brazilian Real)', '₹': 'INR (Indian Rupee)', 'zł': 'PLN (Polish Zloty)',\n",
    "                    'US$': 'USD (United States Dollar)', '€': 'EUR (Euro)', '$U': 'UYU (Uruguayan Peso)',\n",
    "                    'Kč': 'CZK (Czech Koruna)', 'SEK': 'SEK (Swedish Krona)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'E£': 'EGP (Egyptian Pound)', 'CL$': 'CLP (Chilean Peso)'}\n",
    "\n",
    "\n",
    "currency_list = []\n",
    "API_KEY = '8c36bc42cd11c738c1baad3e2000b40c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f48489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7cb5ac",
   "metadata": {},
   "source": [
    "*Code below extract the supplier name from the html content*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964f58bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Setting up logging configuration\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m      3\u001b[0m                     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m] - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                     datefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScraper\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     with open(\"config.json\", 'r', encoding='utf-8') as file:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#         config = json.load(file)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     api_key = config['api_key']\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#     file_path_csv_operator = config['file_path_csv_operator']\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     file_path_all_links_send_to_scraper = congi['file_path_all_links_send_to_scraper']\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, api_key, file_path_csv_operator, file_path_all_links_send_to_scraper):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting up logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[logging.FileHandler('viator_getoperator.log'),\n",
    "                               logging.StreamHandler()])\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "#     with open(\"config.json\", 'r', encoding='utf-8') as file:\n",
    "#         config = json.load(file)\n",
    "#     api_key = config['api_key']\n",
    "#     file_path_csv_operator = config['file_path_csv_operator']\n",
    "#     file_path_all_links_send_to_scraper = congi['file_path_all_links_send_to_scraper']\n",
    "    \n",
    "    \n",
    "    def __init__(self, api_key, file_path_csv_operator, file_path_all_links_send_to_scraper):\n",
    "        self.API_KEY = api_key\n",
    "        self.file_path_csv_operator = file_path_csv_operator\n",
    "        self.file_path_all_links_send_to_scraper = file_path_all_links_send_to_scraper\n",
    "        self.recursive_calls = 0\n",
    "        logging.info(\"Scraper initialized with API key and file paths.\")\n",
    "\n",
    "    def _load_dataframe(self, file_path):\n",
    "        \"\"\"Load data from CSV into a dataframe.\"\"\"\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "    def _save_dataframe(self, df, file_path, header=True, mode='w'):\n",
    "        \"\"\"Save dataframe to CSV.\"\"\"\n",
    "        df.to_csv(file_path, index=False, header=header , mode=mode)\n",
    "\n",
    "    def send_url_to_process_supplier_name(self):\n",
    "        \"\"\"Send URLs to the processing service and update the CSV with the response.\"\"\"\n",
    "        # Load dataframe to process\n",
    "        dataframe_to_process = self._load_dataframe(self.file_path_csv_operator)\n",
    "        dataframe_to_process = dataframe_to_process[dataframe_to_process['Operator'] == 'ToDo']\n",
    "\n",
    "        # Load the already processed URLs if file exists\n",
    "        if os.path.exists(self.file_path_all_links_send_to_scraper):\n",
    "            processed_data = pd.read_csv(self.file_path_all_links_send_to_scraper)\n",
    "            processed_urls = processed_data['UrlRequest'].unique()\n",
    "        else:\n",
    "            processed_urls = []\n",
    "\n",
    "        # print('To process URL wich will be send')\n",
    "        # display(dataframe_to_process)\n",
    "        country_codes = [\"us\",\"en\"]\n",
    "\n",
    "\n",
    "        # Filter out URLs that have already been processed\n",
    "        dataframe_to_process = dataframe_to_process[~dataframe_to_process['Link'].isin(processed_urls)]\n",
    "\n",
    "         # Initialize progress tracking variables\n",
    "        total_urls = len(dataframe_to_process)\n",
    "        processed_count = 0\n",
    "        for _, row in dataframe_to_process.iterrows():\n",
    "            processed_count += 1\n",
    "            url = row['Link']\n",
    "            random_country_code = random.choice(country_codes)\n",
    "            url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "                                        json={'apiKey': self.API_KEY, \n",
    "                                              'country_code': random_country_code,\n",
    "                                              'url': url })\n",
    "            self._handle_url_request_response(url_request, url)\n",
    "\n",
    "            # Log the processing status\n",
    "            percent_done = (processed_count / total_urls) * 100\n",
    "            logging.info(f\"Processing {processed_count}/{total_urls} row. Done {percent_done:.2f}%\")\n",
    "\n",
    "    def _handle_url_request_response(self, response, url):\n",
    "        \"\"\"Handle the response from the URL request.\"\"\"\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                status_url = response.json()['statusUrl']\n",
    "                data_send_df = pd.DataFrame({\n",
    "                    'UrlRequest': [url],\n",
    "                    'UrlResponse': [status_url],\n",
    "                    'Status': 'running',\n",
    "                    'Operator': 'ToDo'\n",
    "                })\n",
    "                self._save_dataframe(data_send_df, \n",
    "                                     self.file_path_all_links_send_to_scraper,\n",
    "                                     header=not os.path.exists(self.file_path_all_links_send_to_scraper),\n",
    "                                     mode='a')\n",
    "                \n",
    "#                 logging.info(f\"Processed URL: {url} with status URL: {status_url}\")\n",
    "            except ValueError:\n",
    "                logging.warning(\"JSON could not be decoded for URL: %s\", url)\n",
    "#                 print(\"JSON could not be decoded\")    \n",
    "        else:\n",
    "            logging.error(f\"HTTP request returned code: {response.status_code} for URL: {url}\")\n",
    "#             print(f\"HTTP request returned code: {response.status_code}\")\n",
    "            \n",
    "            \n",
    "\n",
    "    def check_status_and_add_to_file_path(self):\n",
    "        \"\"\"Check the status of URLs and update the CSV.\"\"\"\n",
    "        all_links = self._load_dataframe(self.file_path_all_links_send_to_scraper)\n",
    "#         print('all_links in check_status_and_add_to_file_path')\n",
    "#         display(all_links)\n",
    "        df_links = all_links[all_links['Status'] == 'running']\n",
    "#         print('df_links in check_status_and_add_to_file_path')\n",
    "#         display(df_links)\n",
    "        \n",
    "        previous_hash = None   # Store the hash of the dataframe for change detection\n",
    "    \n",
    "        while len(df_links[df_links['Status'] == 'running']) > 0:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = {executor.submit(self._get_status, url): url for url in df_links['UrlResponse']}\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    url = futures[future]\n",
    "                    status = future.result()\n",
    "                    # Update the status of these rows in the original dataframe\n",
    "                    all_links.loc[all_links['UrlResponse'] == url, 'Status'] = status\n",
    "                    df_links.loc[df_links['UrlResponse'] == url, 'Status'] = status\n",
    "                    # print(url, status, len(df_links[df_links['Status'] == 'running']))\n",
    "                    # Remove the processed URL from the futures dictionary if its status is 'finished'\n",
    "                    if status == 'finished':\n",
    "                        del futures[future]\n",
    "                        logging.info(f\"Finished processing URL: {url}. Left to process: {len(df_links[df_links['Status'] == 'running'])}\") \n",
    "                    else:\n",
    "                        logging.debug(f\"URL: {url} is still runningm Rows to process{len(df_links[df_links['Status'] == 'running'])}\")\n",
    "                    \n",
    "\n",
    "            # Refresh the df_links dataframe to pick only 'running' URLs\n",
    "            df_links = df_links[df_links['Status'] == 'running']\n",
    "            # Check if the dataframe has changed\n",
    "            current_hash = hash(df_links.to_string())\n",
    "            if previous_hash != current_hash:\n",
    "                self._save_dataframe(all_links, self.file_path_all_links_send_to_scraper)\n",
    "                self.extract_supplier_name()\n",
    "                logging.info(\"Detected changes in df_links and saved the updated dataframe.\")\n",
    "                previous_hash = current_hash\n",
    "            else:\n",
    "                logging.info(\"No changes in df_links. \")\n",
    "            \n",
    "#             if len(df_links[df_links['Status'] == 'running']) <= 3 and previous_hash == current_hash:\n",
    "#                 if self.recursive_calls < 5:  # Check against threshold\n",
    "#                     logging.info(\"Low number of 'running' URLs detected. Triggering further processing...\")\n",
    "#                     self.extract_supplier_name()\n",
    "#                     self.send_url_to_process_supplier_name(150)\n",
    "#                     self.recursive_calls += 1\n",
    "#                     logging.info(f\"Starting recursive call. Current count: {self.recursive_calls}\")\n",
    "#                     self.check_status_and_add_to_file_path(150)\n",
    "#                 else:\n",
    "#                     logging.warning(\"Maximum recursive call threshold reached. Not triggering further processing.\")\n",
    "                \n",
    "#             print('df_links afterwards remvoed running')\n",
    "#             display(df_links)\n",
    "        # Save the entire dataframe back to the CSV, overwriting the original file\n",
    "        self._save_dataframe(all_links, self.file_path_all_links_send_to_scraper)\n",
    "        logging.info(f\"Updated {len(df_links)} links in the dataframe and saved.\")\n",
    "        self.recursive_calls -= 1  # Decrement the counter after processing\n",
    "        logging.info(f\"Recursive calls count: {self.recursive_calls}\")\n",
    "\n",
    "        return f'Updated {len(df_links)} links'\n",
    "\n",
    "\n",
    "    def _get_status(self, url):\n",
    "        \"\"\"Retrieve the status for a given URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.json()['status']\n",
    "        except Exception as e:\n",
    "#             print(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "            logging.error(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "            return 'error'\n",
    "        \n",
    "    def extract_supplier_name(self):\n",
    "        \"\"\"Extract supplier name from the URLs and update the CSV.\"\"\"\n",
    "        all_links_df = self._load_dataframe(self.file_path_all_links_send_to_scraper)\n",
    "        operator_csv = self._load_dataframe(self.file_path_csv_operator)\n",
    "        df = all_links_df[(all_links_df['Status'] == 'finished') & (all_links_df['Operator'] == 'ToDo')]\n",
    "        counter = 1\n",
    "        counter = 1\n",
    "        # Preparing session for HTTPS requests\n",
    "        session = requests.Session()\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            supplier_name = self._get_supplier_name_from_url(session, row['UrlResponse'])\n",
    "            logging.info(f\"Extracted supplier name: {supplier_name} for URL: {row['UrlResponse']}\")\n",
    "            all_links_df.loc[all_links_df['UrlResponse'] == row['UrlResponse'], 'Operator'] = supplier_name\n",
    "            operator_csv.loc[operator_csv['Link'] == row['UrlRequest'], 'Operator'] = supplier_name\n",
    "            counter +=1\n",
    "            print(counter)\n",
    "            if counter % 50 == 0:\n",
    "                print(counter, counter % 50)\n",
    "                logging.info('Saving files...')\n",
    "                self._save_dataframe(all_links_df, self.file_path_all_links_send_to_scraper)\n",
    "                self._save_dataframe(operator_csv, self.file_path_csv_operator)\n",
    "\n",
    "        self._save_dataframe(all_links_df, self.file_path_all_links_send_to_scraper)\n",
    "        self._save_dataframe(operator_csv, self.file_path_csv_operator)    \n",
    "\n",
    "    def _get_supplier_name_from_url(self, session, url):\n",
    "        \"\"\"Extract the supplier name from a given URL.\"\"\"\n",
    "        results = session.get(url)\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        split_supplier = str(soup).split('supplierName')\n",
    "        for supplier in split_supplier:\n",
    "            try:\n",
    "                supplier_name_array = supplier.split('timeZone')\n",
    "                if len(supplier_name_array[0]) <= 100:\n",
    "                    return ''.join(filter(lambda x: x.isalpha() or x.isspace(), supplier_name_array[0]))\n",
    "            except:\n",
    "                logging.error('Time zone not found in the extracted supplier details from URL: %s', url)\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e41769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an instance of the Scraper class\n",
    "def main():\n",
    "    scraper = Scraper(api_key=API_KEY, \n",
    "                    file_path_csv_operator=file_path_csv_operator, \n",
    "                    file_path_all_links_send_to_scraper=file_path_all_links_send_to_scraper)\n",
    "\n",
    "    # Read the operator CSV and get the count of 'ToDo' links\n",
    "    operator_csv = pd.read_csv(scraper.file_path_csv_operator)\n",
    "    print(f\"There are {len(operator_csv[operator_csv['Operator'] == 'ToDo'])} links to do\")\n",
    "\n",
    "    # Continue processing as long as there are 'ToDo' links\n",
    "    while len(operator_csv[operator_csv['Operator'] == 'ToDo']) > 0:\n",
    "        print(\"send_url_to_process_supplier_name\")\n",
    "        scraper.send_url_to_process_supplier_name()\n",
    "        print(\"check_status_and_add_to_file_path\")\n",
    "        scraper.check_status_and_add_to_file_path()\n",
    "        print(\"extract_supplier_name\")\n",
    "        scraper.extract_supplier_name()\n",
    "        operator_csv = pd.read_csv(scraper.file_path_csv_operator)\n",
    "        print(f\"There are {len(operator_csv[operator_csv['Operator'] == 'ToDo'])} links to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3803a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 14:11:22 [INFO] - Scraper initialized with API key and file paths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4341 links to do\n",
      "send_url_to_process_supplier_name\n",
      "To process URL wich will be send\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tytul</th>\n",
       "      <th>Link</th>\n",
       "      <th>City</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Date input</th>\n",
       "      <th>Date update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105024</th>\n",
       "      <td>Big Bus London Hop-On Hop-Off Tour and River C...</td>\n",
       "      <td>https://www.viator.com/tours/amsterdam/amsterd...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>8631</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105025</th>\n",
       "      <td>Grand Circle Island and Haleiwa 9 Hour Tour</td>\n",
       "      <td>https://www.viator.com/tours/amsterdam/anne-fr...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>11790</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105026</th>\n",
       "      <td>Cappadocia Balloon Ride and Champagne Breakfast</td>\n",
       "      <td>https://www.viator.com/tours/amsterdam/small-g...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>4056</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105027</th>\n",
       "      <td>Private Tulips Tour Keukenhof and Private Tuli...</td>\n",
       "      <td>https://www.viator.com/tours/amsterdam/private...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>0</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105028</th>\n",
       "      <td>Small-Group Tour: Historical Pub Walking Tour ...</td>\n",
       "      <td>https://www.viator.com/tours/amsterdam/amsterd...</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>4036</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109360</th>\n",
       "      <td>The Highlight of Ubud and Hidden Waterfall - F...</td>\n",
       "      <td>https://www.viator.com/tours/nusa-dua/the-high...</td>\n",
       "      <td>Bali</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>0</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109361</th>\n",
       "      <td>Ubud Monkey - Jungle Swing - Rice Terrace - Ho...</td>\n",
       "      <td>https://www.viator.com/tours/seminyak/ubud-mon...</td>\n",
       "      <td>Bali</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>1</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109362</th>\n",
       "      <td>Mount Batur Volcano Sunrise Jeep Tour and Whit...</td>\n",
       "      <td>https://www.viator.com/tours/ubud/mount-batur-...</td>\n",
       "      <td>Bali</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>0</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109363</th>\n",
       "      <td>All Inclusive in Kintamani Private Tour</td>\n",
       "      <td>https://www.viator.com/tours/ubud/best-of-kint...</td>\n",
       "      <td>Bali</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>0</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109364</th>\n",
       "      <td>Explore Ubud with an English Speaking Driver/G...</td>\n",
       "      <td>https://www.viator.com/tours/ubud/explore-ubud...</td>\n",
       "      <td>Bali</td>\n",
       "      <td>ToDo</td>\n",
       "      <td>2</td>\n",
       "      <td>06/02/2024</td>\n",
       "      <td>06/02/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4341 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tytul  \\\n",
       "105024  Big Bus London Hop-On Hop-Off Tour and River C...   \n",
       "105025        Grand Circle Island and Haleiwa 9 Hour Tour   \n",
       "105026    Cappadocia Balloon Ride and Champagne Breakfast   \n",
       "105027  Private Tulips Tour Keukenhof and Private Tuli...   \n",
       "105028  Small-Group Tour: Historical Pub Walking Tour ...   \n",
       "...                                                   ...   \n",
       "109360  The Highlight of Ubud and Hidden Waterfall - F...   \n",
       "109361  Ubud Monkey - Jungle Swing - Rice Terrace - Ho...   \n",
       "109362  Mount Batur Volcano Sunrise Jeep Tour and Whit...   \n",
       "109363            All Inclusive in Kintamani Private Tour   \n",
       "109364  Explore Ubud with an English Speaking Driver/G...   \n",
       "\n",
       "                                                     Link       City Operator  \\\n",
       "105024  https://www.viator.com/tours/amsterdam/amsterd...  Amsterdam     ToDo   \n",
       "105025  https://www.viator.com/tours/amsterdam/anne-fr...  Amsterdam     ToDo   \n",
       "105026  https://www.viator.com/tours/amsterdam/small-g...  Amsterdam     ToDo   \n",
       "105027  https://www.viator.com/tours/amsterdam/private...  Amsterdam     ToDo   \n",
       "105028  https://www.viator.com/tours/amsterdam/amsterd...  Amsterdam     ToDo   \n",
       "...                                                   ...        ...      ...   \n",
       "109360  https://www.viator.com/tours/nusa-dua/the-high...       Bali     ToDo   \n",
       "109361  https://www.viator.com/tours/seminyak/ubud-mon...       Bali     ToDo   \n",
       "109362  https://www.viator.com/tours/ubud/mount-batur-...       Bali     ToDo   \n",
       "109363  https://www.viator.com/tours/ubud/best-of-kint...       Bali     ToDo   \n",
       "109364  https://www.viator.com/tours/ubud/explore-ubud...       Bali     ToDo   \n",
       "\n",
       "        Reviews  Date input Date update  \n",
       "105024     8631  06/02/2024  06/02/2024  \n",
       "105025    11790  06/02/2024  06/02/2024  \n",
       "105026     4056  06/02/2024  06/02/2024  \n",
       "105027        0  06/02/2024  06/02/2024  \n",
       "105028     4036  06/02/2024  06/02/2024  \n",
       "...         ...         ...         ...  \n",
       "109360        0  06/02/2024  06/02/2024  \n",
       "109361        1  06/02/2024  06/02/2024  \n",
       "109362        0  06/02/2024  06/02/2024  \n",
       "109363        0  06/02/2024  06/02/2024  \n",
       "109364        2  06/02/2024  06/02/2024  \n",
       "\n",
       "[4341 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 14:11:23 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/amsterdam-canal-cruise/d525-75227p1 with status URL: https://async.scraperapi.com/jobs/10adb915-0e31-4365-b4da-e20ca7d8c43d\n",
      "2024-02-06 14:11:23 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/anne-frank-house-in-amsterdam-reserved-access-tickets/d525-239922p10 with status URL: https://async.scraperapi.com/jobs/8c17e3eb-94f8-41f2-91df-e56e5569699b\n",
      "2024-02-06 14:11:24 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/small-group-tour-to-kinderdijk-windmills-and-delft-from-amsterdam/d525-90127p14 with status URL: https://async.scraperapi.com/jobs/ec5440a0-b9dc-4507-967c-35abd0c5d500\n",
      "2024-02-06 14:11:24 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/private-tulips-tour-keukenhof-and-private-tulip-fields/d525-366978p7 with status URL: https://async.scraperapi.com/jobs/165a546a-19cb-4ef9-9f72-36f3a1aeb42d\n",
      "2024-02-06 14:11:24 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/amsterdam-anne-frank-walking-tour/d525-9093p71 with status URL: https://async.scraperapi.com/jobs/324d264f-2b1a-44ab-a37c-0bff665c56e4\n",
      "2024-02-06 14:11:25 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/amsterdam-anne-frank-house-priority-access-tickets/d525-239343p25 with status URL: https://async.scraperapi.com/jobs/8c52ce9d-b93a-45fa-be34-cb3065ea2584\n",
      "2024-02-06 14:11:25 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/van-gogh-museum-guided-tour-reserved-entry/d525-31070p249 with status URL: https://async.scraperapi.com/jobs/a02d2129-2522-472f-a220-c22240bd391e\n",
      "2024-02-06 14:11:25 [INFO] - Processed URL: https://www.viator.com/tours/amsterdam/all-inclusive-private-cruise-with-captain-jack-himself/d525-287371p2 with status URL: https://async.scraperapi.com/jobs/2cda48ad-58a1-4c12-b363-320202e7d6ca\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ba2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
