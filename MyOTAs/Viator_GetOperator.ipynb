{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fc006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "import io\n",
    "\n",
    "# eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3YmFiYWFiYTEwNWFkZDZiM2ZiYjlmZjNmZjVmZTNkY2E0Y2VkYTEiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiV29qdGVrIEJhbG9uIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGZCODM1WVhSalRJeEl4WmxyTnBaRXpWQk9hZmUyMUFmU1dZZXNnUGc9czk2LWMiLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vZXhhMi1mYjE3MCIsImF1ZCI6ImV4YTItZmIxNzAiLCJhdXRoX3RpbWUiOjE2ODY2NTg5MDYsInVzZXJfaWQiOiJEcWRXRDhRdloyUTkzcTR4WFhWWlFWUk8wSEMyIiwic3ViIjoiRHFkV0Q4UXZaMlE5M3E0eFhYVlpRVlJPMEhDMiIsImlhdCI6MTY4NjY1OTA2MSwiZXhwIjoxNjg2NjYyNjYxLCJlbWFpbCI6IndvamJhbDNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZ29vZ2xlLmNvbSI6WyIxMTUwNTc1NjgzNzI4NjQ1MzA0NTciXSwiZW1haWwiOlsid29qYmFsM0BnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJnb29nbGUuY29tIn19.IAOh_U2LXNXGk1jqG3q6m9utI79QVMDtCuUcDBSH5TEKPmMCEdW962qOZN6J8wfMzexHX1cWoqGcXYBmjLcjQKBhhQoAUAdYjxEivrLHe8Hi37bIwXrEX9mvAKD1wE71Sq1sbB3B9xU51lTsH88l7P0pq9LDgbaKkJCljvvzJ186BTbX9Qw0CF4gma1XjJ1W3Nmd0BK2pE9y0b3arF_V8bSME6BeR4Ls1yKLM9da-MCN5y-IkwGVB6j78Qrt-4_emtAhxjkcYlzauOtEM8dZ0NzblgSxY-hdG_sG-Clg0gM6fxXRQSQJYjqHNgwY7sjAP885JUWbtjWjoXKvdJn_iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2023-10-19'\n",
    "date_yesterday = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "output_viator = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/All Links'\n",
    "file_path_done =fr'{output_viator}/{date_today}-DONE-Viator.csv'  \n",
    "archive_folder = fr'{output_viator}/Archive'\n",
    "\n",
    "file_path_done_archive =fr'{archive_folder}/{date_yesterday}-DONE-Viator.csv'  \n",
    "file_path_output = fr\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "file_path_output_processed = fr\"{output_viator}/All Links Viator - {date_today}.xlsx\"\n",
    "file_path_output_processed_csv = fr\"{output_viator}/All Links Viator - {date_today}.csv\"\n",
    "file_path_csv_operator = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv\"\n",
    "file_path_xlsx_operator = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.xlsx\"\n",
    "file_path_all_links_send_to_scraper = fr\"{output_viator}\\SupplierExtract - {date_today}.csv\"\n",
    "file_path_all_links_send_to_scraper_finished = fr\"{output_viator}\\SupplierExtractFinished - {date_today}.csv\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "all_links_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/AllViator_links.csv'\n",
    "# Set the path of the local file\n",
    "local_file_path = file_path_output\n",
    "# local_file_path = f\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/all_links/viator\"\n",
    "container_name_refined = \"refined/all_links/viator\"\n",
    "\n",
    "blob_name = fr'Viator - {date_today}.xlsx'\n",
    "# file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'â‚´': 'UAH (Ukrainian Hryvnia)', 'zÅ‚': 'PLN (Polish Zloty)',\n",
    "                    'Ð»Ð²': 'BGN Bulgarian Lev', 'US$': 'USD (United States Dollar)', 'lei': 'RON (Romanian Leu)',\n",
    "                    'zł': 'PLN (Polish Zloty)','$U': 'UYU (Uruguayan Peso)', 'COL$': 'COP (Colombian Peso)', \n",
    "                    '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'zł': 'PLN (Polish Zloty)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    'CL$': 'CLP (Chilean Peso)', 'Rp': 'IDR (Indonesian Rupiah)', 'AR$': 'ARS (Argentine Peso)',\n",
    "                    '฿': 'THB (Thai Baht)', 'Kč': 'CZK (Czech Koruna)', 'lei': 'RON (Romanian Leu)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'A$': 'AUD (Australian Dollar)', 'Ft': 'HUF (Hungarian Forint)',\n",
    "                    '€': 'EUR (Euro)', '£': 'GBP (British Pound Sterling)', '₹': 'INR (Indian Rupee)',\n",
    "                    'US$': 'USD (United States Dollar)', 'лв': 'BGN (Bulgarian Lev)',\n",
    "                    'COL$': 'COP (Colombian Peso)', 'lei': 'RON (Romanian Leu)', 'C$': 'NIO (Nicaraguan Cordoba)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'AR$': 'ARS (Argentine Peso)', 'A$': 'AUD (Australian Dollar)',\n",
    "                    'лв': 'BGN (Bulgarian Lev)', 'Ft': 'HUF (Hungarian Forint)', 'DKK': 'DKK (Danish Krone)',\n",
    "                    '₪': 'ILS (Israeli Shekel)', '€.': 'EUR (Euro)', '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'R$': 'BRL (Brazilian Real)', '₹': 'INR (Indian Rupee)', 'zł': 'PLN (Polish Zloty)',\n",
    "                    'US$': 'USD (United States Dollar)', '€': 'EUR (Euro)', '$U': 'UYU (Uruguayan Peso)',\n",
    "                    'Kč': 'CZK (Czech Koruna)', 'SEK': 'SEK (Swedish Krona)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'E£': 'EGP (Egyptian Pound)', 'CL$': 'CLP (Chilean Peso)'}\n",
    "\n",
    "\n",
    "currency_list = []\n",
    "API_KEY = '8c36bc42cd11c738c1baad3e2000b40c'\n",
    "API_KEY_ZENROWS = '56ed5b7f827aa5c258b3f6d3f57d36999aa949e8' # https://app.zenrows.com/buildera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7cb5ac",
   "metadata": {},
   "source": [
    "*Code below extract the supplier name from the html content*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964f58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[logging.FileHandler('viator_getoperator.log'),\n",
    "                               logging.StreamHandler()])\n",
    "\n",
    "\n",
    "class Scraper_API:\n",
    "#     with open(\"config.json\", 'r', encoding='utf-8') as file:\n",
    "#         config = json.load(file)\n",
    "#     api_key = config['api_key']\n",
    "#     file_path_csv_operator = config['file_path_csv_operator']\n",
    "#     file_path_all_links_send_to_scraper = congi['file_path_all_links_send_to_scraper']\n",
    "    \n",
    "    \n",
    "    def __init__(self, api_key, file_path_csv_operator, file_path_all_links_send_to_scraper):\n",
    "        self.API_KEY = api_key\n",
    "        self.file_path_csv_operator = file_path_csv_operator\n",
    "        self.file_path_all_links_send_to_scraper = file_path_all_links_send_to_scraper\n",
    "        self.recursive_calls = 0\n",
    "        logging.info(\"Scraper initialized with API key and file paths.\")\n",
    "\n",
    "    def _load_dataframe_csv(self, file_path):\n",
    "        \"\"\"Load data from CSV into a dataframe.\"\"\"\n",
    "        return pd.read_csv(file_path)\n",
    "    def _load_dataframe_xlsx(self, file_path):\n",
    "        \"\"\"Load data from CSV into a dataframe.\"\"\"\n",
    "        return pd.read_excel(file_path)\n",
    "\n",
    "    def _save_dataframe(self, df, file_path, header=True, mode='w'):\n",
    "        \"\"\"Save dataframe to CSV.\"\"\"\n",
    "        df.to_csv(file_path, index=False, header=header , mode=mode)\n",
    "\n",
    "    def send_url_to_process_supplier_name(self):\n",
    "        \"\"\"Send URLs to the processing service and update the CSV with the response.\"\"\"\n",
    "        # Load dataframe to process\n",
    "        dataframe_to_process = self._load_dataframe_xlsx(self.file_path_csv_operator)\n",
    "        dataframe_to_process = dataframe_to_process[dataframe_to_process['Operator'] == 'ToDo']\n",
    "\n",
    "        # Load the already processed URLs if file exists\n",
    "        if os.path.exists(self.file_path_all_links_send_to_scraper):\n",
    "            processed_data = pd.read_csv(self.file_path_all_links_send_to_scraper)\n",
    "            processed_urls = processed_data['UrlRequest'].unique()\n",
    "        else:\n",
    "            processed_urls = []\n",
    "\n",
    "        # print('To process URL wich will be send')\n",
    "        # display(dataframe_to_process)\n",
    "        country_codes = [\"us\",\"en\"]\n",
    "\n",
    "\n",
    "        # Filter out URLs that have already been processed\n",
    "        dataframe_to_process = dataframe_to_process[~dataframe_to_process['Link'].isin(processed_urls)]\n",
    "\n",
    "         # Initialize progress tracking variables\n",
    "        total_urls = len(dataframe_to_process)\n",
    "        processed_count = 0\n",
    "        for _, row in dataframe_to_process.iterrows():\n",
    "            processed_count += 1\n",
    "            url = row['Link']\n",
    "            random_country_code = random.choice(country_codes)\n",
    "            url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "                                        json={'apiKey': self.API_KEY, \n",
    "                                              'country_code': random_country_code,\n",
    "                                              'url': url })\n",
    "            self._handle_url_request_response(url_request, url)\n",
    "\n",
    "            # Log the processing status\n",
    "            percent_done = (processed_count / total_urls) * 100\n",
    "            logging.info(f\"Processing {processed_count}/{total_urls} row. Done {percent_done:.2f}%\")\n",
    "\n",
    "    def _handle_url_request_response(self, response, url):\n",
    "        \"\"\"Handle the response from the URL request.\"\"\"\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                status_url = response.json()['statusUrl']\n",
    "                data_send_df = pd.DataFrame({\n",
    "                    'UrlRequest': [url],\n",
    "                    'UrlResponse': [status_url],\n",
    "                    'Status': 'running',\n",
    "                    'Operator': 'ToDo'\n",
    "                })\n",
    "                self._save_dataframe(data_send_df, \n",
    "                                     self.file_path_all_links_send_to_scraper,\n",
    "                                     header=not os.path.exists(self.file_path_all_links_send_to_scraper),\n",
    "                                     mode='a')\n",
    "                \n",
    "#                 logging.info(f\"Processed URL: {url} with status URL: {status_url}\")\n",
    "            except ValueError:\n",
    "                logging.warning(\"JSON could not be decoded for URL: %s\", url)\n",
    "#                 print(\"JSON could not be decoded\")    \n",
    "        else:\n",
    "            logging.error(f\"HTTP request returned code: {response.status_code} for URL: {url}\")\n",
    "#             print(f\"HTTP request returned code: {response.status_code}\")\n",
    "            \n",
    "            \n",
    "\n",
    "    def check_status_and_add_to_file_path(self):\n",
    "        \"\"\"Check the status of URLs and update the CSV.\"\"\"\n",
    "        all_links = self._load_dataframe_csv(self.file_path_all_links_send_to_scraper)\n",
    "#         print('all_links in check_status_and_add_to_file_path')\n",
    "#         display(all_links)\n",
    "        df_links = all_links[all_links['Status'] == 'running']\n",
    "#         print('df_links in check_status_and_add_to_file_path')\n",
    "#         display(df_links)\n",
    "        \n",
    "        previous_hash = None   # Store the hash of the dataframe for change detection\n",
    "    \n",
    "        while len(df_links[df_links['Status'] == 'running']) > 0:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = {executor.submit(self._get_status, url): url for url in df_links['UrlResponse']}\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    url = futures[future]\n",
    "                    status = future.result()\n",
    "                    # Update the status of these rows in the original dataframe\n",
    "                    all_links.loc[all_links['UrlResponse'] == url, 'Status'] = status\n",
    "                    df_links.loc[df_links['UrlResponse'] == url, 'Status'] = status\n",
    "                    # print(url, status, len(df_links[df_links['Status'] == 'running']))\n",
    "                    # Remove the processed URL from the futures dictionary if its status is 'finished'\n",
    "                    if status == 'finished':\n",
    "                        del futures[future]\n",
    "                        logging.info(f\"Finished processing URL: {url}. Left to process: {len(df_links[df_links['Status'] == 'running'])}\") \n",
    "                    else:\n",
    "                        logging.debug(f\"URL: {url} is still runningm Rows to process{len(df_links[df_links['Status'] == 'running'])}\")\n",
    "                    \n",
    "\n",
    "            # Refresh the df_links dataframe to pick only 'running' URLs\n",
    "            df_links = df_links[df_links['Status'] == 'running']\n",
    "            # Check if the dataframe has changed\n",
    "            current_hash = hash(df_links.to_string())\n",
    "            if previous_hash != current_hash:\n",
    "                self._save_dataframe(all_links, self.file_path_all_links_send_to_scraper)\n",
    "                self.extract_supplier_name()\n",
    "                logging.info(\"Detected changes in df_links and saved the updated dataframe.\")\n",
    "                previous_hash = current_hash\n",
    "            else:\n",
    "                logging.info(\"No changes in df_links. \")\n",
    "            \n",
    "#             if len(df_links[df_links['Status'] == 'running']) <= 3 and previous_hash == current_hash:\n",
    "#                 if self.recursive_calls < 5:  # Check against threshold\n",
    "#                     logging.info(\"Low number of 'running' URLs detected. Triggering further processing...\")\n",
    "#                     self.extract_supplier_name()\n",
    "#                     self.send_url_to_process_supplier_name(150)\n",
    "#                     self.recursive_calls += 1\n",
    "#                     logging.info(f\"Starting recursive call. Current count: {self.recursive_calls}\")\n",
    "#                     self.check_status_and_add_to_file_path(150)\n",
    "#                 else:\n",
    "#                     logging.warning(\"Maximum recursive call threshold reached. Not triggering further processing.\")\n",
    "                \n",
    "#             print('df_links afterwards remvoed running')\n",
    "#             display(df_links)\n",
    "        # Save the entire dataframe back to the CSV, overwriting the original file\n",
    "        self._save_dataframe(all_links, self.file_path_all_links_send_to_scraper)\n",
    "        logging.info(f\"Updated {len(df_links)} links in the dataframe and saved.\")\n",
    "        self.recursive_calls -= 1  # Decrement the counter after processing\n",
    "        logging.info(f\"Recursive calls count: {self.recursive_calls}\")\n",
    "\n",
    "        return f'Updated {len(df_links)} links'\n",
    "\n",
    "\n",
    "    def _get_status(self, url):\n",
    "        \"\"\"Retrieve the status for a given URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.json()['status']\n",
    "        except Exception as e:\n",
    "#             print(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "            logging.error(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "            return 'error'\n",
    "        \n",
    "    def extract_supplier_name(self):\n",
    "        \"\"\"Extract supplier name from the URLs and update the CSV.\"\"\"\n",
    "        all_links_df = self._load_dataframe_csv(self.file_path_all_links_send_to_scraper)\n",
    "        operator_csv = self._load_dataframe_xlsx(self.file_path_csv_operator)\n",
    "        df = all_links_df[(all_links_df['Status'] == 'finished') & (all_links_df['Operator'] == 'ToDo')]\n",
    "        counter = 1\n",
    "        # Preparing session for HTTPS requests\n",
    "        session = requests.Session()\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            supplier_name = self._get_supplier_name_from_url(session, row['UrlResponse'])\n",
    "            logging.info(f\"Extracted supplier name: {supplier_name} for URL: {row['UrlResponse']}\")\n",
    "            all_links_df.loc[all_links_df['UrlResponse'] == row['UrlResponse'], 'Operator'] = supplier_name\n",
    "            operator_csv.loc[operator_csv['Link'] == row['UrlRequest'], 'Operator'] = supplier_name\n",
    "            counter +=1\n",
    "            print(counter)\n",
    "            if counter % 50 == 0:\n",
    "                print(counter, counter % 50)\n",
    "                logging.info('Saving files...')\n",
    "                self._save_dataframe(all_links_df, self.file_path_all_links_send_to_scraper)\n",
    "                self._save_dataframe(operator_csv, self.file_path_csv_operator)\n",
    "\n",
    "        self._save_dataframe(all_links_df, self.file_path_all_links_send_to_scraper)\n",
    "        self._save_dataframe(operator_csv, self.file_path_csv_operator)    \n",
    "\n",
    "    def _get_supplier_name_from_url(self, session, url):\n",
    "        \"\"\"Extract the supplier name from a given URL.\"\"\"\n",
    "        results = session.get(url)\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        split_supplier = str(soup).split('supplierName')\n",
    "        for supplier in split_supplier:\n",
    "            try:\n",
    "                supplier_name_array = supplier.split('timeZone')\n",
    "                if len(supplier_name_array[0]) <= 100:\n",
    "                    return ''.join(filter(lambda x: x.isalpha() or x.isspace(), supplier_name_array[0]))\n",
    "            except:\n",
    "                logging.error('Time zone not found in the extracted supplier details from URL: %s', url)\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e75575",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc1464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[logging.FileHandler('viator_getoperator.log'),\n",
    "                              logging.StreamHandler()])\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, api_key, file_path_xlsx_operator, file_path_all_links_send_to_scraper):\n",
    "        self.API_KEY = api_key\n",
    "        self.file_path_xlsx_operator = file_path_xlsx_operator\n",
    "        \n",
    "        self.file_path_all_links_send_to_scraper = file_path_all_links_send_to_scraper\n",
    "        logging.info(\"Scraper initialized with API key and file paths.\")\n",
    "\n",
    "    def _load_dataframe_xlsx(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            binary_content = file.read()\n",
    "\n",
    "        # Decode the binary content, ignoring undecodable characters\n",
    "        decoded_content = binary_content.decode('utf-8', errors='ignore')\n",
    "\n",
    "        # Use StringIO to turn the decoded content into a stream\n",
    "        data_stream = StringIO(decoded_content)\n",
    "\n",
    "        # Read the stream into a DataFrame\n",
    "        df = pd.read_xlsx(data_stream)\n",
    "        return df\n",
    "\n",
    "    def _load_dataframe_xlsx(self, file_path):\n",
    "        return pd.read_excel(file_path)\n",
    "\n",
    "    def _save_dataframe(self, df, file_path, header=True):\n",
    "        output = io.BytesIO()\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            workbook = writer.book\n",
    "            workbook.strings_to_urls = False\n",
    "            df.to_excel(writer, index=False, sheet_name='AllLinks')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(output.getvalue())\n",
    "            \n",
    "    def _process_single_url(self, url):\n",
    "        response = self._make_zenrows_request(url)\n",
    "        self.extract_supplier_name(response, url)\n",
    "    \n",
    "    def send_url_to_process_supplier_name(self):\n",
    "        \"\"\"Process URLs using the updated ZenRows-based API interaction method.\"\"\"\n",
    "        dataframe_to_process = self._load_dataframe_xlsx(self.file_path_xlsx_operator)\n",
    "        dataframe_to_process = dataframe_to_process[dataframe_to_process['Operator'] == 'ToDo']\n",
    "\n",
    "        # Check if there are already processed URLs\n",
    "        if os.path.exists(self.file_path_all_links_send_to_scraper):\n",
    "            processed_data = self._load_dataframe_xlsx(self.file_path_all_links_send_to_scraper)\n",
    "            processed_urls = processed_data['UrlRequest'].unique()\n",
    "        else:\n",
    "            processed_urls = []\n",
    "\n",
    "        # Filter out URLs that have already been processed\n",
    "        dataframe_to_process = dataframe_to_process[~dataframe_to_process['Link'].isin(processed_urls)]\n",
    "        total_url_to_do = len(dataframe_to_process)\n",
    "        processed_count = 1\n",
    "\n",
    "\n",
    "        for _, row in dataframe_to_process.iterrows():\n",
    "            url = row['Link']\n",
    "            response = self._make_zenrows_request(url)\n",
    "            self.extract_supplier_name(response, url)\n",
    "            # Log the progress\n",
    "            percent_done = (processed_count / total_url_to_do) * 100\n",
    "            logging.info(f\"Currently processing {processed_count}. Done {percent_done:.2f}%\")\n",
    "            processed_count += 1\n",
    "\n",
    "\n",
    "        # with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        #     future_to_url = {executor.submit(self._process_single_url, row['Link']): row for _, row in dataframe_to_process.iterrows()}\n",
    "        #     processed_count = 1\n",
    "        #     for future in concurrent.futures.as_completed(future_to_url):\n",
    "        #         url = future_to_url[future]['Link']\n",
    "        #         try:\n",
    "        #             future.result()\n",
    "        #         except Exception as e:\n",
    "        #             logging.error(f\"Error processing URL {url}: {e}\")\n",
    "        #         # Log the progress\n",
    "        #         percent_done = (processed_count / total_url_to_do) * 100\n",
    "        #         logging.info(f\"Currently processing {processed_count}. Done {percent_done:.2f}%\")\n",
    "        #         processed_count += 1\n",
    "            \n",
    "\n",
    "    def _make_zenrows_request(self, url):\n",
    "        \"\"\"Send a request to the ZenRows API.\"\"\"\n",
    "        params = {\n",
    "            'url': url,\n",
    "            'apikey': self.API_KEY,\n",
    "            'js_render': 'true',\n",
    "            'json_response': 'true',\n",
    "            'premium_proxy': 'true'\n",
    "        }\n",
    "        return requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "    def extract_supplier_name(self, response, url):\n",
    "        \"\"\"Extract supplier name from the URLs and update the xlsx.\"\"\"\n",
    "        # all_links_df = self._load_dataframe_xlsx(self.file_path_all_links_send_to_scraper)\n",
    "        operator_xlsx = self._load_dataframe_xlsx(self.file_path_xlsx_operator)\n",
    "        counter = 1\n",
    "        # Preparing session for HTTPS requests\n",
    "        supplier_name = self._get_supplier_name_from_url(response, url)\n",
    "        logging.info(f\"Extracted supplier name: {supplier_name} for URL: {url}\")\n",
    "        operator_xlsx.loc[operator_xlsx['Link'] == url, 'Operator'] = supplier_name\n",
    "        counter +=1\n",
    "        print(counter)\n",
    "        if counter % 50 == 0:\n",
    "            print(counter, counter % 50)\n",
    "            logging.info('Saving files...')\n",
    "            self._save_dataframe(operator_xlsx, self.file_path_xlsx_operator)\n",
    "\n",
    "        self._save_dataframe(operator_xlsx, self.file_path_xlsx_operator)    \n",
    "\n",
    "    def _get_supplier_name_from_url(self, response, url):\n",
    "        \"\"\"Extract the supplier name from a given URL.\"\"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        split_supplier = str(soup).split('supplierName')\n",
    "        for supplier in split_supplier:\n",
    "            try:\n",
    "                supplier_name_array = supplier.split('timeZone')\n",
    "                if len(supplier_name_array[0]) <= 100:\n",
    "                    return ''.join(filter(lambda x: x.isalpha() or x.isspace(), supplier_name_array[0]))\n",
    "            except:\n",
    "                logging.error('Time zone not found in the extracted supplier details from URL: %s', url)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8485999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Scraper class\n",
    "def main():\n",
    "    scraper = Scraper(api_key=API_KEY_ZENROWS, \n",
    "                    file_path_xlsx_operator=file_path_xlsx_operator, \n",
    "                    file_path_all_links_send_to_scraper=file_path_all_links_send_to_scraper)\n",
    "\n",
    "    # Read the operator xlsx and get the count of 'ToDo' links\n",
    "    \n",
    "    operator_xlsx = scraper._load_dataframe_xlsx(scraper.file_path_xlsx_operator)\n",
    "    print(f\"There are {len(operator_xlsx[operator_xlsx['Operator'] == 'ToDo'])} links to do\")\n",
    "\n",
    "    # Continue processing as long as there are 'ToDo' links\n",
    "    while len(operator_xlsx[operator_xlsx['Operator'] == 'ToDo']) > 0:\n",
    "        print(\"send_url_to_process_supplier_name_and_extract_data\")\n",
    "        scraper.send_url_to_process_supplier_name()\n",
    "        operator_xlsx = pd.read_xlsx(scraper.file_path_xlsx_operator)\n",
    "        print(f\"There are {len(operator_xlsx[operator_xlsx['Operator'] == 'ToDo'])} links to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e41769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create an instance of the Scraper class\n",
    "# def main():\n",
    "#     scraper = Scraper_API(api_key=API_KEY, \n",
    "#                     file_path_csv_operator=file_path_xlsx_operator, \n",
    "#                     file_path_all_links_send_to_scraper=file_path_all_links_send_to_scraper)\n",
    "\n",
    "#     # Read the operator CSV and get the count of 'ToDo' links\n",
    "#     operator_csv = pd.read_csv(scraper.file_path_csv_operator)\n",
    "#     print(f\"There are {len(operator_csv[operator_csv['Operator'] == 'ToDo'])} links to do\")\n",
    "\n",
    "#     # Continue processing as long as there are 'ToDo' links\n",
    "#     while len(operator_csv[operator_csv['Operator'] == 'ToDo']) > 0:\n",
    "#         print(\"send_url_to_process_supplier_name\")\n",
    "#         scraper.send_url_to_process_supplier_name()\n",
    "#         print(\"check_status_and_add_to_file_path\")\n",
    "#         scraper.check_status_and_add_to_file_path()\n",
    "#         print(\"extract_supplier_name\")\n",
    "#         scraper.extract_supplier_name()\n",
    "#         operator_csv = pd.read_csv(scraper.file_path_csv_operator)\n",
    "#         print(f\"There are {len(operator_csv[operator_csv['Operator'] == 'ToDo'])} links to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3803a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 12:23:40 [INFO] - Scraper initialized with API key and file paths.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 942 links to do\n",
      "send_url_to_process_supplier_name_and_extract_data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operator_xlsx[operator_xlsx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOperator\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToDo\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msend_url_to_process_supplier_name_and_extract_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_url_to_process_supplier_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     operator_xlsx \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_xlsx(scraper\u001b[38;5;241m.\u001b[39mfile_path_xlsx_operator)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(operator_xlsx[operator_xlsx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOperator\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mToDo\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m links to do\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m, in \u001b[0;36mScraper.send_url_to_process_supplier_name\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m url \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     66\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_zenrows_request(url)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_supplier_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Log the progress\u001b[39;00m\n\u001b[0;32m     69\u001b[0m percent_done \u001b[38;5;241m=\u001b[39m (processed_count \u001b[38;5;241m/\u001b[39m total_url_to_do) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 103\u001b[0m, in \u001b[0;36mScraper.extract_supplier_name\u001b[1;34m(self, response, url)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract supplier name from the URLs and update the xlsx.\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# all_links_df = self._load_dataframe_xlsx(self.file_path_all_links_send_to_scraper)\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m operator_xlsx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_dataframe_xlsx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path_xlsx_operator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Preparing session for HTTPS requests\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mScraper._load_dataframe_xlsx\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_dataframe_xlsx\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\excel\\_base.py:778\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    775\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[0;32m    777\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[1;32m--> 778\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\excel\\_openpyxl.py:615\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[1;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[0;32m    613\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 615\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# trim trailing empty elements\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\_read_only.py:81\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[1;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[0;32m     77\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_source()\n\u001b[0;32m     78\u001b[0m parser \u001b[38;5;241m=\u001b[39m WorkSheetParser(src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_strings,\n\u001b[0;32m     79\u001b[0m                          data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mdata_only, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[0;32m     80\u001b[0m                          date_formats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39m_date_formats)\n\u001b[1;32m---> 81\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openpyxl\\worksheet\\_reader.py:156\u001b[0m, in \u001b[0;36mWorkSheetParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m properties \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    138\u001b[0m     PRINT_TAG: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_options\u001b[39m\u001b[38;5;124m'\u001b[39m, PrintOptions),\n\u001b[0;32m    139\u001b[0m     MARGINS_TAG: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_margins\u001b[39m\u001b[38;5;124m'\u001b[39m, PageMargins),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m }\n\u001b[0;32m    154\u001b[0m it \u001b[38;5;241m=\u001b[39m iterparse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource) \u001b[38;5;66;03m# add a finaliser to close the source when this becomes possible\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtag_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdispatcher\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\xml\\etree\\ElementTree.py:1253\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1253\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:965\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[1;32m--> 965\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    967\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\zipfile.py:1041\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1040\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1041\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39dcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eaeae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
