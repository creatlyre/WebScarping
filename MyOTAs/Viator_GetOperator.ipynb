{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fc006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "import io\n",
    "\n",
    "# eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3YmFiYWFiYTEwNWFkZDZiM2ZiYjlmZjNmZjVmZTNkY2E0Y2VkYTEiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiV29qdGVrIEJhbG9uIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGZCODM1WVhSalRJeEl4WmxyTnBaRXpWQk9hZmUyMUFmU1dZZXNnUGc9czk2LWMiLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vZXhhMi1mYjE3MCIsImF1ZCI6ImV4YTItZmIxNzAiLCJhdXRoX3RpbWUiOjE2ODY2NTg5MDYsInVzZXJfaWQiOiJEcWRXRDhRdloyUTkzcTR4WFhWWlFWUk8wSEMyIiwic3ViIjoiRHFkV0Q4UXZaMlE5M3E0eFhYVlpRVlJPMEhDMiIsImlhdCI6MTY4NjY1OTA2MSwiZXhwIjoxNjg2NjYyNjYxLCJlbWFpbCI6IndvamJhbDNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZ29vZ2xlLmNvbSI6WyIxMTUwNTc1NjgzNzI4NjQ1MzA0NTciXSwiZW1haWwiOlsid29qYmFsM0BnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJnb29nbGUuY29tIn19.IAOh_U2LXNXGk1jqG3q6m9utI79QVMDtCuUcDBSH5TEKPmMCEdW962qOZN6J8wfMzexHX1cWoqGcXYBmjLcjQKBhhQoAUAdYjxEivrLHe8Hi37bIwXrEX9mvAKD1wE71Sq1sbB3B9xU51lTsH88l7P0pq9LDgbaKkJCljvvzJ186BTbX9Qw0CF4gma1XjJ1W3Nmd0BK2pE9y0b3arF_V8bSME6BeR4Ls1yKLM9da-MCN5y-IkwGVB6j78Qrt-4_emtAhxjkcYlzauOtEM8dZ0NzblgSxY-hdG_sG-Clg0gM6fxXRQSQJYjqHNgwY7sjAP885JUWbtjWjoXKvdJn_iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2023-10-19'\n",
    "date_yesterday = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "output_viator = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Viator/All Links'\n",
    "file_path_done =fr'{output_viator}/{date_today}-DONE-Viator.csv'  \n",
    "archive_folder = fr'{output_viator}/Archive'\n",
    "\n",
    "file_path_done_archive =fr'{archive_folder}/{date_yesterday}-DONE-Viator.csv'  \n",
    "file_path_output = fr\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "file_path_output_processed = fr\"{output_viator}/All Links Viator - {date_today}.xlsx\"\n",
    "file_path_output_processed_csv = fr\"{output_viator}/All Links Viator - {date_today}.csv\"\n",
    "file_path_csv_operator = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv\"\n",
    "file_path_xlsx_operator = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Viator.xlsx\"\n",
    "file_path_all_links_send_to_scraper = fr\"{output_viator}\\SupplierExtract - {date_today}.csv\"\n",
    "file_path_all_links_send_to_scraper_finished = fr\"{output_viator}\\SupplierExtractFinished - {date_today}.csv\"\n",
    "link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "all_links_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/AllViator_links.csv'\n",
    "# Set the path of the local file\n",
    "local_file_path = file_path_output\n",
    "# local_file_path = f\"{output_viator}/AllLinksViator - {date_today}.xlsx\"\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/all_links/viator\"\n",
    "container_name_refined = \"refined/all_links/viator\"\n",
    "\n",
    "blob_name = fr'Viator - {date_today}.xlsx'\n",
    "# file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'â‚´': 'UAH (Ukrainian Hryvnia)', 'zÅ‚': 'PLN (Polish Zloty)',\n",
    "                    'Ð»Ð²': 'BGN Bulgarian Lev', 'US$': 'USD (United States Dollar)', 'lei': 'RON (Romanian Leu)',\n",
    "                    'zł': 'PLN (Polish Zloty)','$U': 'UYU (Uruguayan Peso)', 'COL$': 'COP (Colombian Peso)', \n",
    "                    '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'zł': 'PLN (Polish Zloty)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    'CL$': 'CLP (Chilean Peso)', 'Rp': 'IDR (Indonesian Rupiah)', 'AR$': 'ARS (Argentine Peso)',\n",
    "                    '฿': 'THB (Thai Baht)', 'Kč': 'CZK (Czech Koruna)', 'lei': 'RON (Romanian Leu)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'A$': 'AUD (Australian Dollar)', 'Ft': 'HUF (Hungarian Forint)',\n",
    "                    '€': 'EUR (Euro)', '£': 'GBP (British Pound Sterling)', '₹': 'INR (Indian Rupee)',\n",
    "                    'US$': 'USD (United States Dollar)', 'лв': 'BGN (Bulgarian Lev)',\n",
    "                    'COL$': 'COP (Colombian Peso)', 'lei': 'RON (Romanian Leu)', 'C$': 'NIO (Nicaraguan Cordoba)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'AR$': 'ARS (Argentine Peso)', 'A$': 'AUD (Australian Dollar)',\n",
    "                    'лв': 'BGN (Bulgarian Lev)', 'Ft': 'HUF (Hungarian Forint)', 'DKK': 'DKK (Danish Krone)',\n",
    "                    '₪': 'ILS (Israeli Shekel)', '€.': 'EUR (Euro)', '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'R$': 'BRL (Brazilian Real)', '₹': 'INR (Indian Rupee)', 'zł': 'PLN (Polish Zloty)',\n",
    "                    'US$': 'USD (United States Dollar)', '€': 'EUR (Euro)', '$U': 'UYU (Uruguayan Peso)',\n",
    "                    'Kč': 'CZK (Czech Koruna)', 'SEK': 'SEK (Swedish Krona)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'E£': 'EGP (Egyptian Pound)', 'CL$': 'CLP (Chilean Peso)'}\n",
    "\n",
    "\n",
    "currency_list = []\n",
    "API_KEY = '8c36bc42cd11c738c1baad3e2000b40c'\n",
    "API_KEY_ZENROWS = '56ed5b7f827aa5c258b3f6d3f57d36999aa949e8' # https://app.zenrows.com/buildera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7cb5ac",
   "metadata": {},
   "source": [
    "*Code below extract the supplier name from the html content*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964f58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[logging.FileHandler('viator_getoperator.log'),\n",
    "                               logging.StreamHandler()])\n",
    "\n",
    "\n",
    "class Scraper_API:\n",
    "#     with open(\"config.json\", 'r', encoding='utf-8') as file:\n",
    "#         config = json.load(file)\n",
    "#     api_key = config['api_key']\n",
    "#     file_path_csv_operator = config['file_path_csv_operator']\n",
    "#     file_path_all_links_send_to_scraper = congi['file_path_all_links_send_to_scraper']\n",
    "    \n",
    "    \n",
    "    def __init__(self, api_key, file_path_csv_operator, file_path_all_links_send_to_scraper):\n",
    "        self.API_KEY = api_key\n",
    "        self.file_path_csv_operator = file_path_csv_operator\n",
    "        self.file_path_all_links_send_to_scraper = file_path_all_links_send_to_scraper\n",
    "        self.recursive_calls = 0\n",
    "        logging.info(\"Scraper initialized with API key and file paths.\")\n",
    "\n",
    "    def _load_dataframe_csv(self, file_path):\n",
    "        \"\"\"Load data from CSV into a dataframe.\"\"\"\n",
    "        return pd.read_csv(file_path)\n",
    "    def _load_dataframe_xlsx(self, file_path):\n",
    "        \"\"\"Load data from CSV into a dataframe.\"\"\"\n",
    "        return pd.read_excel(file_path)\n",
    "\n",
    "    def _save_dataframe(self, df, file_path, header=True, mode='w'):\n",
    "        \"\"\"Save dataframe to CSV.\"\"\"\n",
    "        df.to_csv(file_path, index=False, header=header , mode=mode)\n",
    "\n",
    "    def send_url_to_process_supplier_name(self):\n",
    "        \"\"\"Send URLs to the processing service and update the CSV with the response.\"\"\"\n",
    "        # Load dataframe to process\n",
    "        dataframe_to_process = self._load_dataframe_xlsx(self.file_path_csv_operator)\n",
    "        dataframe_to_process = dataframe_to_process[dataframe_to_process['Operator'] == 'ToDo']\n",
    "\n",
    "        # Load the already processed URLs if file exists\n",
    "        if os.path.exists(self.file_path_all_links_send_to_scraper):\n",
    "            processed_data = pd.read_csv(self.file_path_all_links_send_to_scraper)\n",
    "            processed_urls = processed_data['UrlRequest'].unique()\n",
    "        else:\n",
    "            processed_urls = []\n",
    "\n",
    "        # print('To process URL wich will be send')\n",
    "        # display(dataframe_to_process)\n",
    "        country_codes = [\"us\",\"en\"]\n",
    "\n",
    "\n",
    "        # Filter out URLs that have already been processed\n",
    "        dataframe_to_process = dataframe_to_process[~dataframe_to_process['Link'].isin(processed_urls)]\n",
    "\n",
    "         # Initialize progress tracking variables\n",
    "        total_urls = len(dataframe_to_process)\n",
    "        processed_count = 0\n",
    "        for _, row in dataframe_to_process.iterrows():\n",
    "            processed_count += 1\n",
    "            url = row['Link']\n",
    "            random_country_code = random.choice(country_codes)\n",
    "            url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "                                        json={'apiKey': self.API_KEY, \n",
    "                                              'country_code': random_country_code,\n",
    "                                              'url': url })\n",
    "            self._handle_url_request_response(url_request, url)\n",
    "\n",
    "            # Log the processing status\n",
    "            percent_done = (processed_count / total_urls) * 100\n",
    "            logging.info(f\"Processing {processed_count}/{total_urls} row. Done {percent_done:.2f}%\")\n",
    "\n",
    "    def _handle_url_request_response(self, response, url):\n",
    "        \"\"\"Handle the response from the URL request.\"\"\"\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                status_url = response.json()['statusUrl']\n",
    "                data_send_df = pd.DataFrame({\n",
    "                    'UrlRequest': [url],\n",
    "                    'UrlResponse': [status_url],\n",
    "                    'Status': 'running',\n",
    "                    'Operator': 'ToDo'\n",
    "                })\n",
    "                self._save_dataframe(data_send_df, \n",
    "                                     self.file_path_all_links_send_to_scraper,\n",
    "                                     header=not os.path.exists(self.file_path_all_links_send_to_scraper),\n",
    "                                     mode='a')\n",
    "                \n",
    "#                 logging.info(f\"Processed URL: {url} with status URL: {status_url}\")\n",
    "            except ValueError:\n",
    "                logging.warning(\"JSON could not be decoded for URL: %s\", url)\n",
    "#                 print(\"JSON could not be decoded\")    \n",
    "        else:\n",
    "            logging.error(f\"HTTP request returned code: {response.status_code} for URL: {url}\")\n",
    "#             print(f\"HTTP request returned code: {response.status_code}\")\n",
    "            \n",
    "            \n",
    "\n",
    "    def check_status_and_add_to_file_path(self):\n",
    "        \"\"\"Check the status of URLs and update the CSV.\"\"\"\n",
    "        all_links = self._load_dataframe_csv(self.file_path_all_links_send_to_scraper)\n",
    "#         print('all_links in check_status_and_add_to_file_path')\n",
    "#         display(all_links)\n",
    "        df_links = all_links[all_links['Status'] == 'running']\n",
    "#         print('df_links in check_status_and_add_to_file_path')\n",
    "#         display(df_links)\n",
    "        \n",
    "        previous_hash = None   # Store the hash of the dataframe for change detection\n",
    "    \n",
    "        while len(df_links[df_links['Status'] == 'running']) > 0:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "                futures = {executor.submit(self._get_status, url): url for url in df_links['UrlResponse']}\n",
    "                for future in concurrent.futures.as_completed(futures):\n",
    "                    url = futures[future]\n",
    "                    status = future.result()\n",
    "                    # Update the status of these rows in the original dataframe\n",
    "                    all_links.loc[all_links['UrlResponse'] == url, 'Status'] = status\n",
    "                    df_links.loc[df_links['UrlResponse'] == url, 'Status'] = status\n",
    "                    # print(url, status, len(df_links[df_links['Status'] == 'running']))\n",
    "                    # Remove the processed URL from the futures dictionary if its status is 'finished'\n",
    "                    if status == 'finished':\n",
    "                        del futures[future]\n",
    "                        logging.info(f\"Finished processing URL: {url}. Left to process: {len(df_links[df_links['Status'] == 'running'])}\") \n",
    "                    else:\n",
    "                        logging.debug(f\"URL: {url} is still runningm Rows to process{len(df_links[df_links['Status'] == 'running'])}\")\n",
    "                    \n",
    "\n",
    "            # Refresh the df_links dataframe to pick only 'running' URLs\n",
    "            df_links = df_links[df_links['Status'] == 'running']\n",
    "            # Check if the dataframe has changed\n",
    "            current_hash = hash(df_links.to_string())\n",
    "            if previous_hash != current_hash:\n",
    "                self._save_dataframe(all_links, self.file_path_all_links_send_to_scraper)\n",
    "                self.extract_supplier_name()\n",
    "                logging.info(\"Detected changes in df_links and saved the updated dataframe.\")\n",
    "                previous_hash = current_hash\n",
    "            else:\n",
    "                logging.info(\"No changes in df_links. \")\n",
    "            \n",
    "#             if len(df_links[df_links['Status'] == 'running']) <= 3 and previous_hash == current_hash:\n",
    "#                 if self.recursive_calls < 5:  # Check against threshold\n",
    "#                     logging.info(\"Low number of 'running' URLs detected. Triggering further processing...\")\n",
    "#                     self.extract_supplier_name()\n",
    "#                     self.send_url_to_process_supplier_name(150)\n",
    "#                     self.recursive_calls += 1\n",
    "#                     logging.info(f\"Starting recursive call. Current count: {self.recursive_calls}\")\n",
    "#                     self.check_status_and_add_to_file_path(150)\n",
    "#                 else:\n",
    "#                     logging.warning(\"Maximum recursive call threshold reached. Not triggering further processing.\")\n",
    "                \n",
    "#             print('df_links afterwards remvoed running')\n",
    "#             display(df_links)\n",
    "        # Save the entire dataframe back to the CSV, overwriting the original file\n",
    "        self._save_dataframe(all_links, self.file_path_all_links_send_to_scraper)\n",
    "        logging.info(f\"Updated {len(df_links)} links in the dataframe and saved.\")\n",
    "        self.recursive_calls -= 1  # Decrement the counter after processing\n",
    "        logging.info(f\"Recursive calls count: {self.recursive_calls}\")\n",
    "\n",
    "        return f'Updated {len(df_links)} links'\n",
    "\n",
    "\n",
    "    def _get_status(self, url):\n",
    "        \"\"\"Retrieve the status for a given URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            return response.json()['status']\n",
    "        except Exception as e:\n",
    "#             print(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "            logging.error(f\"Error while fetching URL: {url}, Error: {e}\")\n",
    "            return 'error'\n",
    "        \n",
    "    def extract_supplier_name(self):\n",
    "        \"\"\"Extract supplier name from the URLs and update the CSV.\"\"\"\n",
    "        all_links_df = self._load_dataframe_csv(self.file_path_all_links_send_to_scraper)\n",
    "        operator_csv = self._load_dataframe_xlsx(self.file_path_csv_operator)\n",
    "        df = all_links_df[(all_links_df['Status'] == 'finished') & (all_links_df['Operator'] == 'ToDo')]\n",
    "        counter = 1\n",
    "        # Preparing session for HTTPS requests\n",
    "        session = requests.Session()\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            supplier_name = self._get_supplier_name_from_url(session, row['UrlResponse'])\n",
    "            logging.info(f\"Extracted supplier name: {supplier_name} for URL: {row['UrlResponse']}\")\n",
    "            all_links_df.loc[all_links_df['UrlResponse'] == row['UrlResponse'], 'Operator'] = supplier_name\n",
    "            operator_csv.loc[operator_csv['Link'] == row['UrlRequest'], 'Operator'] = supplier_name\n",
    "            counter +=1\n",
    "            print(counter)\n",
    "            if counter % 50 == 0:\n",
    "                print(counter, counter % 50)\n",
    "                logging.info('Saving files...')\n",
    "                self._save_dataframe(all_links_df, self.file_path_all_links_send_to_scraper)\n",
    "                self._save_dataframe(operator_csv, self.file_path_csv_operator)\n",
    "\n",
    "        self._save_dataframe(all_links_df, self.file_path_all_links_send_to_scraper)\n",
    "        self._save_dataframe(operator_csv, self.file_path_csv_operator)    \n",
    "\n",
    "    def _get_supplier_name_from_url(self, session, url):\n",
    "        \"\"\"Extract the supplier name from a given URL.\"\"\"\n",
    "        results = session.get(url)\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        split_supplier = str(soup).split('supplierName')\n",
    "        for supplier in split_supplier:\n",
    "            try:\n",
    "                supplier_name_array = supplier.split('timeZone')\n",
    "                if len(supplier_name_array[0]) <= 100:\n",
    "                    return ''.join(filter(lambda x: x.isalpha() or x.isspace(), supplier_name_array[0]))\n",
    "            except:\n",
    "                logging.error('Time zone not found in the extracted supplier details from URL: %s', url)\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e75575",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc1464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging configuration\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[logging.FileHandler('viator_getoperator.log'),\n",
    "                              logging.StreamHandler()])\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, api_key, file_path_xlsx_operator, file_path_all_links_send_to_scraper):\n",
    "        self.API_KEY = api_key\n",
    "        self.file_path_xlsx_operator = file_path_xlsx_operator\n",
    "        \n",
    "        self.file_path_all_links_send_to_scraper = file_path_all_links_send_to_scraper\n",
    "        logging.info(\"Scraper initialized with API key and file paths.\")\n",
    "\n",
    "    def _load_dataframe_xlsx(self, file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            binary_content = file.read()\n",
    "\n",
    "        # Decode the binary content, ignoring undecodable characters\n",
    "        decoded_content = binary_content.decode('utf-8', errors='ignore')\n",
    "\n",
    "        # Use StringIO to turn the decoded content into a stream\n",
    "        data_stream = StringIO(decoded_content)\n",
    "\n",
    "        # Read the stream into a DataFrame\n",
    "        df = pd.read_xlsx(data_stream)\n",
    "        return df\n",
    "\n",
    "    def _load_dataframe_xlsx(self, file_path):\n",
    "        return pd.read_excel(file_path)\n",
    "\n",
    "    def _save_dataframe(self, df, file_path, header=True):\n",
    "        output = io.BytesIO()\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            workbook = writer.book\n",
    "            workbook.strings_to_urls = False\n",
    "            df.to_excel(writer, index=False, sheet_name='AllLinks')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(output.getvalue())\n",
    "            \n",
    "    def _process_single_url(self, url):\n",
    "        response = self._make_zenrows_request(url)\n",
    "        self.extract_supplier_name(response, url)\n",
    "    \n",
    "    def send_url_to_process_supplier_name(self):\n",
    "        \"\"\"Process URLs using the updated ZenRows-based API interaction method.\"\"\"\n",
    "        dataframe_to_process = self._load_dataframe_xlsx(self.file_path_xlsx_operator)\n",
    "        dataframe_to_process = dataframe_to_process[dataframe_to_process['Operator'] == 'ToDo']\n",
    "\n",
    "        # Check if there are already processed URLs\n",
    "        if os.path.exists(self.file_path_all_links_send_to_scraper):\n",
    "            processed_data = self._load_dataframe_xlsx(self.file_path_all_links_send_to_scraper)\n",
    "            processed_urls = processed_data['UrlRequest'].unique()\n",
    "        else:\n",
    "            processed_urls = []\n",
    "\n",
    "        # Filter out URLs that have already been processed\n",
    "        dataframe_to_process = dataframe_to_process[~dataframe_to_process['Link'].isin(processed_urls)]\n",
    "        total_url_to_do = len(dataframe_to_process)\n",
    "        processed_count = 1\n",
    "\n",
    "\n",
    "        for _, row in dataframe_to_process.iterrows():\n",
    "            url = row['Link']\n",
    "            response = self._make_zenrows_request(url)\n",
    "            self.extract_supplier_name(response, url)\n",
    "            # Log the progress\n",
    "            percent_done = (processed_count / total_url_to_do) * 100\n",
    "            logging.info(f\"Currently processing {processed_count}. Done {percent_done:.2f}%\")\n",
    "            processed_count += 1\n",
    "\n",
    "\n",
    "        # with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        #     future_to_url = {executor.submit(self._process_single_url, row['Link']): row for _, row in dataframe_to_process.iterrows()}\n",
    "        #     processed_count = 1\n",
    "        #     for future in concurrent.futures.as_completed(future_to_url):\n",
    "        #         url = future_to_url[future]['Link']\n",
    "        #         try:\n",
    "        #             future.result()\n",
    "        #         except Exception as e:\n",
    "        #             logging.error(f\"Error processing URL {url}: {e}\")\n",
    "        #         # Log the progress\n",
    "        #         percent_done = (processed_count / total_url_to_do) * 100\n",
    "        #         logging.info(f\"Currently processing {processed_count}. Done {percent_done:.2f}%\")\n",
    "        #         processed_count += 1\n",
    "            \n",
    "\n",
    "    def _make_zenrows_request(self, url):\n",
    "        \"\"\"Send a request to the ZenRows API.\"\"\"\n",
    "        params = {\n",
    "            'url': url,\n",
    "            'apikey': self.API_KEY,\n",
    "            'js_render': 'true',\n",
    "            'json_response': 'true',\n",
    "            'premium_proxy': 'true'\n",
    "        }\n",
    "        return requests.get('https://api.zenrows.com/v1/', params=params)\n",
    "\n",
    "    def extract_supplier_name(self, response, url):\n",
    "        \"\"\"Extract supplier name from the URLs and update the xlsx.\"\"\"\n",
    "        # all_links_df = self._load_dataframe_xlsx(self.file_path_all_links_send_to_scraper)\n",
    "        operator_xlsx = self._load_dataframe_xlsx(self.file_path_xlsx_operator)\n",
    "        counter = 1\n",
    "        # Preparing session for HTTPS requests\n",
    "        supplier_name = self._get_supplier_name_from_url(response, url)\n",
    "        logging.info(f\"Extracted supplier name: {supplier_name} for URL: {url}\")\n",
    "        operator_xlsx.loc[operator_xlsx['Link'] == url, 'Operator'] = supplier_name\n",
    "        counter +=1\n",
    "        print(counter)\n",
    "        if counter % 50 == 0:\n",
    "            print(counter, counter % 50)\n",
    "            logging.info('Saving files...')\n",
    "            self._save_dataframe(operator_xlsx, self.file_path_xlsx_operator)\n",
    "\n",
    "        self._save_dataframe(operator_xlsx, self.file_path_xlsx_operator)    \n",
    "\n",
    "    def _get_supplier_name_from_url(self, response, url):\n",
    "        \"\"\"Extract the supplier name from a given URL.\"\"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        split_supplier = str(soup).split('supplierName')\n",
    "        for supplier in split_supplier:\n",
    "            try:\n",
    "                supplier_name_array = supplier.split('timeZone')\n",
    "                if len(supplier_name_array[0]) <= 100:\n",
    "                    return ''.join(filter(lambda x: x.isalpha() or x.isspace(), supplier_name_array[0]))\n",
    "            except:\n",
    "                logging.error('Time zone not found in the extracted supplier details from URL: %s', url)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8485999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Scraper class\n",
    "def main():\n",
    "    scraper = Scraper(api_key=API_KEY_ZENROWS, \n",
    "                    file_path_xlsx_operator=file_path_xlsx_operator, \n",
    "                    file_path_all_links_send_to_scraper=file_path_all_links_send_to_scraper)\n",
    "\n",
    "    # Read the operator xlsx and get the count of 'ToDo' links\n",
    "    \n",
    "    operator_xlsx = scraper._load_dataframe_xlsx(scraper.file_path_xlsx_operator)\n",
    "    print(f\"There are {len(operator_xlsx[operator_xlsx['Operator'] == 'ToDo'])} links to do\")\n",
    "\n",
    "    # Continue processing as long as there are 'ToDo' links\n",
    "    while len(operator_xlsx[operator_xlsx['Operator'] == 'ToDo']) > 0:\n",
    "        print(\"send_url_to_process_supplier_name_and_extract_data\")\n",
    "        scraper.send_url_to_process_supplier_name()\n",
    "        operator_xlsx = pd.read_xlsx(scraper.file_path_xlsx_operator)\n",
    "        print(f\"There are {len(operator_xlsx[operator_xlsx['Operator'] == 'ToDo'])} links to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce87f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e41769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create an instance of the Scraper class\n",
    "# def main():\n",
    "#     scraper = Scraper_API(api_key=API_KEY, \n",
    "#                     file_path_csv_operator=file_path_xlsx_operator, \n",
    "#                     file_path_all_links_send_to_scraper=file_path_all_links_send_to_scraper)\n",
    "\n",
    "#     # Read the operator CSV and get the count of 'ToDo' links\n",
    "#     operator_csv = pd.read_csv(scraper.file_path_csv_operator)\n",
    "#     print(f\"There are {len(operator_csv[operator_csv['Operator'] == 'ToDo'])} links to do\")\n",
    "\n",
    "#     # Continue processing as long as there are 'ToDo' links\n",
    "#     while len(operator_csv[operator_csv['Operator'] == 'ToDo']) > 0:\n",
    "#         print(\"send_url_to_process_supplier_name\")\n",
    "#         scraper.send_url_to_process_supplier_name()\n",
    "#         print(\"check_status_and_add_to_file_path\")\n",
    "#         scraper.check_status_and_add_to_file_path()\n",
    "#         print(\"extract_supplier_name\")\n",
    "#         scraper.extract_supplier_name()\n",
    "#         operator_csv = pd.read_csv(scraper.file_path_csv_operator)\n",
    "#         print(f\"There are {len(operator_csv[operator_csv['Operator'] == 'ToDo'])} links to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3803a3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 09:24:58 [INFO] - Scraper initialized with API key and file paths.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a39dcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Wojciech\\\\Documents\\\\Python\\\\MyOTAs\\\\OTAs_Application\\\\MyOTAs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eaeae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
