{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39d9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PySimpleGUI as sg\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pyodbc\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import io \n",
    "# from Viator_AllLinks import main as main_alllinks\n",
    "# from Viator_GetOperator import main as main_getoperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e379d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "The main function orchestrates the process of reading an Excel file, excluding sheets named 'DONE', \n",
    "processing data to remove duplicate 'Tytul URL' entries, and either appending to or overwriting an \n",
    "output Excel file based on date comparison. It leverages the functions defined below to handle each step.\n",
    "\"\"\"\n",
    "\n",
    "# Function to read Excel, ignoring sheets named 'DONE'\n",
    "def read_excel_sheets(excel_path):\n",
    "    \"\"\"Read all sheets from the Excel file except those named 'DONE'.\"\"\"\n",
    "    all_sheets_data = pd.read_excel(excel_path, sheet_name=None)\n",
    "    sheets_data = {sheet_name: data for sheet_name, data in all_sheets_data.items() if sheet_name.upper() != 'DONE'}\n",
    "    return sheets_data\n",
    "\n",
    "# Function to process and combine sheets data\n",
    "def process_sheets(sheets_data):\n",
    "    \"\"\"Combine data from all sheets into a single DataFrame and remove duplicates based on 'Tytul URL'.\"\"\"\n",
    "    combined_data = pd.DataFrame()\n",
    "    for sheet, data in sheets_data.items():\n",
    "        data['Tytul URL'] = data['Tytul URL'].str.lower()\n",
    "        data.drop_duplicates(subset=['Tytul URL'], inplace=True)\n",
    "        combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "    return combined_data\n",
    "\n",
    "# Function to append or overwrite the output file\n",
    "def update_output_file(output_path, combined_data, input_date):\n",
    "    \"\"\"Append to or overwrite the output file based on the 'Data zestawienia' date comparison.\"\"\"\n",
    "    if output_path.exists():\n",
    "        output_data = pd.read_excel(output_path)\n",
    "        if not output_data.empty and 'Data zestawienia' in output_data.columns and output_data['Data zestawienia'].eq(input_date).any():\n",
    "            combined_data = pd.concat([combined_data, output_data], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            print(\"Dates do not match, overwriting the output file.\")\n",
    "    combined_data.to_excel(output_path, index=False, sheet_name='AllLinks')\n",
    "\n",
    "# Main workflow\n",
    "def main(input_excel, output_excel):\n",
    "    \"\"\"The main function coordinating the reading, processing, and output of Excel data.\"\"\"\n",
    "    input_path = Path(input_excel)\n",
    "    output_path = Path(output_excel)\n",
    "    \n",
    "    # Reading the input file\n",
    "    sheets_data = read_excel_sheets(input_path)\n",
    "\n",
    "    first_sheet_name = next(iter(sheets_data))\n",
    "    input_date = sheets_data[first_sheet_name]['Data zestawienia'].iloc[0]\n",
    "\n",
    "    # Processing sheets data\n",
    "    combined_data = process_sheets(sheets_data)\n",
    "    \n",
    "    # Updating the output file\n",
    "    update_output_file(output_path, combined_data, input_date)\n",
    "\n",
    "   \n",
    "# Today's date to be used in the file name\n",
    "today_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# File paths, replace the placeholders with actual paths\n",
    "\n",
    "file_output_g = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Get Your Guide\\All Links\\All Links GYG - {today_date}.xlsx\"\n",
    "file_input_g = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Get Your Guide\\GYG - {today_date}.xlsx\"\n",
    "file_input_v = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\Daily\\Viator - {today_date}.xlsx\"\n",
    "file_output_v = fr\"G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\All Links\\All Links Viator - {today_date}.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "# main(file_input_g, file_output_g)\n",
    "# main(file_input_v, file_output_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02470f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_main = pd.read_excel(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx')\n",
    "# df_day = pd.read_excel(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\All Links\\All Links Viator - 2023-03-08.xlsx', sheet_name='AllLinks', header=None)\n",
    "# df_day =  pd.read_excel(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Get Your Guide\\AllLinks\\All Links GYG - 2023-03-07.xlsx',  sheet_name='AllLinks')\n",
    "# df_main = pd.read_csv(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1079c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_day = df_day[[0, 1, 3, 6, 9]]\n",
    "# df_day.drop_duplicates('Tytul Url')\n",
    "# df_day.rename(columns = {0:'Tytul', 1:'Tytul Url', 3:'IloscOpini', 6:'Data zestawienia', 9:'Miasto'})\n",
    "# df_day.dtypes\n",
    "# df_main.drop(columns='Tytul', inplace=True)\n",
    "# df_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9109bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_new_links(path_main, path_daily, sheetExcel):\n",
    "    get_time = time.time()\n",
    "    df_main = pd.read_csv(path_main)\n",
    "    display(df_main)\n",
    "    #Based on input read files\n",
    "    if 'Viator' in path_daily:\n",
    "#         df_day = df_day[[0, 1, 3, 6, 9]]\n",
    "        try:\n",
    "            df_day = pd.read_excel(path_daily, sheetExcel)\n",
    "        except:\n",
    "            raise Exception(\"File not found\")\n",
    "#         df_day.rename(columns = {0:'Tytul', 1:'Tytul Url', 3:'IloscOpini', 6:'Data zestawienia', 9:'Miasto'}, inplace=True)\n",
    "    elif 'GYG' in path_daily:\n",
    "        try:\n",
    "            df_day = pd.read_excel(path_daily, sheetExcel)\n",
    "        except:\n",
    "            raise Exception(\"File not found\")\n",
    "    #SHORTEN DFS\n",
    "#     df_day = df_day.head(100) #short version for testing    \n",
    "#     display(df_day)\n",
    "#     df_main = df_main.head(50) #short version for testing\n",
    "    #____\n",
    "    \n",
    "    df_day = df_day[['Tytul', 'Tytul URL', 'IloscOpini' ,'Data zestawienia', 'Miasto']]    \n",
    "    df_day.rename(columns={'Tytul URL': 'Link', 'IloscOpini': 'Reviews', 'Miasto': 'City', 'Data zestawienia': 'Date input'}, inplace=True)\n",
    "    df_day.insert(len(df_day.columns), 'Date update', df_day['Date input'])\n",
    "    df_day.insert(len(df_day.columns),'Operator', 'ToDo')\n",
    "    df_day = df_day.drop_duplicates('Link').reset_index(drop=True)\n",
    "    df_day['Link'] = df_day['Link'].str.lower()\n",
    "    df_day['Link'] = df_day['Link'].str.replace(\"'\", \"\", regex=True)\n",
    "#     df_day.set_index('Link', inplace=True)\n",
    "#     df_main.set_index('Link', inplace=True)\n",
    "    match = 0\n",
    "    added = 0\n",
    "    amnt_do = df_day['Link'].count()\n",
    "    #METHOD ONE\n",
    "    print(f'Load DFs time =  {time.time() - get_time}')\n",
    "#     display(df_day)\n",
    "    method_one = time.time()\n",
    "    for index, row in df_day.iterrows():\n",
    "        if index % 598 == 0:\n",
    "            print(f'Processed { round((index / amnt_do) * 100, 2)}%')\n",
    "        if len(df_main[df_main['Link'] == row['Link']]) == 1: # checking if URL matches exisitng one\n",
    "            index_main = df_main[df_main['Link'] == row['Link']].index\n",
    "            df_main.at[index_main[0], 'Date update'] = row['Date input']\n",
    "            df_main.at[index_main[0], 'Reviews'] = row['Reviews']\n",
    "            df_main.at[index_main[0], 'Tytul'] = row['Tytul']\n",
    "            match = match + 1\n",
    "        else:\n",
    "            df_main = pd.concat([df_main, row.to_frame().T])     \n",
    "            added = added + 1\n",
    "\n",
    "    print(f'Matched {match}; Added {added}; Time taken: {round((time.time() - method_one)/60,4)} minutes')\n",
    "    df_main.drop_duplicates('Link', inplace=True)\n",
    "    df_main.to_csv(path_main, index=False, encoding='utf-8')            \n",
    "    return f'Matched {match}; Added {added}; Time taken: {round((time.time() - method_one)/60,4)} minutes'\n",
    "    \n",
    "##@@@@@@@@@@@@ FUNCTION TESTING BELOW\n",
    "# sheet = 'AllLinks'\n",
    "# ##VIATOR\n",
    "# pp = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.xlsx'\n",
    "# day = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\All Links\\All Links Viator - 2023-03-08.xlsx'\n",
    "# insert_new_links(pp, day, 'AllLinks')\n",
    "# ##GYG\n",
    "# pp = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG - Copy.xlsx'\n",
    "# day = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Get Your Guide\\AllLinks\\All Links GYG - 2023-03-07.xlsx'\n",
    "# insert_new_links(pp, day, sheet)\n",
    "#@@@@@@@@@@@@ FUNCTION TESTING ABOVE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9013a89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21578eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_operators_name_from_chrome(path_main_file):\n",
    "#     print(f'--------------{path_main_file}')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "#     path = path_gyg.replace('\\\\','/') # replaces backslashes with forward slashes\n",
    "#     path = path_gyg[1:len(path_gyg)-1] # to remove quote marks\n",
    "#     print(path)\n",
    "\n",
    "    df = pd.read_csv(path_main_file, encoding='latin-1')    \n",
    "    df['Link'] = df['Link'].str.lower()\n",
    "    df.drop_duplicates(subset=['Link'], inplace=True)              \n",
    "    df.reset_index()\n",
    "    timeS=time.time()\n",
    "    countDone = 0\n",
    "    countFailed = 0\n",
    "    start = time.time()\n",
    "    if 'GYG' in path_main_file:\n",
    "        for index, row in df.iterrows():\n",
    "            notFound = False\n",
    "            if (row[\"Operator\"] == \"ToDo\" and row['Link'] != 'tytul url'):\n",
    "#             or ('Â' in str(row[\"Operator\"])  and row['Link'] != 'tytul url'):\n",
    "            #   print(row['GYG Link'], row['Tytul'])\n",
    "#                 print(str(row[\"Operator\"]))\n",
    "                url = row['Link']\n",
    "                driver.get(url)\n",
    "                try:\n",
    "                    elem = driver.find_element(By.CLASS_NAME, \"supplier-name__link\")\n",
    "#                     print(f\"Re: {re.sub('[^A-Za-z0-9 ]+', '', elem.text)}\")\n",
    "                except:\n",
    "                    notFound = True\n",
    "\n",
    "\n",
    "                if notFound == True:\n",
    "                    df.at[index,'Operator'] = 'Incorrect URL'\n",
    "                    countDone = countDone + 1    \n",
    "                    countFailed = countFailed + 1\n",
    "                else:\n",
    "                    df.at[index,'Operator'] = re.sub('[^A-Za-z0-9 ]+', '', elem.text)\n",
    "                    countDone = countDone + 1\n",
    "\n",
    "                \n",
    "                print(f'Index: {index} | Total time: {time.time() - timeS} | Avg per record: {(time.time() - timeS) / countDone} | Total done | {countDone} | {((countDone - countFailed)/countDone)*100}%')\n",
    "\n",
    "                if countDone % 100 == 0:\n",
    "                    print('INSERTING DF TO EXCEL')\n",
    "                    df.to_csv(path_main_file, index=False)\n",
    "\n",
    "        \n",
    "        df.to_csv(path_main_file, index=False)    \n",
    "\n",
    "        driver.quit()\n",
    "        return f'Done: {countDone} Accuracy: {((countDone - countFailed)/countDone)*100}%'\n",
    "    else:\n",
    "        driver.quit()\n",
    "        main_getoperator()\n",
    "#         df['Length'] = df['Operator'].str.len()\n",
    "#         for index, row in df.iterrows():\n",
    "#             notFound = False\n",
    "#             # VERIFY IF IT'S GONNA WORK ON THURSDAY OR CHANGE THE ISWEEKDAY\n",
    "#             if row[\"Operator\"] == \"ToDo\" or (datetime.datetime.today().isoweekday() == 4 and row[\"Operator\"] == \"Error\"): # or row['Length'] > 75:\n",
    "#         #         print(row['GYG Link'], row['Tytul'])\n",
    "#                 url = row['Link']\n",
    "#                 driver.get(url)\n",
    "#                 try:\n",
    "#                     elem = driver.find_element(By.CLASS_NAME, \"supplierName__1So1\") #supplierName__1JZV\n",
    "#                 except:\n",
    "#                     notFound = True\n",
    "\n",
    "# # IF WAS NOT FOUND IN BASE APPROACH PAGE REFRESH\n",
    "#                 if notFound == True:\n",
    "#                     time.sleep(1)\n",
    "#                     driver.get(url)\n",
    "#                     try:\n",
    "#                         elem = driver.find_element(By.CLASS_NAME, \"supplierName__1So1\") #supplierName__1JZV\n",
    "#                     except:\n",
    "#                         notFound = False\n",
    "#                     # CHECK OPEN SEE MORE\n",
    "#                     if notFound == False:\n",
    "# # try to open see more\n",
    "#                         try:\n",
    "#                             additional_info_section = driver.find_element(By.XPATH, \"//div[@data-automation='additional-info-section']\")\n",
    "#                             see_more_button = additional_info_section.find_element(By.CLASS_NAME, 'seeMoreLink__2-eS')\n",
    "#                             driver.execute_script(\"arguments[0].scrollIntoView();\", see_more_button)\n",
    "#                             see_more_button.click()\n",
    "# #                             try:\n",
    "# #                                 see_more_buttons = driver.find_all(By.CLASS_NAME,'seeMoreLink__2-eS')\n",
    "# #                                 print(see_more_buttons)\n",
    "# #                                 for buttons in see_more_buttons:\n",
    "# #                                     print(buttons.text.strip())\n",
    "# #                                     driver.execute_script(\"arguments[0].scrollIntoView();\", buttons)\n",
    "# #                                     buttons.click()\n",
    "# #                                     try:\n",
    "# #                                         elem = driver.find_element(By.CLASS_NAME, \"supplierName__1So1\") #supplierName__1JZV\n",
    "# #                                         Found = 'YYYY'\n",
    "# #                                         df.at[index,'Operator'] = elem.text\n",
    "# #                                     except:\n",
    "# #                                         pass\n",
    "# #                                     driver.get(url)\n",
    "# #                                     time.sleep(1)\n",
    "# #                             except:\n",
    "# #                                 print('No for luup')\n",
    "\n",
    "#                         except:\n",
    "#                             pass                       \n",
    "#                         try:\n",
    "#                             elem = driver.find_element(By.CLASS_NAME, \"supplierName__1So1\") #supplierName__1JZV\n",
    "#                             Found = 'YYY'\n",
    "#                             df.at[index,'Operator'] = re.sub('[^A-Za-z0-9 ]+', '', elem.text)\n",
    "#                         except:\n",
    "#                             notFound = True\n",
    "#                             df.at[index,'Operator'] = 'Incorrect URL'\n",
    "#                             Found = \"N\"\n",
    "#                             countFailed = countFailed + 1\n",
    "#                     else:\n",
    "#                         if len(elem.text) > 75:\n",
    "#                             df.at[index,'Operator'] = 'Incorrect_Operator'\n",
    "#                             Found = \"Long\"\n",
    "#                         else:\n",
    "#                             df.at[index,'Operator'] = re.sub('[^A-Za-z0-9 ]+', '', elem.text)\n",
    "#                             Found = \"YY\"\n",
    "#                     countDone = countDone + 1  \n",
    "#                 else:\n",
    "#                     if len(re.sub('[^A-Za-z0-9 ]+', '', elem.text)) > 75:\n",
    "#                         df.at[index,'Operator'] = 'Incorrect_Operator'\n",
    "#                         Found = \"Long\"\n",
    "#                     else:\n",
    "#                         df.at[index,'Operator'] = re.sub('[^A-Za-z0-9 ]+', '', elem.text)\n",
    "#                         Found = \"Y\"\n",
    "#                     countDone = countDone + 1\n",
    "\n",
    "\n",
    "#                 print(f'Index: {index} | Total time: {time.time() - timeS} | Avg per record: {(time.time() - timeS) / countDone} | Total done| {countDone} | {Found} - {((countDone - countFailed)/countDone)*100}%')\n",
    "\n",
    "#                 if countDone % 100 == 0:\n",
    "#                     print('INSERTING DF TO EXCEL')\n",
    "#                     df.to_csv(path_main_file, index=False)\n",
    "#         end = time.time()\n",
    "#         df.drop(columns='Length', inplace=True)\n",
    "#         df.to_csv(path_main_file, index=False)            \n",
    "#         driver.quit()\n",
    "#         return f'Done: {countDone} Accuracy: {((countDone - countFailed)/countDone)*100}% Time taken: {round((end - start)/60,4)} min'\n",
    "\n",
    "## function testing\n",
    "# get_operators_name_from_chrome(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2689246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO FIX ANY DUPLCIATES IN OPERATOR UID \n",
    "\n",
    "\n",
    "# Function to extract UID from URL\n",
    "def extract_uid_gyg(link):\n",
    "    try:\n",
    "        # Split the link by '-' and take the last part\n",
    "        parts = link.split('-')\n",
    "        return parts[-1].replace('/', '').lower()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def extract_uid_viator(link):\n",
    "    try:\n",
    "        # Split the link by '/' and take the last part\n",
    "        parts = link.split('/')\n",
    "        return parts[-1].lower()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "\n",
    "# df = pd.read_excel(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx')\n",
    "# df_raw = pd.read_excel(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx')\n",
    "def clean_add_uid(site, df):\n",
    "    df = df[df['Link'].str.strip().str.len() > 1]\n",
    "    # Convert the 'Link' column to string\n",
    "    # Apply the function to create a new column 'UID'\n",
    "    if site == 'GYG':\n",
    "        df['uid'] = df['Link'].apply(extract_uid_gyg)\n",
    "    elif site == 'Viator':\n",
    "        df['uid'] = df['Link'].apply(extract_uid_viator)\n",
    "    # Convert 'Date' column to datetime format\n",
    "    df['Date input'] = pd.to_datetime(df['Date input'])\n",
    "    # Sort by 'UID' and 'Date' to get the latest entry for each UID\n",
    "    df = df.sort_values(by=['uid', 'Date input'], ascending=[True, False])\n",
    "    # Drop duplicates to keep only the latest URL for each UID\n",
    "    df = df.drop_duplicates(subset='uid')\n",
    "    # Reset index for clarity\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# df.to_csv(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.csv',index=False)\n",
    "# df_viator.to_csv(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380cf511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(data, cursor, insert_query):\n",
    "    midpoint = int(round(len(data) / 2,0))\n",
    "    print(midpoint)\n",
    "    if len(data) == 20:  # Base case: only one row left.\n",
    "        try:\n",
    "            cursor.execute(insert_query, data[0])\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error encountered with data {data[0]}: {e}\")\n",
    "        return\n",
    "    \n",
    "    first_half = data[:midpoint]\n",
    "    print(first_half)\n",
    "    second_half = data[midpoint:]\n",
    "    print(second_half)\n",
    "\n",
    "    try:\n",
    "        cursor.executemany(insert_query, first_half)\n",
    "    except pyodbc.Error:\n",
    "        insert_data(first_half, cursor, insert_query)  # Recursively handle the problematic half\n",
    "\n",
    "    try:\n",
    "        cursor.executemany(insert_query, second_half)\n",
    "    except pyodbc.Error:\n",
    "        insert_data(second_half, cursor, insert_query)  # Recursively handle the problematic half\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367c3251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_problematic_bytes(file_path, encoding='windows-1252'):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        byte_position = 0\n",
    "        try:\n",
    "            # Read the file byte by byte\n",
    "            while byte := file.read(1):\n",
    "                # Attempt to decode the byte(s)\n",
    "                byte.decode(encoding)\n",
    "                byte_position += 1\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Log the position and the byte that caused the error\n",
    "            print(f\"Error at byte position {byte_position}: {e}\")\n",
    "            print(f\"Problematic byte sequence: {byte}\")\n",
    "            \n",
    "            # Optionally, read a few more bytes to get more context\n",
    "            context = file.read(500)\n",
    "            print(f\"Following bytes: {context}\")\n",
    "            \n",
    "            # Handle the error (skip, replace, etc.) or exit\n",
    "            # ...\n",
    "\n",
    "# Usage\n",
    "# find_problematic_bytes(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a28cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_df_to_sql_db(path_df_main, database_name):\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    import time\n",
    "    from io import BytesIO\n",
    "\n",
    "    df_main = pd.read_excel(path_df_main, sheet_name='AllLinks', engine='openpyxl')\n",
    "    df_main['Reviews'] = df_main['Reviews'].fillna(0)\n",
    "    df_main['Operator'] = df_main['Operator'].fillna('Error')\n",
    "    df_main['Tytul'] = df_main['Tytul'].fillna('Error')\n",
    "    df_main = df_main[df_main['City'].str.len() >= 3]\n",
    "\n",
    "    if 'GYG' in path_df_main:\n",
    "        table_name = 'Operators_GYG'\n",
    "        df_main = clean_add_uid('GYG', df_main)\n",
    "    else:\n",
    "        table_name = 'Operators_Viator'\n",
    "        df_main = clean_add_uid('Viator', df_main)\n",
    "    df_main = df_main.drop_duplicates(subset=['uid'])\n",
    "\n",
    "    server = 'sqlserver-myotas.database.windows.net'\n",
    "    database = database_name\n",
    "    username = 'azureadmin'\n",
    "    password = 'brudnyHarry!66'\n",
    "    driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "    try:\n",
    "        cnxn = pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{server};PORT=1433;DATABASE={database};UID={username};PWD={password}')\n",
    "        print('Connected')\n",
    "    except:\n",
    "        return \"Couldn't connect to database\"\n",
    "\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.fast_executemany = True\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    create_table_query = f\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='{table_name}' AND xtype='U')\n",
    "        CREATE TABLE [dbo].[{table_name}] (\n",
    "            [Tytul]       NVARCHAR (MAX) NULL,\n",
    "            [Link]        NVARCHAR (MAX) NULL,\n",
    "            [City]        NVARCHAR (255) NULL,\n",
    "            [Operator]    NVARCHAR (MAX) NULL,\n",
    "            [Reviews]     NVARCHAR (255) NULL,\n",
    "            [Date input]  NVARCHAR (255) NULL,\n",
    "            [Date update] NVARCHAR (255) NULL,\n",
    "            [uid]         NVARCHAR (255) NOT NULL PRIMARY KEY\n",
    "        );\n",
    "    \"\"\"\n",
    "    print('Ensuring table exists...')\n",
    "    cursor.execute(create_table_query)\n",
    "    cnxn.commit()\n",
    "\n",
    "    # Upsert query\n",
    "    merge_query = f\"\"\"\n",
    "        MERGE [dbo].[{table_name}] AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?)) AS source (([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews]))\n",
    "        ON target.[uid] = source.[uid]\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                target.[Tytul] = source.[Tytul],\n",
    "                target.[Link] = source.[Link],\n",
    "                target.[City] = source.[City],\n",
    "                target.[Operator] = source.[Operator],\n",
    "                target.[Date input] = source.[Date input],\n",
    "                target.[Date update] = source.[Date update],\n",
    "                target.[Reviews] = source.[Reviews]\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews])\n",
    "            VALUES (source.[Tytul], source.[Link], source.[City], source.[Operator], source.[Date input], source.[Date update], source.[uid], source.[Reviews]);\n",
    "    \"\"\"\n",
    "    data_list = [tuple(row) for row in df_main.values]\n",
    "    print('Upserting data...')\n",
    "    try:\n",
    "        cursor.executemany(merge_query, data_list)\n",
    "        cnxn.commit()\n",
    "        print(f'Successfully upserted: {len(data_list)} rows')\n",
    "    except pyodbc.DataError as e:\n",
    "        print(e)\n",
    "\n",
    "    cnxn.close()\n",
    "    return f'Successfully upserted: {len(data_list)} rows to {table_name} table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98ce851",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_df_to_sql_db(path_df_main, database_name):\n",
    "    # Open the file in binary mode and read the content\n",
    "    # with open(path_df_main, 'rb') as file:\n",
    "    #     binary_content = file.read()\n",
    "\n",
    "    # # Use pandas to read the binary content directly\n",
    "    # # We use the 'io.BytesIO' to create an in-memory binary stream\n",
    "    # # Create an in-memory binary stream\n",
    "    # data_stream = io.BytesIO(binary_content)\n",
    "\n",
    "    # # Read the stream into a DataFrame using the appropriate engine\n",
    "    df_main = pd.read_excel(path_df_main, sheet_name=0, engine='openpyxl')\n",
    "    df_main['Reviews'] = df_main['Reviews'].fillna(0)\n",
    "    df_main['Operator'] = df_main['Operator'].fillna('Error')\n",
    "    df_main['Tytul'] = df_main['Tytul'].fillna('Error')\n",
    "    df_main = df_main[df_main['City'].str.len() >= 3]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     df_main['Date input'] = pd.to_datetime(df_main['Date input'])\n",
    "#     df_main['Date update'] = pd.to_datetime(df_main['Date update'])\n",
    "    # Define the table name\n",
    "    if 'GYG' in path_df_main:\n",
    "        table_name = 'Operators_GYG'\n",
    "        #### CLEAN FROM UID AND DUPLCIATED URL BASED ON UID\n",
    "        df_main = clean_add_uid('GYG', df_main)\n",
    "    else:\n",
    "        table_name = 'Operators_Viator'\n",
    "        #### CLEAN FROM UID AND DUPLCIATED URL BASED ON UID\n",
    "        df_main = clean_add_uid('Viator', df_main)\n",
    "    df_main = df_main.drop_duplicates(subset=['uid'])\n",
    "    server = 'sqlserver-myotas.database.windows.net'\n",
    "    database = database_name\n",
    "    username = 'azureadmin'\n",
    "    password = paaas  \n",
    "    driver = '{ODBC Driver 18 for SQL Server}'\n",
    "    try:\n",
    "        cnxn = pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "        print('Connected')\n",
    "    except:\n",
    "        return \"Coundn't connect to database\"\n",
    "\n",
    "    # Read data into pandas dataframe\n",
    "    data = df_main\n",
    "\n",
    "    # Create a cursor and set fast_executemany to True\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.fast_executemany = True\n",
    "    # Drop the table if it exists\n",
    "    start_drop_table = time.time()\n",
    "    print('Droping table...')\n",
    "    cursor.execute(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name}\")\n",
    "    end_drop_table = time.time()\n",
    "    # Create the table\n",
    "    start_create_table = time.time()\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE [dbo].[{table_name}] (\n",
    "        [Tytul]       NVARCHAR (MAX) NULL,\n",
    "        [Link]        NVARCHAR (MAX) NULL,\n",
    "        [City]        NVARCHAR (255) NULL,\n",
    "        [Operator]    NVARCHAR (MAX) NULL,\n",
    "        [Reviews]     NVARCHAR (255) NULL,\n",
    "        [Date input]  NVARCHAR (255) NULL,\n",
    "        [Date update] NVARCHAR (255) NULL,\n",
    "        [uid]         NVARCHAR (255) NOT NULL,\n",
    "        CONSTRAINT [PK_{table_name}] PRIMARY KEY CLUSTERED ([uid] ASC)\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # CONSTRAINT [PK_{table_name}] PRIMARY KEY CLUSTERED ([uid] ASC)\n",
    "    print('Creating table...')\n",
    "    cursor.execute(create_table_query)\n",
    "    end_create_table = time.time()\n",
    "    # Note start time\n",
    "    start = time.time()\n",
    "\n",
    "    # Use executemany to insert data into the table\n",
    "    insert_query = f\"INSERT INTO {table_name} ([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews])\\\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    data_list = [tuple(row) for row in data.values]\n",
    "    # print(data_list)\n",
    "    print('Insert data...')\n",
    "    try:\n",
    "        cursor.executemany(insert_query, data_list)\n",
    "        cnxn.commit()\n",
    "        print(f'Sucessfully exectued inserted: {len(data_list)} rows')\n",
    "    except pyodbc.DataError as e:\n",
    "        ## Print the error message and the row causing the error\n",
    "        print(e)\n",
    "    # # insert_data(data_list, cursor, insert_query)\n",
    "    # for i, row in enumerate(data_list):\n",
    "    #     print(i,row)\n",
    "    #     try:\n",
    "    #         cursor.execute(insert_query, row)\n",
    "    #     except pyodbc.DataError:\n",
    "    #         print(f\"Row {i}: {row}\")\n",
    "    #     cnxn.commit()\n",
    "\n",
    "    # # Commit the changes and close the connection\n",
    "    cnxn.close()\n",
    "\n",
    "    # Calculate and print the execution time\n",
    "    end = time.time()\n",
    "    print(f\"Drop time: {round(end_drop_table - start_drop_table, 4) } Create time: {round(end_create_table - start_create_table,4)} Execution time: {round(end - start,4)} seconds\")\n",
    "    return f'Sucesfully inserted: {len(data_list)} rows to {table_name} table in {round(end - start,4)} seconds'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8a6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_df_main = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv'\n",
    "# # path_df_main = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.csv'\n",
    "# df_main = pd.read_csv(path_df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a11743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_daily_to_sql(file_path):\n",
    "    # Set up database connection details\n",
    "    server = 'sqlserver-myotas.database.windows.net'\n",
    "    database = 'OTAs'\n",
    "    username = 'azureadmin'\n",
    "    password = 'brudnyHarry!66'   \n",
    "    driver = '{ODBC Driver 18 for SQL Server}'\n",
    "    print(pyodbc.drivers())\n",
    "    drive_path = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs'\n",
    "    viator_daily_path = rf'{drive_path}\\Baza Excel\\Viator\\Daily\\\\'\n",
    "    viator_report_path = rf'{drive_path}\\Baza Excel\\Viator\\Daily\\ImportReports\\\\'\n",
    "    gyg_report_path = rf'{drive_path}\\Baza Excel\\Get Your Guide\\ImportReports\\\\'\n",
    "    gyg_daily_path = rf'{drive_path}\\Baza Excel\\Get Your Guide\\\\'\n",
    "    try:\n",
    "        cnxn = pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password, timeout=300)\n",
    "    except:\n",
    "        return \"Coundn't connect to database - retry\"\n",
    "\n",
    "    # Get path to Excel file from user input\n",
    "#     excel_path = input(\"Enter path to Excel file: \")\n",
    "    if '||' in file_path:\n",
    "        files_upload = file_path.split('||')\n",
    "    else:\n",
    "        print('Notsplited')\n",
    "        files_upload = file_path\n",
    "    \n",
    "    for file_upload in files_upload:\n",
    "        excel_path = file_upload\n",
    "        # excel_path = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\Daily\\TestVPN_Viator - 2023-05-10.xlsx'\n",
    "        # List of sheet names to exclude\n",
    "        exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "        date_of_import = excel_path.split()[-1].split('.')[0]\n",
    "\n",
    "        # Save report to file ImportOfVPN_Viator - 2023-05-10.txt\n",
    "        if 'Viator' in excel_path:\n",
    "            report_path = f\"{viator_report_path}ImportOfVPN_Viator - {date_of_import}.txt\"\n",
    "            folder_path = viator_report_path\n",
    "            excel_path = f\"{viator_daily_path}{file_upload}\"\n",
    "            header = ['Tytul', 'Tytul Url', 'Cena', 'Opinia','IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto']\n",
    "#             header = ['Tytul', 'Tytul Url', 'Cena', 'IloscOpini', 'Opinia', 'RozmiarCena', 'Data zestawienia', 'Pozycja', 'Kategoria', 'SiteUse', 'Miasto']\n",
    "        elif 'GYG' in excel_path:\n",
    "            report_path = f\"{gyg_report_path}ImportOfGYG - {date_of_import}.txt\"\n",
    "            folder_path = gyg_report_path\n",
    "            excel_path = f\"{gyg_daily_path}{file_upload}\"\n",
    "#             header = ['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'VPN_City', 'Booked', 'SiteUse', 'Miasto']\n",
    "            header = ['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'Booked', 'SiteUse', 'Miasto', 'VPN_City']\n",
    "\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        if any(date_of_import in file for file in files):\n",
    "            print(f'Import report already exisit for file {file_path}')\n",
    "            print(f'Import report already exisit for file {file_upload}')\n",
    "    #         continue\n",
    "        else:\n",
    "            # Load Excel file into pandas dataframe\n",
    "            xls = pd.ExcelFile(excel_path)\n",
    "\n",
    "            # Initialize report string\n",
    "            report_str = \"\"\n",
    "            cursor = cnxn.cursor()\n",
    "            cursor.fast_executemany = True\n",
    "            i = 1\n",
    "            # Iterate over each sheet in the Excel file\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                # Skip excluded sheets\n",
    "                if sheet_name in exclude_sheets:\n",
    "                    print('next', sheet_name)\n",
    "                    continue\n",
    "                print(f'{i} - {sheet_name}')\n",
    "                start = time.time()\n",
    "                df = pd.read_excel(excel_path, sheet_name=sheet_name, header=None)\n",
    "\n",
    "                print(f'DF read {round(time.time() - start, 4)}s')\n",
    "                df.columns = header\n",
    "                df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "                df['IloscOpini'] = df['IloscOpini'].fillna(0)\n",
    "                df['Opinia'] = df['Opinia'].fillna('N/A')\n",
    "                df = df[df['Tytul'] != 'Tytul']\n",
    "                df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "                df = df[df['Data zestawienia'].str.len() > 4]\n",
    "#                 display(df)\n",
    "                if sheet_name == 'Mt-Vesuvius':\n",
    "                    sheet_name = 'Mount-Vesuvius'\n",
    "                    df['Miasto'] = 'Mount-Vesuvius'\n",
    "                # Insert Dataframe into SQL Server:\n",
    "                if 'Viator' in excel_path:\n",
    "                    insert_query = f\"INSERT INTO [{sheet_name}] ([Tytul], [Tytul Url], [Cena], [Opinia], [IloscOpini],\\\n",
    "                    [Data zestawienia], [Pozycja], [Kategoria], [SiteUse], [Miasto])\\\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "                    df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "                    df = df.drop(columns=('Przecena'))\n",
    "                    df = df.drop(columns=('Tekst'))\n",
    "                    \n",
    "                    \n",
    "                elif 'GYG' in excel_path:\n",
    "#                     insert_query = f\"INSERT INTO [{sheet_name}] ([Tytul], [Tytul Url], [Cena], [Opinia], [IloscOpini], [Przecena],\\\n",
    "#                     [Tekst], [Data zestawienia], [Pozycja], [Kategoria], [VPN_City], [Booked], [SiteUse], [Miasto])\\\n",
    "#                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "                    insert_query = f\"INSERT INTO [{sheet_name}] ([Tytul], [Tytul Url], [Cena], [Opinia], [IloscOpini], [Przecena],\\\n",
    "                    [Data zestawienia], [Pozycja], [Kategoria], [Booked], [SiteUse], [Miasto], [VPN_City])\\\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "##                    USED WHEN IMPORT GYG FROM PYTHON\n",
    "                    df = df.drop(columns=('Tekst'))\n",
    "                    df['Booked'] = df['Booked'].astype('str')\n",
    "                    df['Przecena'] = df['Przecena'].astype('str')\n",
    "                    df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "                    df['Booked'] = df['Booked'].map(lambda x: x.split('Booked')[1].split()[0] if len(x) > 5 else x)\n",
    "                    df['Przecena'] = df['Przecena'].map(lambda x: x.split()[1].replace(\",\", \"\") if len(x) > 4 else x)\n",
    "##                  _________________\n",
    "                    df['Przecena'] = df['Przecena'].fillna(\"NULL\")\n",
    "                    df['VPN_City'] = df['VPN_City'].fillna(\"NULL\")\n",
    "                    df['Booked'] = df['Booked'].fillna(\"NULL\")\n",
    "\n",
    "                data_list = [tuple(row) for row in df.values]\n",
    "#                 print(data_list)\n",
    "\n",
    "\n",
    "            # FOR TESTING PURPOSE IN CASE OF ANY ERROR\n",
    "#                 for i, row in enumerate(data_list):\n",
    "#                     print(i, row)\n",
    "#                     try:\n",
    "#                         cursor.execute(insert_query, row)\n",
    "#                     except pyodbc.DataError:\n",
    "#                         print(f\"Row {i}: {row}\")\n",
    "#                     cnxn.commit()\n",
    "            ##############################\n",
    "\n",
    "                start_1 = time.time()\n",
    "                try:\n",
    "                    cursor.executemany(insert_query, data_list)\n",
    "                    cnxn.commit()\n",
    "            #         print(f'Sucessfully exectued inserted: {len(data_list)} rows')\n",
    "                except pyodbc.DataError as e:\n",
    "                    # Print the error message and the row causing the error\n",
    "                    print(e)\n",
    "                    print(e.with_traceback())\n",
    "                print(f'DF insert {round(time.time() - start_1, 4)}s')\n",
    "                report_str += f\"\\n{i} - {sheet_name} \\n Import successful for sheet: {sheet_name}\\n Sucessfully exectued inserted: {len(data_list)} rows \\n\"\n",
    "                i = i +1\n",
    "\n",
    "            #     except:\n",
    "            #         report_str += f\"Import failed for sheet: {sheet_name}\\n\"\n",
    "            #         print(report_str)\n",
    "\n",
    "            # Close database connection\n",
    "            \n",
    "            print(report_str)\n",
    "\n",
    "\n",
    "            with open(report_path, \"w\") as f:\n",
    "                f.write(report_str)\n",
    "\n",
    "            print(\"Data upload complete!\")\n",
    "    cursor.close()\n",
    "    cnxn.close()\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c0e9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.xlsx\n",
      "Connected\n",
      "Droping table...\n",
      "Creating table...\n",
      "Insert data...\n",
      "Sucessfully exectued inserted: 113291 rows\n",
      "Drop time: 0.0396 Create time: 0.039 Execution time: 520.2754 seconds\n",
      "Connected\n",
      "Droping table...\n",
      "Creating table...\n",
      "Insert data...\n",
      "Sucessfully exectued inserted: 113291 rows\n",
      "Drop time: 0.0367 Create time: 0.0398 Execution time: 498.6801 seconds\n"
     ]
    }
   ],
   "source": [
    "def start():\n",
    "\n",
    "    sg.theme('DarkBlue2')  # please make your windows colorful\n",
    "\n",
    "    layout = [\n",
    "        [\n",
    "            sg.Image(filename='logo_color.png', key='image')\n",
    "        ], \n",
    "#               [sg.Text('_'  * 30)], \n",
    "#               [sg.FileBrowse('Browse File', file_types=((\"Excel File\", \"*.xlsx\"),), size=(36, 1), key=\"File\"),\n",
    "#               [sg.Text('_'  * 60)], \n",
    "#                sg.InputText('asdasd', size =(75, 2), key='PathExcel'),\n",
    "#                ],\n",
    "        \n",
    "         [\n",
    "            sg.Button('GYG file (Operators)', size=(20,1)),    \n",
    "            sg.Button('Viator file (Operators)', size=(20,1)),\n",
    "        ],\n",
    "        \n",
    "        [\n",
    "            sg.Text('Path main: '),\n",
    "            sg.Input('', size= (75,1), tooltip='Main excel file', key='FilePath')\n",
    "#             sg.Text('Excel path GYG/Viator: '),\n",
    "#             sg.InputText('', size= (30,2), tooltip='Excel file'),\n",
    "#             sg.Button('Get Operators Name'),\n",
    "#             sg.Button('Insert Data to SQL')\n",
    "        ],\n",
    "        [\n",
    "              \n",
    "            sg.Button(\"Add Today GYG (AllLinks)\", size=(20,1)),\n",
    "            sg.Button(\"Add Today Viator (AllLinks)\", size=(20,1))\n",
    "        ],\n",
    "        [\n",
    "            sg.Text('All Links Path: '),\n",
    "            sg.Input('', size= (70,1), tooltip='All Links excel file', key='Date_Insert'),            \n",
    "        ],\n",
    "        [\n",
    "            sg.Button(\"Insert New Links\"),\n",
    "            sg.Button('Get Operators Name'),\n",
    "            sg.Button('Insert Data to SQL')\n",
    "            \n",
    "            \n",
    "        ],\n",
    "        [\n",
    "            [sg.Text('_' * 90, pad=(0, 10))],\n",
    "        ],\n",
    "        [\n",
    "              \n",
    "            sg.Button(\"Add Today GYG (Daily)\", size=(20,1)),\n",
    "            sg.Button(\"Add Today Viator (Daily)\", size=(20,1)),\n",
    "            sg.Button(\"Add Today files (Daily)\", size=(20,1)),\n",
    "            sg.Button(\"Collect: All Links Viator\", size=(20,1))\n",
    "            \n",
    "        ],\n",
    "        [\n",
    "            sg.Text('Daily Path: '),\n",
    "            sg.Input('', size= (75,1), tooltip='Daily excel file', key='Daily_file'),            \n",
    "        ],\n",
    "        [\n",
    "            sg.Button(\"Upload daily file to DB\"),           \n",
    "            \n",
    "        ],\n",
    "        [\n",
    "            sg.Text('---', size=(70,1), key='Done'),\n",
    "            sg.Button('Exit')\n",
    "        ]\n",
    "        \n",
    "        ]\n",
    "\n",
    "    window = sg.Window('Get Data MyOTAs', layout, default_element_size=(50, 1), grab_anywhere=False, keep_on_top=False)\n",
    "    \n",
    "    \n",
    "    while True:  # Event Loop\n",
    "        filePath = ''\n",
    "        date_add = ''\n",
    "        event, values = window.Read()\n",
    "        if event == 'Insert New Links':\n",
    "#             print(f'VALUES IN FUNCTION WILL BE{values['FilePath']},{values['Date_Insert']}')\n",
    "            window['Insert New Links'].Update(button_color=('black','yellow')) \n",
    "            window['Done'].update('Working...')\n",
    "            window.Refresh()\n",
    "#             temp_val = \n",
    "#                 day_file_path = fr'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Get Your Guide\\AllLinks\\All Links GYG - {temp_val}.xlsx'\n",
    "#             day_file_path = temp_val\n",
    "            print(values['FilePath'],values['Date_Insert'],'AllLinks')\n",
    "            text = insert_new_links(values['FilePath'],values['Date_Insert'],'AllLinks')\n",
    "#             current_time = datetime.datetime.now()             \n",
    "            window['Insert New Links'].Update(button_color=('white','black')) \n",
    "            window['Done'].update(f'{text}')\n",
    "            window.Refresh()\n",
    "        if event == 'GYG file (Operators)':\n",
    "            window['GYG file (Operators)'].Update(button_color=('black','yellow')) \n",
    "            window['Viator file (Operators)'].Update(button_color=('black','white')) \n",
    "            #Reset color of buttons\n",
    "            window['Insert New Links'].Update(button_color=('black','white')) \n",
    "            window['Get Operators Name'].Update(button_color=('black','white'))\n",
    "            window['Insert Data to SQL'].Update(button_color=('black','white')) \n",
    "            filePath = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx'\n",
    "            window['FilePath'].update(filePath)\n",
    "            window.Refresh()\n",
    "        if event == 'Viator file (Operators)':\n",
    "            window['GYG file (Operators)'].Update(button_color=('black','white')) \n",
    "            window['Viator file (Operators)'].Update(button_color=('black','yellow')) \n",
    "            #Reset color of buttons\n",
    "            window['Insert New Links'].Update(button_color=('black','white')) \n",
    "            window['Get Operators Name'].Update(button_color=('black','white'))\n",
    "            window['Insert Data to SQL'].Update(button_color=('black','white')) \n",
    "            filePath = r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.xlsx'\n",
    "            window['FilePath'].update(filePath)\n",
    "            window.Refresh()\n",
    "        if event == 'Get Operators Name':\n",
    "            #DEBUG COMMENT - ask if VPN is on to run script\n",
    "            answer = sg.popup_yes_no('Do you have VPN on?')\n",
    "            if answer == 'No':\n",
    "                break\n",
    "            \n",
    "            if len(values['FilePath']) < 5:\n",
    "                    pass\n",
    "                    print('Empty')\n",
    "            else:\n",
    "                print(values['FilePath'])\n",
    "                window['Done'].update('Working...')\n",
    "                window.Refresh()\n",
    "                text = get_operators_name_from_chrome(values['FilePath'])\n",
    "                window['Get Operators Name'].Update(button_color=('white','black')) \n",
    "                window['Done'].update(f'{text}')\n",
    "                window.Refresh()\n",
    "        if event == 'Add Today GYG (AllLinks)':\n",
    "            date_add = datetime.date.today()\n",
    "            path = fr'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Get Your Guide\\All Links\\All Links GYG - {date_add}.xlsx'\n",
    "            window['Date_Insert'].update(path)\n",
    "            window['Add Today GYG (AllLinks)'].Update(button_color=('black','yellow')) \n",
    "            window['Add Today Viator (AllLinks)'].Update(button_color=('black','white')) \n",
    "            window.Refresh()\n",
    "#             print(values['FilePath'],values['Date_Insert'])\n",
    "        if event == 'Add Today Viator (AllLinks)':\n",
    "            date_add = datetime.date.today()\n",
    "            path = fr'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\All Links\\All Links Viator - {date_add}.xlsx'\n",
    "            window['Date_Insert'].update(path)\n",
    "            window['Add Today Viator (AllLinks)'].Update(button_color=('black','yellow')) \n",
    "            window['Add Today GYG (AllLinks)'].Update(button_color=('black','white')) \n",
    "            window.Refresh()\n",
    "#             print(values['FilePath'],values['Date_Insert'])\n",
    "\n",
    "        if event == 'Insert Data to SQL':\n",
    "            print(values['FilePath'])\n",
    "            \n",
    "            window['Done'].update('Working...')\n",
    "            window.Refresh()\n",
    "            text = upsert_df_to_sql_db(values['FilePath'], 'OTAs')\n",
    "            window['Done'].update(f'{text}')\n",
    "            text_2 = upsert_df_to_sql_db(values['FilePath'], 'db_ota_future_price')\n",
    "            text = \"OTAs:\" + text + \"\\n\" + \"db_ota_future_prcie\" + text_2\n",
    "            window['Done'].update(f'{text}')\n",
    "            window['Insert Data to SQL'].Update(button_color=('white','black')) \n",
    "            window.Refresh()\n",
    "            \n",
    "        if event == 'Add Today GYG (Daily)':\n",
    "            date_add = datetime.date.today()\n",
    "            path = fr'GYG - {date_add}.xlsx'\n",
    "            window['Daily_file'].update(path)\n",
    "            window['Add Today GYG (Daily)'].Update(button_color=('black','yellow')) \n",
    "            window['Add Today Viator (Daily)'].Update(button_color=('black','white')) \n",
    "            window.Refresh()\n",
    "#             print(values['FilePath'],values['Date_Insert'])\n",
    "        if event == 'Add Today Viator (Daily)':\n",
    "            date_add = datetime.date.today()\n",
    "            path = fr'Viator - {date_add}.xlsx'\n",
    "            window['Daily_file'].update(path)\n",
    "            window['Add Today Viator (Daily)'].Update(button_color=('black','yellow')) \n",
    "            window['Add Today GYG (Daily)'].Update(button_color=('black','white')) \n",
    "            window.Refresh()\n",
    "        if event == 'Add Today files (Daily)':\n",
    "            date_add = datetime.date.today()\n",
    "            path = fr'Viator - {date_add}.xlsx||GYG - {date_add}.xlsx'\n",
    "            window['Daily_file'].update(path)\n",
    "            window['Add Today files (Daily)'].Update(button_color=('black','yellow')) \n",
    "            window['Add Today Viator (Daily)'].Update(button_color=('black','white'))\n",
    "            window['Add Today GYG (Daily)'].Update(button_color=('black','white')) \n",
    "            \n",
    "            window.Refresh()\n",
    "                     \n",
    "        if event == 'Upload daily file to DB':\n",
    "            print(values['Daily_file'])\n",
    "            window['Done'].update('Working...')\n",
    "            window.Refresh()\n",
    "            text = upload_daily_to_sql(values['Daily_file'])\n",
    "            window['Done'].update(f'{text}')\n",
    "            window['Upload daily file to DB'].Update(button_color=('white','black')) \n",
    "            window.Refresh()\n",
    "        if event ==  'Exit' or event == sg.WIN_CLOSED:      \n",
    "            break    \n",
    "        if event == \"Collect: All Links Viator\":\n",
    "            window['Done'].update('Working...')\n",
    "            window.Refresh()\n",
    "            text = main_alllinks()\n",
    "            window['Done'].update(f'{text}')\n",
    "            window.Refresh()\n",
    "\n",
    "    window.close()\n",
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fc649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27d87cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_main = pd.read_csv(r'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Groups.csv')\n",
    "# display(df_main)\n",
    "# #Based on input read files\n",
    "\n",
    "# df_day = pd.read_excel(fr'G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Baza Excel\\Viator\\All Links\\All Links Viator - 2023-08-22.xlsx', 'AllLinks')\n",
    "\n",
    "# #SHORTEN DFS\n",
    "# df_day = df_day[df_day['Miasto'] == 'Amsterdam'] #short version for testing\n",
    "# display(df_day)\n",
    "# # df_main = df_main.head(5000) #short version for testing\n",
    "# #____\n",
    "\n",
    "# df_day = df_day[['Tytul', 'Tytul URL', 'IloscOpini' ,'Data zestawienia', 'Miasto']]    \n",
    "# df_day.rename(columns={'Tytul URL': 'Link', 'IloscOpini': 'Reviews', 'Miasto': 'City', 'Data zestawienia': 'Date input'}, inplace=True)\n",
    "# df_day.insert(len(df_day.columns), 'Date update', df_day['Date input'])\n",
    "# df_day.insert(len(df_day.columns),'Operator', 'ToDo')\n",
    "# df_day.drop_duplicates('Link', inplace=True)\n",
    "# df_day['Link'] = df_day['Link'].str.lower()\n",
    "# df_day['Link'] = df_day['Link'].str.replace(\"'\", \"\", regex=True)\n",
    "# match = 0\n",
    "# added = 0\n",
    "# amnt_do = df_day['Link'].count()\n",
    "# #METHOD ONE\n",
    "# for index, row in df_day.iterrows():\n",
    "#     if index % 598 == 0:\n",
    "#         print(f'Processed { round((index / amnt_do) * 100, 2)}%')\n",
    "#     if len(df_main[df_main['Link'] == row['Link']]) == 1: # checking if URL matches exisitng one\n",
    "#         index_main = df_main[df_main['Link'] == row['Link']].index\n",
    "#         df_main.at[index_main[0], 'Date update'] = row['Date input']\n",
    "#         df_main.at[index_main[0], 'Reviews'] = row['Reviews']\n",
    "#         df_main.at[index_main[0], 'Tytul'] = row['Tytul']\n",
    "#         match = match + 1\n",
    "#     else:\n",
    "#         df_main = pd.concat([df_main, row.to_frame().T])     \n",
    "#         added = added + 1\n",
    "#         print('Adding...')\n",
    "#         print(df_day[df_day['Link'] == row['Link']]['Reviews'] == df_main[df_main['Link'] == row['Link']]['Reviews'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15bdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51699534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
