{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39d9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from OTAs.file_management.file_path_manager import FilePathManager\n",
    "import re\n",
    "\n",
    "USERNAME = 'azureadmin'\n",
    "PASSWORD = 'brudnyHarry!66'\n",
    "# from Viator_AllLinks import main as main_alllinks\n",
    "# from Viator_GetOperator import main as main_getoperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9497ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class CustomLogger:\n",
    "    def __init__(self, log_file='app.log'):\n",
    "        # Create a custom logger\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # Create handlers for writing to log file and console\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        console_handler = logging.StreamHandler()\n",
    "\n",
    "        # Set level for handlers\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Create formatters and add it to handlers\n",
    "        log_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(log_format)\n",
    "        console_handler.setFormatter(log_format)\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    # Define methods for info, done (treated as info), and error logging\n",
    "    def logger_info(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def logger_done(self, message):\n",
    "        self.logger.info(message)  # Treating 'done' as an info log\n",
    "\n",
    "    def logger_err(self, message):\n",
    "        self.logger.error(message)\n",
    "\n",
    "# Initialize logger instance\n",
    "logger = CustomLogger(log_file='upsert_process.log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2689246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_add_uid(df):\n",
    "    df = df[df['Link'].str.strip().str.len() > 1]\n",
    "    df = df.sort_values(by=['uid', 'Date input'], ascending=[True, False])\n",
    "    # Drop duplicates to keep only the latest URL for each UID\n",
    "    df = df.drop_duplicates(subset='uid')\n",
    "    # Reset index for clarity\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32704c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean out non-UTF characters\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        # Remove non-ASCII characters or replace with a safe placeholder\n",
    "        cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        return cleaned_text\n",
    "    except TypeError:\n",
    "        return text  # If it's not a string, return the original value\n",
    "\n",
    "\n",
    "\n",
    "# Now the dataframe should be free of non-ASCII characters, and can be uploaded to SQL.\n",
    "# upsert_df_to_sql_db(df_main, 'your_table_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a28cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_df_to_sql_db(path_df_main, database_name):\n",
    "\n",
    "\n",
    "    # Log start of process\n",
    "    logger.logger_info(f\"Starting upsert process for file {path_df_main} for database {database_name}\")\n",
    "\n",
    "    df_main = pd.read_excel(path_df_main, engine='openpyxl')\n",
    "    logger.logger_info(f\"Loaded {len(df_main)} rows from {path_df_main}\")\n",
    "\n",
    "    # Apply this function to all string columns in the dataframe to clean non-ASCII characters\n",
    "    for column in df_main.select_dtypes(include=['object']).columns:\n",
    "        df_main[column] = df_main[column].apply(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "    logger.logger_info(f\"Cleaned text data in dataframe.\")\n",
    "\n",
    "    # Fill missing values and filter data\n",
    "    df_main['Reviews'] = df_main['Reviews'].fillna(0)\n",
    "    df_main['Operator'] = df_main['Operator'].fillna('Error')\n",
    "    df_main['Tytul'] = df_main['Tytul'].fillna('Error')\n",
    "    df_main['Reviews'] = df_main['Reviews'].astype(str)\n",
    "    df_main['Operator'] = df_main['Operator'].astype(str)\n",
    "    df_main['Date input'] = df_main['Date input'].astype(str)\n",
    "    df_main['Date update'] = df_main['Date update'].astype(str)\n",
    "    df_main = df_main[df_main['City'].str.len() >= 3]\n",
    "    logger.logger_info(f\"Processed missing values and filtered cities.\")\n",
    "\n",
    "    # Determine table name based on file\n",
    "    if 'GYG' in path_df_main:\n",
    "        table_name = 'Operators_GYG'\n",
    "        df_main = clean_add_uid(df_main)\n",
    "    elif 'Musement' in path_df_main:\n",
    "        table_name = 'Operators_Musement'\n",
    "    elif 'Headout' in path_df_main:\n",
    "        table_name = 'Operators_Headout'\n",
    "    else:\n",
    "        table_name = 'Operators_Viator'\n",
    "    df_main = clean_add_uid(df_main)\n",
    "    df_main = df_main.drop_duplicates(subset=['uid'])\n",
    "    logger.logger_info(f\"Using table {table_name} for upsert operation.\")\n",
    "\n",
    "    # Database connection settings\n",
    "    server = 'sqlserver-myotas.database.windows.net'\n",
    "    database = database_name\n",
    "    driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "    try:\n",
    "        cnxn = pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{server};PORT=1433;DATABASE={database};UID={USERNAME};PWD={PASSWORD}')\n",
    "        logger.logger_info(f\"Successfully connected to database {database}.\")\n",
    "    except Exception as e:\n",
    "        logger.logger_err(f\"Failed to connect to database: {str(e)}\")\n",
    "        return \"Couldn't connect to database\"\n",
    "\n",
    "    cursor = cnxn.cursor()\n",
    "    cursor.fast_executemany = True\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    create_table_query = f\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='{table_name}' AND xtype='U')\n",
    "        CREATE TABLE [dbo].[{table_name}] (\n",
    "            [Tytul]       NVARCHAR (MAX) NULL,\n",
    "            [Link]        NVARCHAR (MAX) NULL,\n",
    "            [City]        NVARCHAR (255) NULL,\n",
    "            [Operator]    NVARCHAR (MAX) NULL,\n",
    "            [Reviews]     NVARCHAR (255) NULL,\n",
    "            [Date input]  NVARCHAR (255) NULL,\n",
    "            [Date update] NVARCHAR (255) NULL,\n",
    "            [uid]         NVARCHAR (255) NOT NULL PRIMARY KEY\n",
    "        );\n",
    "    \"\"\"\n",
    "    logger.logger_info(f\"Ensuring table {table_name} exists.\")\n",
    "\n",
    "    try:\n",
    "        # cursor.execute(create_table_query)\n",
    "        # cnxn.commit()\n",
    "        logger.logger_done(f\"Table {table_name} checked/created successfully.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logger.logger_err(f\"Error creating table: {str(e)}\")\n",
    "        return \"Table creation failed\"\n",
    "\n",
    "    # Upsert query\n",
    "    merge_query = f\"\"\"\n",
    "        MERGE [dbo].[{table_name}] AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?)) AS source ([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews])\n",
    "        ON target.[uid] = source.[uid]\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET\n",
    "                target.[Tytul] = source.[Tytul],\n",
    "                target.[Link] = source.[Link],\n",
    "                target.[City] = source.[City],\n",
    "                target.[Operator] = source.[Operator],\n",
    "                target.[Date input] = source.[Date input],\n",
    "                target.[Date update] = source.[Date update],\n",
    "                target.[Reviews] = source.[Reviews]\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT ([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews])\n",
    "            VALUES (source.[Tytul], source.[Link], source.[City], source.[Operator], source.[Date input], source.[Date update], source.[uid], source.[Reviews]);\n",
    "    \"\"\"\n",
    "    data_list = [tuple(row) for row in df_main.values]\n",
    "    logger.logger_info(f\"Preparing to upsert {len(data_list)} rows.\")\n",
    "\n",
    "    try:\n",
    "        cursor.executemany(merge_query, data_list)\n",
    "        cnxn.commit()\n",
    "        logger.logger_done(f\"Successfully upserted {len(data_list)} rows.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logger.logger_err(f\"Data upsert failed: {str(e)}\")\n",
    "\n",
    "    cnxn.close()\n",
    "    logger.logger_done(f\"Database connection closed.\")\n",
    "    return f'Successfully upserted: {len(data_list)} rows to {table_name} table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8ff777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 09:45:48,152 - INFO - Starting upsert process for file G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx for database OTAs\n",
      "2024-11-20 09:45:54,064 - INFO - Loaded 64235 rows from G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx\n",
      "2024-11-20 09:45:54,512 - INFO - Cleaned text data in dataframe.\n",
      "2024-11-20 09:45:54,627 - INFO - Processed missing values and filtered cities.\n",
      "2024-11-20 09:45:54,817 - INFO - Using table Operators_GYG for upsert operation.\n",
      "2024-11-20 09:46:25,086 - ERROR - Failed to connect to database: ('08001', '[08001] [Microsoft][ODBC Driver 18 for SQL Server]TCP Provider: Timeout error [258].  (258) (SQLDriverConnect); [08001] [Microsoft][ODBC Driver 18 for SQL Server]Login timeout expired (0); [08001] [Microsoft][ODBC Driver 18 for SQL Server]Invalid connection string attribute (0); [08001] [Microsoft][ODBC Driver 18 for SQL Server]Unable to complete login process due to delay in login response (258)')\n",
      "2024-11-20 09:46:25,104 - INFO - Starting upsert process for file G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx for database db_ota_future_price\n",
      "2024-11-20 09:46:31,495 - INFO - Loaded 64235 rows from G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_GYG.xlsx\n",
      "2024-11-20 09:46:32,023 - INFO - Cleaned text data in dataframe.\n",
      "2024-11-20 09:46:32,147 - INFO - Processed missing values and filtered cities.\n",
      "2024-11-20 09:46:32,380 - INFO - Using table Operators_GYG for upsert operation.\n",
      "2024-11-20 09:46:32,866 - INFO - Successfully connected to database db_ota_future_price.\n",
      "2024-11-20 09:46:32,867 - INFO - Ensuring table Operators_GYG exists.\n",
      "2024-11-20 09:46:32,867 - INFO - Table Operators_GYG checked/created successfully.\n",
      "2024-11-20 09:46:32,944 - INFO - Preparing to upsert 57844 rows.\n",
      "2024-11-20 09:46:47,675 - INFO - Successfully upserted 57844 rows.\n",
      "2024-11-20 09:46:47,733 - INFO - Database connection closed.\n",
      "2024-11-20 09:46:47,745 - INFO - Starting upsert process for file G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Musement.xlsx for database OTAs\n",
      "2024-11-20 09:46:48,537 - INFO - Loaded 2176 rows from G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Musement.xlsx\n",
      "2024-11-20 09:46:48,553 - INFO - Cleaned text data in dataframe.\n",
      "2024-11-20 09:46:48,559 - INFO - Processed missing values and filtered cities.\n",
      "2024-11-20 09:46:48,565 - INFO - Using table Operators_Musement for upsert operation.\n",
      "2024-11-20 09:46:49,087 - INFO - Successfully connected to database OTAs.\n",
      "2024-11-20 09:46:49,088 - INFO - Ensuring table Operators_Musement exists.\n",
      "2024-11-20 09:46:49,088 - INFO - Table Operators_Musement checked/created successfully.\n",
      "2024-11-20 09:46:49,091 - INFO - Preparing to upsert 2158 rows.\n",
      "2024-11-20 09:46:49,743 - INFO - Successfully upserted 2158 rows.\n",
      "2024-11-20 09:46:49,801 - INFO - Database connection closed.\n",
      "2024-11-20 09:46:49,802 - INFO - Starting upsert process for file G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Viator.xlsx for database OTAs\n",
      "2024-11-20 09:47:01,269 - INFO - Loaded 115352 rows from G:\\.shortcut-targets-by-id\\1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2\\MyOTAs\\Pliki firmowe\\Operators_Viator.xlsx\n",
      "2024-11-20 09:47:02,033 - INFO - Cleaned text data in dataframe.\n",
      "2024-11-20 09:47:02,272 - INFO - Processed missing values and filtered cities.\n",
      "2024-11-20 09:47:02,526 - INFO - Using table Operators_Viator for upsert operation.\n",
      "2024-11-20 09:47:02,557 - INFO - Successfully connected to database OTAs.\n",
      "2024-11-20 09:47:02,558 - INFO - Ensuring table Operators_Viator exists.\n",
      "2024-11-20 09:47:02,559 - INFO - Table Operators_Viator checked/created successfully.\n",
      "2024-11-20 09:47:02,656 - INFO - Preparing to upsert 114999 rows.\n",
      "2024-11-20 09:47:31,996 - INFO - Successfully upserted 114999 rows.\n",
      "2024-11-20 09:47:32,058 - INFO - Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "sites = [\"GYG\", \"Musement\", \"Viator\"]\n",
    "for site in sites:\n",
    "    file_manager = FilePathManager(site, 'N/A')\n",
    "    file_path_xlsx_operator = file_manager.get_file_paths()['file_path_xlsx_operator']\n",
    "    upsert_df_to_sql_db(file_path_xlsx_operator, 'OTAs')\n",
    "    if site == \"GYG\":\n",
    "        upsert_df_to_sql_db(file_path_xlsx_operator, 'db_ota_future_price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732eeaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7ce4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be15bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG IN CASE OF ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51699534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def upsert_df_to_sql_db(path_df_main, database_name, batch_size=1000):\n",
    "#     import pandas as pd\n",
    "#     import pyodbc\n",
    "\n",
    "#     # Log start of process\n",
    "#     logger.logger_info(f\"Starting upsert process for file {path_df_main}\")\n",
    "\n",
    "#     df_main = pd.read_excel(path_df_main, engine='openpyxl')\n",
    "#     logger.logger_info(f\"Loaded {len(df_main)} rows from {path_df_main}\")\n",
    "\n",
    "#     # Apply this function to all string columns in the dataframe to clean non-ASCII characters\n",
    "#     for column in df_main.select_dtypes(include=['object']).columns:\n",
    "#         df_main[column] = df_main[column].apply(lambda x: clean_text(x) if isinstance(x, str) else x)\n",
    "#     logger.logger_info(f\"Cleaned text data in dataframe.\")\n",
    "\n",
    "#     # Fill missing values and filter data\n",
    "#     df_main['Reviews'] = df_main['Reviews'].fillna(0)\n",
    "#     df_main['Operator'] = df_main['Operator'].fillna('Error')\n",
    "#     df_main['Tytul'] = df_main['Tytul'].fillna('Error')\n",
    "#     df_main['Reviews'] = df_main['Reviews'].astype(str)\n",
    "#     df_main['Operator'] = df_main['Operator'].astype(str)\n",
    "#     df_main['Date input'] = df_main['Date input'].astype(str)\n",
    "#     df_main['Date update'] = df_main['Date update'].astype(str)\n",
    "#     df_main = df_main[df_main['City'].str.len() >= 3]\n",
    "#     logger.logger_info(f\"Processed missing values and filtered cities.\")\n",
    "\n",
    "#     # Determine table name based on file\n",
    "#     if 'GYG' in path_df_main:\n",
    "#         table_name = 'Operators_GYG'\n",
    "#         df_main = clean_add_uid(df_main)\n",
    "#     elif 'Musement' in path_df_main:\n",
    "#         table_name = 'Operators_Musement'\n",
    "#     elif 'Headout' in path_df_main:\n",
    "#         table_name = 'Operators_Headout'\n",
    "#     else:\n",
    "#         table_name = 'Operators_Viator'\n",
    "#     df_main = clean_add_uid(df_main)\n",
    "#     df_main = df_main.drop_duplicates(subset=['uid'])\n",
    "#     logger.logger_info(f\"Using table {table_name} for upsert operation.\")\n",
    "\n",
    "#     # Database connection settings\n",
    "#     server = 'sqlserver-myotas.database.windows.net'\n",
    "#     database = database_name\n",
    "#     driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "#     try:\n",
    "#         cnxn = pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{server};PORT=1433;DATABASE={database};UID={USERNAME};PWD={PASSWORD}')\n",
    "#         logger.logger_info(f\"Successfully connected to database {database}.\")\n",
    "#     except Exception as e:\n",
    "#         logger.logger_err(f\"Failed to connect to database: {str(e)}\")\n",
    "#         return \"Couldn't connect to database\"\n",
    "\n",
    "#     cursor = cnxn.cursor()\n",
    "#     cursor.fast_executemany = True\n",
    "\n",
    "#     # Create table if it doesn't exist\n",
    "#     create_table_query = f\"\"\"\n",
    "#         IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='{table_name}' AND xtype='U')\n",
    "#         CREATE TABLE [dbo].[{table_name}] (\n",
    "#             [Tytul]       NVARCHAR (MAX) NULL,\n",
    "#             [Link]        NVARCHAR (MAX) NULL,\n",
    "#             [City]        NVARCHAR (255) NULL,\n",
    "#             [Operator]    NVARCHAR (MAX) NULL,\n",
    "#             [Reviews]     NVARCHAR (255) NULL,\n",
    "#             [Date input]  NVARCHAR (255) NULL,\n",
    "#             [Date update] NVARCHAR (255) NULL,\n",
    "#             [uid]         NVARCHAR (255) NOT NULL PRIMARY KEY\n",
    "#         );\n",
    "#     \"\"\"\n",
    "#     logger.logger_info(f\"Ensuring table {table_name} exists.\")\n",
    "#     try:\n",
    "#         cursor.execute(create_table_query)\n",
    "#         cnxn.commit()\n",
    "#         logger.logger_done(f\"Table {table_name} checked/created successfully.\")\n",
    "#     except pyodbc.Error as e:\n",
    "#         logger.logger_err(f\"Error creating table: {str(e)}\")\n",
    "#         return \"Table creation failed\"\n",
    "\n",
    "#     # Upsert query\n",
    "#     merge_query = f\"\"\"\n",
    "#         MERGE [dbo].[{table_name}] AS target\n",
    "#         USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?)) AS source ([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews])\n",
    "#         ON target.[uid] = source.[uid]\n",
    "#         WHEN MATCHED THEN\n",
    "#             UPDATE SET\n",
    "#                 target.[Tytul] = source.[Tytul],\n",
    "#                 target.[Link] = source.[Link],\n",
    "#                 target.[City] = source.[City],\n",
    "#                 target.[Operator] = source.[Operator],\n",
    "#                 target.[Date input] = source.[Date input],\n",
    "#                 target.[Date update] = source.[Date update],\n",
    "#                 target.[Reviews] = source.[Reviews]\n",
    "#         WHEN NOT MATCHED THEN\n",
    "#             INSERT ([Tytul], [Link], [City], [Operator], [Date input], [Date update], [uid], [Reviews])\n",
    "#             VALUES (source.[Tytul], source.[Link], source.[City], source.[Operator], source.[Date input], source.[Date update], source.[uid], source.[Reviews]);\n",
    "#     \"\"\"\n",
    "    \n",
    "#     data_list = [tuple(row) for row in df_main.values]\n",
    "#     logger.logger_info(f\"Preparing to upsert {len(data_list)} rows in batches of {batch_size}.\")\n",
    "\n",
    "#     # Split data into batches\n",
    "#     for i in range(0, len(data_list), batch_size):\n",
    "#         batch = data_list[i:i + batch_size]\n",
    "#         logger.logger_info(f\"Processing batch {i // batch_size + 1} of {len(data_list) // batch_size + 1}\")\n",
    "        \n",
    "#         try:\n",
    "#             cursor.executemany(merge_query, batch)\n",
    "#             cnxn.commit()\n",
    "#             logger.logger_done(f\"Successfully upserted batch {i // batch_size + 1}\")\n",
    "#         except pyodbc.Error as e:\n",
    "#             logger.logger_err(f\"Batch {i // batch_size + 1} failed with error: {str(e)}\")\n",
    "#             # Further process the failing batch row-by-row\n",
    "#             for idx, row in enumerate(batch):\n",
    "#                 try:\n",
    "#                     cursor.execute(merge_query, row)\n",
    "#                     cnxn.commit()\n",
    "#                     logger.logger_done(f\"Successfully upserted row {i + idx + 1} from batch {i // batch_size + 1}\")\n",
    "#                 except pyodbc.Error as e:\n",
    "#                     logger.logger_err(f\"Row {i + idx + 1} failed: {row} with error: {str(e)}\")\n",
    "\n",
    "#     cnxn.close()\n",
    "#     logger.logger_done(f\"Database connection closed.\")\n",
    "#     return f'Successfully upserted rows to {table_name} table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1632e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites = [\"Viator\"]\n",
    "# for site in sites:\n",
    "#     file_manager = common_functions.FilePathManager(site, 'N/A')\n",
    "#     file_path_xlsx_operator = file_manager.get_file_paths()['file_path_xlsx_operator']\n",
    "#     upsert_df_to_sql_db(file_path_xlsx_operator, 'OTAs')\n",
    "#     if site == \"GYG\":\n",
    "#         upsert_df_to_sql_db(file_path_xlsx_operator, 'db_ota_future_price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af8f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
