{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153cdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactored_file.py\n",
    "\n",
    "import json\n",
    "from commmon_functions_gyg import GYG_Scraper\n",
    "import logging\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory instead of using __file__\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "# Now you can import the modules\n",
    "import common_functions\n",
    "import Azure_stopVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_selectors = {\n",
    "    'currency': 'div[data-test*=\"dropdown-currency\"]',\n",
    "    'currency_list': 'section[class*=\"row-start-center\"]',\n",
    "    'products_count': 'span[data-test-id*=\"search-component-activity-count-text\"]',\n",
    "    'view_more_button': 'button[data-test-id=\"search-component-test-btn\"]',\n",
    "    'show_more_button': 'a[data-qa-marker*=\"loading-button\"]',\n",
    "    'product_card': 'div[data-test*=\"ActivityCard\"]',\n",
    "    'tour_price': 'span[data-test=\"realPrice\"]',\n",
    "    'tour_price_discount': 'div[class=\"tour-scratch-price\"]',\n",
    "    'ratings': 'div[data-test=\"reviewTest\"]',\n",
    "    'review_count': 'p[class*=\"reviewsNumber\"]',\n",
    "    'category_label': 'div[data-test=\"main-category\"]',\n",
    "    'js_script_for_shadow_root': ' document.querySelector(\"msm-cookie-banner\").shadowRoot',\n",
    "    'cookies_banner': 'button[data-test*=\"decline-cookies\"]',\n",
    "    'sort_by': 'select[data-test-id=\"search-component-sort-selector\"]',\n",
    "    'option_rating': 'option[value*=\"rating\"]',\n",
    "    'option_popularity': 'option[value*=\"relevance-city\"]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the GYG scraping workflow.\n",
    "    This function initializes the necessary managers, loads the links from the link file,\n",
    "    and orchestrates the scraping and uploading processes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize site and file manager\n",
    "        site = \"GYG\"\n",
    "        file_manager = common_functions.FilePathManager(site, \"NA\")  # 'NA' can be a default city or placeholder\n",
    "        logger = common_functions.LoggerManager(file_manager)\n",
    "        \n",
    "        logger.logger_info.info(f\"Starting scraping process for site: {site}\")\n",
    "\n",
    "        # Load all links and categories from the link file\n",
    "        link_file_path = file_manager.get_file_paths()['link_file']\n",
    "        if not os.path.exists(link_file_path):\n",
    "            logger.logger_err.error(f\"Link file '{link_file_path}' does not exist. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        df_links = pd.read_csv(link_file_path)\n",
    "        logger.logger_info.info(f\"Loaded {len(df_links)} links from '{link_file_path}'.\")\n",
    "\n",
    "        # Initialize the scraper with the file manager and logger\n",
    "        scraper = GYG_Scraper(file_manager, logger)\n",
    "        \n",
    "        # Execute the daily scraping run with the loaded links\n",
    "        while True:\n",
    "            try:\n",
    "                result = scraper.daily_run_gyg(df_links=df_links)\n",
    "            except Exception as e:\n",
    "                scraper.handle_error_and_rerun(e)\n",
    "                logger.logger_err.error(\"An error occurred during the scraping process.\")\n",
    "        \n",
    "            if result == \"Done\":\n",
    "                break\n",
    "        \n",
    "        \n",
    "        \n",
    "        # After scraping all links, proceed to upload the consolidated Excel file to Azure\n",
    "        try:\n",
    "            scraper.upload_excel_to_azure_storage_account()\n",
    "            logger.logger_info.info(\"Uploaded the consolidated Excel file to Azure Blob Storage (raw container).\")\n",
    "        except Exception as e:\n",
    "            scraper.handle_error_and_rerun(e)\n",
    "            logger.logger_err.error(\"Failed to upload the Excel file to Azure Blob Storage (raw container).\")\n",
    "        \n",
    "        # Transform the Excel file and upload the refined version to Azure\n",
    "        try:\n",
    "            scraper.transform_upload_to_refined()\n",
    "            logger.logger_info.info(\"Transformed and uploaded the refined Excel file to Azure Blob Storage (refined container).\")\n",
    "        except Exception as e:\n",
    "            scraper.handle_error_and_rerun(e)\n",
    "            logger.logger_err.error(\"Failed to transform and upload the refined Excel file to Azure Blob Storage.\")\n",
    "        \n",
    "        logger.logger_done.info(\"All scraping and uploading tasks completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch any unforeseen errors in the main workflow\n",
    "        logging.basicConfig(level=logging.ERROR)\n",
    "        logging.error(f\"An unexpected error occurred in the main workflow: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "    if 'backup' in os.getcwd():\n",
    "        script_name = 'Viator_daily.py'\n",
    "\n",
    "        check_if_viator_running = Azure_stopVM.check_if_script_is_running(script_name)\n",
    "        if check_if_viator_running:\n",
    "            logger.logger_done.info(f\"{script_name} is currently running.\")\n",
    "        else:\n",
    "            logger.logger_done.info(f\"{script_name} is not running. Stoping VM\")\n",
    "            Azure_stopVM.stop_vm()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f2557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 20:08:58,665 - Info_logger - INFO - Starting scraping process for site: GYG\n",
      "2024-10-01 20:08:58,710 - Info_logger - INFO - Loaded 1913 links from 'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/GYG_links.csv'.\n",
      "2024-10-01 20:08:58,711 - Info_logger - INFO - Initializing the Chrome driver.\n",
      "2024-10-01 20:09:00,543 - Info_logger - INFO - Chrome driver initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize site and file manager\n",
    "site = \"GYG\"\n",
    "file_manager = common_functions.FilePathManager(site, \"NA\")  # 'NA' can be a default city or placeholder\n",
    "logger = common_functions.LoggerManager(file_manager)\n",
    "\n",
    "logger.logger_info.info(f\"Starting scraping process for site: {site}\")\n",
    "\n",
    "# Load all links and categories from the link file\n",
    "link_file_path = file_manager.get_file_paths()['link_file']\n",
    "if not os.path.exists(link_file_path):\n",
    "    logger.logger_err.error(f\"Link file '{link_file_path}' does not exist. Exiting.\")\n",
    "    \n",
    "\n",
    "df_links = pd.read_csv(link_file_path)\n",
    "logger.logger_info.info(f\"Loaded {len(df_links)} links from '{link_file_path}'.\")\n",
    "\n",
    "# Initialize the scraper with the file manager and logger\n",
    "scraper = GYG_Scraper(file_manager, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d15f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper.driver.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e5f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 20:15:17,982 - Error_logger - ERROR - Sheet Porto has 1657 invalid date entries in 'Data zestawienia' column.\n",
      "C:\\Users\\Wojciech\\AppData\\Local\\Temp\\ipykernel_6476\\1823196204.py:18: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Data zestawienia'] = pd.to_datetime(df['Data zestawienia']).dt.strftime('%Y-%m-%d')\n"
     ]
    }
   ],
   "source": [
    "# exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "# excel_data = pd.read_excel(file_manager.get_file_paths()['file_path_output'], sheet_name=None)\n",
    "# for sheet_name, df in excel_data.items():\n",
    "#     if sheet_name in exclude_sheets:\n",
    "#         continue\n",
    "#     # Read the Excel file into a Pandas DataFrame\n",
    "#     # Check 'Data zestawienia' for valid date formats\n",
    "#     df['Data zestawienia'] = df['Data zestawienia'].astype(str)\n",
    "\n",
    "#     # Filter rows where 'Data zestawienia' does not have a valid date\n",
    "#     invalid_rows = df[~df['Data zestawienia'].apply(is_valid_date)]\n",
    "\n",
    "#     # Log sheet name and number of invalid rows if found\n",
    "#     if not invalid_rows.empty:\n",
    "#         scraper.logger.logger_err.error(f\"Sheet {sheet_name} has {len(invalid_rows)} invalid date entries in 'Data zestawienia' column.\")\n",
    "#         raise ValueError(f\"Sheet {sheet_name} has {len(invalid_rows)} invalid date entries in 'Data zestawienia' column.\")\n",
    "    \n",
    "\n",
    "#     # Convert 'Data zestawienia' to YYYY-MM-DD format if valid\n",
    "#     df['Data zestawienia'] = pd.to_datetime(df['Data zestawienia']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#     # Transform the DataFrame (add your transformation logic here)\n",
    "#     df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "#     df['IloscOpini'] = df['IloscOpini'].astype(str)\n",
    "#     df['IloscOpini'] = df['IloscOpini'].fillna(0)\n",
    "#     df['IloscOpini'] = df['IloscOpini'].str.replace('(', '').str.replace(')','')\n",
    "#     df['IloscOpini'] = df['IloscOpini'].apply(lambda x: int(float(x.replace('K', '')) * 1000) if isinstance(x, str) and 'K' in x else x)\n",
    "\n",
    "#     df['Opinia'] = df['Opinia'].astype(str)\n",
    "#     df['Opinia'] = df['Opinia'].fillna('N/A')\n",
    "#     df['Opinia'] = df['Opinia'].map(lambda x: x.replace(\"NEW\", '') if isinstance(x, str) else x)\n",
    "\n",
    "#     df = df[df['Tytul'] != 'Tytul']\n",
    "#     df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "#     df = df[df['Data zestawienia'].str.len() > 4]\n",
    "\n",
    "#     df['Cena'] = df['Cena'].str.lower()\n",
    "#     df['Cena'] = df['Cena'].map(lambda x: x.split('from')[-1] if isinstance(x, str) and 'from' in x else x)\n",
    "#     df['Cena'] = df['Cena'].apply(lambda x: str(x).replace('€', '').replace('$', '').replace('£', '').strip() if isinstance(x, str) else x)\n",
    "#     df['Cena'] = df['Cena'].map(lambda x: x.split('per person')[0] if isinstance(x, str) and 'per person' in x.lower() else x)\n",
    "#     df['Cena'] = df['Cena'].map(lambda x: x.split('per group')[0] if isinstance(x, str) and 'per group' in x.lower() else x)\n",
    "\n",
    "#     df['Przecena'] = df['Przecena'].apply(lambda x: str(x).replace('€', '').replace('$', '').replace('£', '').strip() if isinstance(x, str) else x)\n",
    "#     df['Przecena'] = df['Przecena'].map(lambda x: x.split('per person')[0] if isinstance(x, str) and 'per person' in x.lower() else x)\n",
    "#     df['Przecena'] = df['Przecena'].map(lambda x: x.split('per group')[0] if isinstance(x, str) and 'per group' in x.lower() else x)\n",
    "\n",
    "\n",
    "#     # Apply str.replace only if the value is a string\n",
    "#     df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee572d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
