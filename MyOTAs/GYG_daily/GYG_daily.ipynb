{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153cdbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactored_file.py\n",
    "\n",
    "import json\n",
    "from commmon_functions_gyg import GYG_Scraper\n",
    "import logging\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory instead of using __file__\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "# Now you can import the modules\n",
    "import common_functions\n",
    "import Azure_stopVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54732b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "css_selectors = {\n",
    "    'currency': 'div[data-test*=\"dropdown-currency\"]',\n",
    "    'currency_list': 'section[class*=\"row-start-center\"]',\n",
    "    'products_count': 'span[data-test-id*=\"search-component-activity-count-text\"]',\n",
    "    'view_more_button': 'button[data-test-id=\"search-component-test-btn\"]',\n",
    "    'show_more_button': 'a[data-qa-marker*=\"loading-button\"]',\n",
    "    'product_card': 'div[data-test*=\"ActivityCard\"]',\n",
    "    'tour_price': 'span[data-test=\"realPrice\"]',\n",
    "    'tour_price_discount': 'div[class=\"tour-scratch-price\"]',\n",
    "    'ratings': 'div[data-test=\"reviewTest\"]',\n",
    "    'review_count': 'p[class*=\"reviewsNumber\"]',\n",
    "    'category_label': 'div[data-test=\"main-category\"]',\n",
    "    'js_script_for_shadow_root': ' document.querySelector(\"msm-cookie-banner\").shadowRoot',\n",
    "    'cookies_banner': 'button[data-test*=\"decline-cookies\"]',\n",
    "    'sort_by': 'select[data-test-id=\"search-component-sort-selector\"]',\n",
    "    'option_rating': 'option[value*=\"rating\"]',\n",
    "    'option_popularity': 'option[value*=\"relevance-city\"]'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the GYG scraping workflow.\n",
    "    This function initializes the necessary managers, loads the links from the link file,\n",
    "    and orchestrates the scraping and uploading processes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize site and file manager\n",
    "        site = \"GYG\"\n",
    "        file_manager = common_functions.FilePathManager(site, \"NA\")  # 'NA' can be a default city or placeholder\n",
    "        logger = common_functions.LoggerManager(file_manager)\n",
    "        \n",
    "        logger.logger_info.info(f\"Starting scraping process for site: {site}\")\n",
    "\n",
    "        # Load all links and categories from the link file\n",
    "        link_file_path = file_manager.get_file_paths()['link_file']\n",
    "        if not os.path.exists(link_file_path):\n",
    "            logger.logger_err.error(f\"Link file '{link_file_path}' does not exist. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        df_links = pd.read_csv(link_file_path)\n",
    "        logger.logger_info.info(f\"Loaded {len(df_links)} links from '{link_file_path}'.\")\n",
    "\n",
    "        # Initialize the scraper with the file manager and logger\n",
    "        scraper = GYG_Scraper(file_manager, logger)\n",
    "        \n",
    "        try:\n",
    "            # Execute the daily scraping run with the loaded links\n",
    "            result = scraper.daily_run_gyg(df_links=df_links)\n",
    "            if result == 'Done':\n",
    "                logger.logger_info.info(\"Scraping already completed for today. No action needed.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            scraper.handle_error_and_rerun(e)\n",
    "            logger.logger_err.error(\"An error occurred during the scraping process.\")\n",
    "            return  # Exit after handling the error\n",
    "        \n",
    "        # After scraping all links, proceed to upload the consolidated Excel file to Azure\n",
    "        try:\n",
    "            scraper.upload_excel_to_azure_storage_account()\n",
    "            logger.logger_info.info(\"Uploaded the consolidated Excel file to Azure Blob Storage (raw container).\")\n",
    "        except Exception as e:\n",
    "            scraper.handle_error_and_rerun(e)\n",
    "            logger.logger_err.error(\"Failed to upload the Excel file to Azure Blob Storage (raw container).\")\n",
    "        \n",
    "        # Transform the Excel file and upload the refined version to Azure\n",
    "        try:\n",
    "            scraper.transform_upload_to_refined()\n",
    "            logger.logger_info.info(\"Transformed and uploaded the refined Excel file to Azure Blob Storage (refined container).\")\n",
    "        except Exception as e:\n",
    "            scraper.handle_error_and_rerun(e)\n",
    "            logger.logger_err.error(\"Failed to transform and upload the refined Excel file to Azure Blob Storage.\")\n",
    "        \n",
    "        logger.logger_done.info(\"All scraping and uploading tasks completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch any unforeseen errors in the main workflow\n",
    "        logging.basicConfig(level=logging.ERROR)\n",
    "        logging.error(f\"An unexpected error occurred in the main workflow: {e}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "    if 'backup' in os.getcwd():\n",
    "        script_name = 'Viator_daily.py'\n",
    "\n",
    "        check_if_viator_running = Azure_stopVM.check_if_script_is_running(script_name)\n",
    "        if check_if_viator_running:\n",
    "            logger.logger_done.info(f\"{script_name} is currently running.\")\n",
    "        else:\n",
    "            logger.logger_done.info(f\"{script_name} is not running. Stoping VM\")\n",
    "            Azure_stopVM.stop_vm()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683d52db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdaee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##################DEBUG CURRENCY SWITCHER\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "# driver.maximize_window()\n",
    "# # Define the URL of the website we want to scrape\n",
    "# start_time = time.time()\n",
    "# total_pages = 0\n",
    "# #     CHECK IF FILE PATH EXISIT IF SO CHECK THE DATA INSIDE\n",
    "# #         print(index, row)\n",
    "# page = 1\n",
    "# max_pages = 9999\n",
    "# data = []\n",
    "# position = 0\n",
    "# url_time = time.time()\n",
    "\n",
    "# url = f'https://www.getyourguide.com/s?q=Amsterdam&p=1'\n",
    "\n",
    "# driver.get(url)\n",
    "# time.sleep(1)\n",
    "# #     VERIFY IF THE CURRENCY IS CORRECT\n",
    "# login_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@title='Profile']\")))\n",
    "# login_button.click()\n",
    "# # currency = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@title='Select Currency']\")))\n",
    "# currency = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='option option-currency']\")))\n",
    "# currency\n",
    "# html = driver.page_source\n",
    "# soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2841482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currency_switcher_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='option option-currency']\")))\n",
    "# # hover over the currency switcher button to show the menu\n",
    "# actions = ActionChains(driver)\n",
    "# actions.move_to_element(currency_switcher_button).perform()\n",
    "# currency_switcher_button .click()\n",
    "# # wait for the EUR currency option to be clickable\n",
    "# eur_currency_option = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//li[@class='currency-modal-picker__item-parent item__currency-modal item__currency-modal--EUR']\")))\n",
    "# # click on the EUR currency option to change the currency\n",
    "# eur_currency_option.click()\n",
    "\n",
    "# html = driver.page_source\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# tour_items = soup.select(\"[data-test-id=vertical-activity-card]\")\n",
    "# len(tour_items)\n",
    "# title = tour_items[0].find('p', {'class': 'vertical-activity-card__title'}).text.strip()\n",
    "# price = tour_items[0].find('div', {'class': 'baseline-pricing__value'}).text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
