{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65054b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import xlsxwriter\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "# from undetected_chromedriver import Chrome, ChromeOptions\n",
    "# from user_agent import generate_user_agent\n",
    "# import ctypes  # An included library with Python install.   \n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "# eyJhbGciOiJSUzI1NiIsImtpZCI6IjY3YmFiYWFiYTEwNWFkZDZiM2ZiYjlmZjNmZjVmZTNkY2E0Y2VkYTEiLCJ0eXAiOiJKV1QifQ.eyJuYW1lIjoiV29qdGVrIEJhbG9uIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FBY0hUdGZCODM1WVhSalRJeEl4WmxyTnBaRXpWQk9hZmUyMUFmU1dZZXNnUGc9czk2LWMiLCJpc3MiOiJodHRwczovL3NlY3VyZXRva2VuLmdvb2dsZS5jb20vZXhhMi1mYjE3MCIsImF1ZCI6ImV4YTItZmIxNzAiLCJhdXRoX3RpbWUiOjE2ODY2NTg5MDYsInVzZXJfaWQiOiJEcWRXRDhRdloyUTkzcTR4WFhWWlFWUk8wSEMyIiwic3ViIjoiRHFkV0Q4UXZaMlE5M3E0eFhYVlpRVlJPMEhDMiIsImlhdCI6MTY4NjY1OTA2MSwiZXhwIjoxNjg2NjYyNjYxLCJlbWFpbCI6IndvamJhbDNAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImZpcmViYXNlIjp7ImlkZW50aXRpZXMiOnsiZ29vZ2xlLmNvbSI6WyIxMTUwNTc1NjgzNzI4NjQ1MzA0NTciXSwiZW1haWwiOlsid29qYmFsM0BnbWFpbC5jb20iXX0sInNpZ25faW5fcHJvdmlkZXIiOiJnb29nbGUuY29tIn19.IAOh_U2LXNXGk1jqG3q6m9utI79QVMDtCuUcDBSH5TEKPmMCEdW962qOZN6J8wfMzexHX1cWoqGcXYBmjLcjQKBhhQoAUAdYjxEivrLHe8Hi37bIwXrEX9mvAKD1wE71Sq1sbB3B9xU51lTsH88l7P0pq9LDgbaKkJCljvvzJ186BTbX9Qw0CF4gma1XjJ1W3Nmd0BK2pE9y0b3arF_V8bSME6BeR4Ls1yKLM9da-MCN5y-IkwGVB6j78Qrt-4_emtAhxjkcYlzauOtEM8dZ0NzblgSxY-hdG_sG-Clg0gM6fxXRQSQJYjqHNgwY7sjAP885JUWbtjWjoXKvdJn_iA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3965c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "# date_today = '2023-09-25'\n",
    "date_yesterday = (datetime.date.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "output_gyg = r'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Get Your Guide/All Links'\n",
    "file_path_done =fr'{output_gyg}/{date_today}-DONE-GYG.csv'  \n",
    "archive_folder = fr'{output_gyg}/Archive'\n",
    "\n",
    "file_path_done_archive =fr'{archive_folder}/{date_yesterday}-DONE-GYG.csv'  \n",
    "file_path_output = fr\"{output_gyg}/AllProductsGYG - {date_today}.xlsx\"\n",
    "file_path_output_processed = fr\"{output_gyg}/All Links GYG - {date_today}.xlsx\"\n",
    "file_path_output_processed_csv = fr\"{output_gyg}/All Links GYG - {date_today}.csv\"\n",
    "\n",
    "\n",
    "# link_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/Viator_links.csv'\n",
    "all_links_file = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Resource/AllGYG_links.csv'\n",
    "# Set the path of the local file\n",
    "local_file_path = file_path_output\n",
    "\n",
    "# Set the name of your Azure Storage account and the corresponding access key\n",
    "storage_account_name = \"storagemyotas\"\n",
    "storage_account_key = \"vyHHUXSN761ELqivtl/U3F61lUY27jGrLIKOyAplmE0krUzwaJuFVomDXsIc51ZkFWMjtxZ8wJiN+AStbsJHjA==\"\n",
    "\n",
    "# Set the name of the container and the desired blob name\n",
    "container_name_raw = \"raw/all_links/gyg\"\n",
    "container_name_refined = \"refined/all_links/gyg\"\n",
    "\n",
    "blob_name = fr'GYG - {date_today}.xlsx'\n",
    "# file_path_logs_processed = fr'G:/.shortcut-targets-by-id/1ER8hilqZ2TuX2C34R3SMAtd1Xbk94LE2/MyOTAs/Baza Excel/Logs/files_processed/{blob_name.split(\".\")[0]}'\n",
    "\n",
    "mapping_currency = {'COP\\xa0': 'COP (Colombian Peso)', 'HK$': 'HKD (Hong Kong Dollar)', \n",
    "                    '¥': 'JPY (Japanese Yen)', 'DKK': 'DKK (Danish Krone)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    '₹': 'INR (Indian Rupee)', 'MX$': 'MXN (Mexican Peso)', 'ZAR\\xa0': 'ZAR (South African Rand)',\n",
    "                    'PEN\\xa0': 'PEN (Peruvian Sol)', 'NZ$': 'NZD (New Zealand Dollar)', '€': 'EUR (Euro)',\n",
    "                    'CA$': 'CAD (Canadian Dollar)', 'Â£': 'GBP (British Pound Sterling)',\n",
    "                    'PEN': 'PEN (Peruvian Sol)', 'SEK\\xa0': 'SEK (Swedish Krona)', 'NOK': 'NOK (Norwegian Krone)',\n",
    "                    '$': 'USD (United States Dollar)', 'COP': 'COP (Colombian Peso)', \n",
    "                    'NT$': 'TWD (New Taiwan Dollar)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'â‚¬': 'EUR (Euro)', 'Â¥': 'JPY (Japanese Yen)',\n",
    "                    'â‚¹': 'INR (Indian Rupee)', 'SEK': 'SEK (Swedish Krona)', 'ZAR': 'ZAR (South African Rand)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'â‚´': 'UAH (Ukrainian Hryvnia)', 'zÅ‚': 'PLN (Polish Zloty)',\n",
    "                    'Ð»Ð²': 'BGN Bulgarian Lev', 'US$': 'USD (United States Dollar)', 'lei': 'RON (Romanian Leu)',\n",
    "                    'zł': 'PLN (Polish Zloty)','$U': 'UYU (Uruguayan Peso)', 'COL$': 'COP (Colombian Peso)', \n",
    "                    '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'CHF': 'CHF (Swiss Franc)', 'zł': 'PLN (Polish Zloty)', 'R$': 'BRL (Brazilian Real)',\n",
    "                    'CL$': 'CLP (Chilean Peso)', 'Rp': 'IDR (Indonesian Rupiah)', 'AR$': 'ARS (Argentine Peso)',\n",
    "                    '฿': 'THB (Thai Baht)', 'Kč': 'CZK (Czech Koruna)', 'lei': 'RON (Romanian Leu)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'A$': 'AUD (Australian Dollar)', 'Ft': 'HUF (Hungarian Forint)',\n",
    "                    '€': 'EUR (Euro)', '£': 'GBP (British Pound Sterling)', '₹': 'INR (Indian Rupee)',\n",
    "                    'US$': 'USD (United States Dollar)', 'лв': 'BGN (Bulgarian Lev)',\n",
    "                    'COL$': 'COP (Colombian Peso)', 'lei': 'RON (Romanian Leu)', 'C$': 'NIO (Nicaraguan Cordoba)',\n",
    "                    '₺': 'TRY (Turkish Lira)', 'AR$': 'ARS (Argentine Peso)', 'A$': 'AUD (Australian Dollar)',\n",
    "                    'лв': 'BGN (Bulgarian Lev)', 'Ft': 'HUF (Hungarian Forint)', 'DKK': 'DKK (Danish Krone)',\n",
    "                    '₪': 'ILS (Israeli Shekel)', '€.': 'EUR (Euro)', '₴': 'UAH (Ukrainian Hryvnia)',\n",
    "                    'R$': 'BRL (Brazilian Real)', '₹': 'INR (Indian Rupee)', 'zł': 'PLN (Polish Zloty)',\n",
    "                    'US$': 'USD (United States Dollar)', '€': 'EUR (Euro)', '$U': 'UYU (Uruguayan Peso)',\n",
    "                    'Kč': 'CZK (Czech Koruna)', 'SEK': 'SEK (Swedish Krona)', '£': 'GBP (British Pound Sterling)',\n",
    "                    'E£': 'EGP (Egyptian Pound)', 'CL$': 'CLP (Chilean Peso)'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "currency_list = []\n",
    "API_KEY = '8c36bc42cd11c738c1baad3e2000b40c'\n",
    "country_codes = [\"eu\", \"us\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUR_City = [\n",
    "    'AllLinks',\n",
    "    'Madrid',\n",
    "    'Florence',\n",
    "    'Capri',\n",
    "    'Naples',\n",
    "    'Taormina',\n",
    "    'Mount-Etna',\n",
    "    'Bali',\n",
    "    'Porto',\n",
    "    'Krakow',\n",
    "    'Barcelona',\n",
    "    'Athens',\n",
    "    'Palermo',\n",
    "    'Paris',\n",
    "    'Dubrovnik',\n",
    "    'Berlin',\n",
    "    'Istanbul',\n",
    "    'Adelaide',\n",
    "    'Venice',\n",
    "    'Amsterdam',\n",
    "    'Cairns-and-the-Tropical-North',\n",
    "    'Sorrento',\n",
    "    'Dublin',\n",
    "    'Rome',\n",
    "    'Perth',\n",
    "    'Gold-Coast',\n",
    "    'Amalfi-Coast',\n",
    "    'Salta',\n",
    "    'Bariloche',\n",
    "    'Milan',\n",
    "    'Hobart',\n",
    "    'Mount-Vesuvius',\n",
    "    'Reykjavik',\n",
    "    'Pompeii',\n",
    "    'Vienna',\n",
    "    'Herculaneum',\n",
    "    'Lisbon',\n",
    "    'Brisbane',\n",
    "    'Marrakech',\n",
    "    'Mt-Vesuvius',\n",
    "    'Buenos-Aires',\n",
    "    'Cartagena',\n",
    "    'Mendoza',\n",
    "    'Prague',\n",
    "    'Rio-de-Janeiro'\n",
    "]\n",
    "\n",
    "USD_City = [\n",
    "    'Oahu',\n",
    "    'New-York-City',\n",
    "    'Miami',\n",
    "    'Cancun',\n",
    "    'Vancouver',\n",
    "    'Cappadocia',\n",
    "    'Las-Vegas',\n",
    "    'Niagara-Falls-and-Around',\n",
    "    'Toronto',\n",
    "    'Dubai',\n",
    "    'Montreal',\n",
    "    'San-Francisco',\n",
    "    'Maui',\n",
    "    'Punta-Cana',\n",
    "    'Quebec-City',\n",
    "    'Queenstown',\n",
    "    'Singapore',\n",
    "    'Tokyo'\n",
    "]\n",
    "\n",
    "GBP_City = [\n",
    "    'Belfast',\n",
    "    'Killarney',\n",
    "    'Galway',\n",
    "    'Lanzarote',\n",
    "    'Edinburgh',\n",
    "    'Manchester',\n",
    "    'England',\n",
    "    'London'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb590719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rates(of_date, currency_code='EUR'):\n",
    "# USING API TO GET RATES FROM SITE https://fixer.io/documentation\n",
    "    res = requests.get(fr'http://data.fixer.io/api/{of_date}?access_key=acfed48df1159d37fa4305e5e95c234f&base={currency_code}')\n",
    "    rates = res.json()['rates']\n",
    "    return rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe39c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_url_to_process_scraperapi(url_input, city_input, category_input, page = 1, max_pages = 25):\n",
    "    global date_today\n",
    "    global output_gyg\n",
    "    global file_path_done\n",
    "    global file_path_output\n",
    "    global avg_file\n",
    "    global re_run_path\n",
    "    global folder_path_with_txt_to_count_avg\n",
    "    global archive_folder\n",
    "    data = []\n",
    "    city_path_done = fr'{output_gyg}/{date_today}-{city_input}-{category_input}.csv'          \n",
    "    if os.path.exists(city_path_done):\n",
    "        city_done_msg = pd.read_csv(city_path_done)\n",
    "        page = int(city_done_msg.drop_duplicates(subset='City', keep='last')['Page']) + 1\n",
    "        \n",
    "    \n",
    "    url_time = time.time()\n",
    "    while page <= max_pages:\n",
    "        if page == 1:\n",
    "            url = f'{url_input}'\n",
    "        else:\n",
    "            url = f'{url_input}&p={page}'\n",
    "#         print(url)\n",
    "\n",
    "\n",
    "        random_country_code = random.choice(country_codes)\n",
    "        \n",
    "# CHECK THE TXT FILE FOR DATE-CITY IF THERE IS ANYTHING DONE \n",
    "#         print(random_country_code)\n",
    "    \n",
    "        url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "                                    json={'apiKey': f'{API_KEY}', \n",
    "                                          'country_code': random_country_code,\n",
    "                                          'url': url })\n",
    "#         time.sleep(random.uniform(1, 10))\n",
    "        if url_request.status_code == 200:\n",
    "            try:\n",
    "                print('send_url_to_process_scraperapi: ', url_request.json()['statusUrl'])\n",
    "                status_url = url_request.json()['statusUrl']\n",
    "                data_send_df = pd.DataFrame({\n",
    "                    'UrlRequest': [url],\n",
    "                    'UrlResponse': [status_url],\n",
    "                    'City': [city_input],\n",
    "                    'Page': [page],\n",
    "                    'Category': category_input\n",
    "                }, columns=['UrlRequest', 'UrlResponse', 'City', 'Page', 'Category'])\n",
    "                data_send_df.to_csv(city_path_done, header=not os.path.exists(city_path_done), index=False, mode='a')\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"send_url_to_process_scraperapi:  JSON could not be decoded\")\n",
    "        else:\n",
    "            print(\"send_url_to_process_scraperapi:  HTTP request returned code: \", url_request.status_code, \"reduced page number from: \", page, \" to \", page-1)\n",
    "            page -=1\n",
    "\n",
    "\n",
    "# IN THE TEXT FILE ADD URL AND STATUS AND WHICH PAGE IS IT RELATED TO \n",
    "        \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66d0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_pages(url):\n",
    "    try:\n",
    "        results = requests.get(url)\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        product_list_count = None\n",
    "        count_of_products_per_page = 24\n",
    "        # Try finding the productListCount label using two different CSS selectors\n",
    "        \n",
    "        selectors = [\"[class*=trip-item-pagination]\", \"[class*=search-header__left__data-wrapper__count]\"]\n",
    "        # Try finding the productListCount label using two different CSS selectors\n",
    "        for selector in selectors:\n",
    "            count_element = soup.select_one(selector)\n",
    "            if count_element:\n",
    "                if 'count' in selector:\n",
    "                    product_list_count = round(int(count_element.text.strip().split()[0]) / count_of_products_per_page, 0)\n",
    "                else:\n",
    "                    product_list_count = int(count_element.text.strip().split( )[-1])\n",
    "                break\n",
    "\n",
    "        if product_list_count is None:\n",
    "            print(\"get_max_pages: Product count not found in the HTML content.\")\n",
    "            return None\n",
    "\n",
    "        max_pages = int(product_list_count)\n",
    "        return max_pages\n",
    "    except Exception as e:\n",
    "        print(f\"get_max_pages: Error while fetching HTML content: {e}\")\n",
    "        print(url, '\\n', soup)\n",
    "        return 25  # Return a default value of 25 pages if there's an error\n",
    "    \n",
    "def get_status(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return response.json()['status'], response.json()['attempts']\n",
    "    except Exception as e:\n",
    "        print(f\"get_status: Error while fetching URL: {url}, Error: {e}\")\n",
    "        return 'error'\n",
    "\n",
    "def check_status_and_process_city_data(df_links):\n",
    "    cities_to_process = []\n",
    "       \n",
    "        \n",
    "    for index, row in df_links.iterrows():\n",
    "        city = row['City']\n",
    "        category = row['Category']\n",
    "        city_path_done = fr'{output_gyg}/{date_today}-{city}-{category}.csv'\n",
    "        if os.path.exists(city_path_done):\n",
    "            print(\"check_status_and_process_city_data: \",  city, '-', category)\n",
    "            city_done_msg = pd.read_csv(city_path_done)\n",
    "            city_done_msg.drop_duplicates(inplace=True)\n",
    "        else:\n",
    "#             MAYBE REMOVE VALUE FROM DF_LINKS WHEN THERE IS NO FILE\n",
    "            df_links = df_links[(df_links['City'] != city) & (df_links['Category'] != category)]\n",
    "            continue\n",
    "        start_time_get_resposne = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = {executor.submit(get_status, url): url for url in city_done_msg['UrlResponse']}\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                url = futures[future]\n",
    "                status, attempts = future.result()\n",
    "                city_done_msg.loc[city_done_msg['UrlResponse'] == url, 'Status'] = status\n",
    "                print(f'check_status_and_process_city_data: {status} for attempt {attempts}')\n",
    "        end_time_get_resposne = time.time()\n",
    "#         print(f'First option with concurrent: {round(end_time_get_resposne-start_time_get_resposne,2)}s')\n",
    "        print(f\"For {city} finished {len(city_done_msg[city_done_msg['Status'] == 'finished'])} from {len(city_done_msg)}\")\n",
    "        # Check if all statuses are finished\n",
    "        if len(city_done_msg[city_done_msg['Status'] == 'finished']) == len(city_done_msg):\n",
    "            df_links = df_links[(df_links['City'] != city) & (df_links['Category'] != category)]\n",
    "            try:\n",
    "                position = df_links[df_links['City'] == city]['Page'] * 24\n",
    "            except:\n",
    "                position = 0\n",
    "\n",
    "            max_page_for_city = get_max_pages(city_done_msg.iloc[0]['UrlResponse'])\n",
    "            city_done_msg['MaxPage'] = max_page_for_city\n",
    "            process_html_from_response_scraperapi(city_done_msg, city_path_done,  position)\n",
    "            \n",
    "    return df_links, cities_to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9aece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67257d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_from_response_scraperapi(data_city_df, city_path_done, position = 0):\n",
    "# data_city_df = pd.read_csv(city_path_done)\n",
    "    data = []\n",
    "    display(data_city_df)\n",
    "    for index, row in data_city_df.iterrows():\n",
    "        print(index)\n",
    "        results = requests.get(row['UrlResponse'])   \n",
    "        soup = BeautifulSoup(results.content, 'html.parser')       \n",
    "        \n",
    "        selectors = [\"[class*=list-element]\", \"[data-test-id*=vertical-activity-card]\"]\n",
    "        for selector in selectors:\n",
    "            tour_items = soup.select(selector)\n",
    "            if tour_items:\n",
    "                break\n",
    "        if len(tour_items) == 0:\n",
    "            print('process_html_from_response_scraperapi: tour_items lengght is 0 and need to get once again results', tour_items)\n",
    "            continue\n",
    "            \n",
    "#             print(soup)\n",
    "#             print(row)\n",
    "            while len(tour_items) == 0:\n",
    "                print('process_html_from_response_scraperapi: tour_items lengght is 0 and need to get once again results', tour_items)\n",
    "                random_country_code = random.choice(country_codes)\n",
    "                url_request = requests.post(url = 'https://async.scraperapi.com/jobs', \n",
    "                                    json={'apiKey': f'{API_KEY}', \n",
    "                                          'country_code': random_country_code,\n",
    "                                          'url': row['UrlRequest'] })\n",
    "                \n",
    "                print('process_html_from_response_scraperapi: ', url_request.json()['statusUrl'])\n",
    "                finished, attempts = get_status(url_request.json()['statusUrl'])\n",
    "                while finished != 'finished':\n",
    "                    finished, attempts = get_status(url_request.json()['statusUrl'])\n",
    "                    \n",
    "                results = requests.get(url_request.json()['statusUrl'])            \n",
    "                soup = BeautifulSoup(results.content, 'html.parser') \n",
    "                for selector in selectors:\n",
    "                    tour_items = soup.select(selector)\n",
    "                    if tour_items:\n",
    "                        print(f'New lenght of tour items is: {len(tour_items)}')\n",
    "                        break\n",
    "            \n",
    "        for tour_item in tour_items:\n",
    "        #     title = tour_item.find({'class': 'vertical-activity-card__title'}).text.strip()\n",
    "            title = tour_item.select_one('[data-test-id*=activity-card-title]').text.strip()\n",
    "            price = tour_item.select_one('[data-test-id*=activity-card-starting-price]').text.strip().replace('\\xa0', '')\n",
    "            splitter = price.split('From')[-1].strip()[0]\n",
    "            part_url = tour_item.find('a')['href'].replace('\\\\\"', '')\n",
    "            product_url = f\"https://www.getyourguide.com/{part_url}\"\n",
    "            product_url = product_url.split('?ranking_uuid')[0]\n",
    "            position = position + 1\n",
    "            siteuse = 'GYG'\n",
    "            city = row['City']\n",
    "            category = row['Category']\n",
    "            try:\n",
    "                discount = tour_item.select_one('[data-test-id*=activity-card-base-price]').text.strip().replace('\\xa0', '')\n",
    "                discount = price\n",
    "                price = tour_item.select_one('[data-test-id*=activity-card-base-price]').text.strip().replace('\\xa0', '')\n",
    "            except:\n",
    "                discount = 'N/A'\n",
    "            try:\n",
    "                amount_reviews = tour_item.select_one('[data-test-id*=activity-card-review-count]').text.strip()\n",
    "                amount_reviews = re.sub('[( , reviews)]', '', amount_reviews)\n",
    "            except:\n",
    "                amount_reviews = 'N/A'\n",
    "            try:\n",
    "                stars = tour_item.select_one('[data-test-id*=activity-card-rating-overall]').text.strip()\n",
    "            except:\n",
    "                stars = 'N/A'\n",
    "            try:\n",
    "                booked = tour_item.select_one('[data-test-id*=activity-card-badge-booked]').text.strip()\n",
    "            except:\n",
    "                booked = 'N/A'\n",
    "            try:\n",
    "                new_activity = tour_item.select_one('[data-test-id*=activity-card-badge-new-activity]').text.strip()\n",
    "            except:\n",
    "                new_activity = 'N/A'\n",
    "\n",
    "            text = tour_item.text.strip()\n",
    "\n",
    "            data.append([title,product_url, price, stars, amount_reviews, discount, text, date_today, position, category, booked, siteuse, city ])\n",
    "        print(f'process_html_from_response_scraperapi: URL city: {city} currency splitter: {splitter}')\n",
    "    url_done = time.time()\n",
    "    # message = f'Time for {city}-{category}: {round((url_done - url_time)/60, 3)}min | Pages: {max_pages} | AVG {round((url_done - url_time)/max_pages, 2)}s per page Currency: 1-{first_style_curr}, 2-{second_style_curr}, 3-{thirtd_style_curr}'\n",
    "    # print(message)\n",
    "    # logger_info.info(message)\n",
    "    df = pd.DataFrame(data, columns=['Tytul', 'Tytul URL', 'Cena', 'Opinia', 'IloscOpini', 'Przecena', 'Tekst', 'Data zestawienia', 'Pozycja', 'Kategoria', 'Booked', 'SiteUse', 'Miasto'])\n",
    "    df['Cena'] = df['Cena'].map(lambda x: x.split(' ')[-1])\n",
    "    df['Przecena'] = df['Przecena'].map(lambda x: x.split('From')[1] if x != 'N/A' else 'N/A')\n",
    "    df['IloscOpini'] = df['IloscOpini'].map(lambda x: x.split('(')[-1].split(')')[0].split(' ')[0].replace(',', '') if x != 'N/A' else x)\n",
    "    df['VPN_City'] = ''\n",
    "    file_path = fr'{output_gyg}/{date_today}-{city}-GYG.csv' \n",
    "    df.to_csv(file_path, header=not os.path.exists(file_path), index=False, mode='a', encoding=\"utf-8\")\n",
    "    data_city_df.to_csv(file_path_done, header=not os.path.exists(file_path_done), index=False, mode='a')\n",
    "    os.remove(city_path_done)\n",
    "#     row.to_csv(file_path_done, header=True, index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c421959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_to_xlsx():\n",
    "    # Get all CSV files with the specified date prefix\n",
    "    csv_files = [file for file in os.listdir(f'{output_gyg}') if file.endswith('.csv') and file.startswith(date_today)]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found with the date prefix '{date_today}'\")\n",
    "        return\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    output_file = file_path_output\n",
    "    writer = pd.ExcelWriter(output_file, engine='xlsxwriter')\n",
    "    dataframes_to_combine = []\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE', 'Done']\n",
    "    for csv_file in csv_files:\n",
    "        \n",
    "        csv_path = os.path.join(f'{output_gyg}', csv_file)\n",
    "        sheet_name = os.path.splitext(csv_file)[0]\n",
    "        sheet_name = sheet_name.split(date_today + '-')[1].split('-GYG')[0]\n",
    "        # Read the CSV file into a DataFrame\n",
    "        if sheet_name not in exclude_sheets:\n",
    "            df = pd.read_csv(csv_path)\n",
    "#             Adding position values to dataframe\n",
    "            for row_num in range(0,len(df)):\n",
    "                df.at[row_num, 'Pozycja'] = row_num + 1\n",
    "            dataframes_to_combine.append(df)\n",
    "\n",
    "        # Write the DataFrame to the Excel file\n",
    "#         if len(sheet_name) > 30:\n",
    "#             sheet_name = sheet_name.split('-')[0]\n",
    "#         df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    combined_data = pd.concat(dataframes_to_combine, ignore_index=True)\n",
    "    combined_data.to_excel(writer, sheet_name='AllLinks', index=False)\n",
    "\n",
    "    # Save the Excel file\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    print(f\"Combined CSV files with date prefix '{date_today}' into '{output_file}'\")\n",
    "\n",
    "    # Remove the CSV files\n",
    "#     for csv_file in csv_files:\n",
    "#         os.remove(csv_file)\n",
    "    # Move the CSV files to the Archive folder\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(f'{output_gyg}', csv_file)\n",
    "        destination_path = os.path.join(archive_folder, csv_file)\n",
    "        shutil.move(csv_path, destination_path)\n",
    "\n",
    "    print(f\"Moved {len(csv_files)} CSV file(s) to the '{archive_folder}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ad35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_1st_page_in_dataframe(df_links):\n",
    "    if os.path.exists(file_path_done):\n",
    "        done_msg = pd.read_csv(file_path_done).drop_duplicates(subset=['City', 'Category'], keep='last').reset_index()\n",
    "        df_links = df_links[~(df_links['City'].isin(done_msg['City']) & df_links['Category'].isin(done_msg['Category']))]\n",
    "#         df_links = df_links[~df_links['City'].isin(done_msg['City'].values)]\n",
    "        \n",
    "    else:\n",
    "        print(\"Nothing done yet\")\n",
    "# ################# THE BELWO CODE PROCESSED ONLY ONE PAGE TO GET MAXIMUM AMOUNT OF PAGES ON THE WEBSITE   \n",
    "    for index, row in df_links.iterrows():\n",
    "        url = row[\"URL\"]\n",
    "        city = row['City']\n",
    "        category = row['Category']\n",
    "        print('run_1st_page_in_dataframe: ', city, category, url )\n",
    "        send_url_to_process_scraperapi(url, city, category, max_pages=1)\n",
    "        \n",
    "    while not df_links.empty:\n",
    "        print(len(df_links))\n",
    "        df_links, processed_cities = check_status_and_process_city_data(df_links)\n",
    "# ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f55fbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pages_in_dataframe(df_links):\n",
    "    if os.path.exists(file_path_done):\n",
    "        done_msg = pd.read_csv(file_path_done).drop_duplicates(subset=['City', 'Category'], keep='last').reset_index()\n",
    "        df_links_with_page_maxpage = df_links[df_links['City'].isin(done_msg['City'].values)]\n",
    "        df_links_with_page_maxpage = pd.merge(df_links_with_page_maxpage, done_msg[['City', 'Page', 'MaxPage']], on='City', how='left')\n",
    "#         df_links = df_links[~df_links['City'].isin(done_msg['City'].values)]\n",
    "        \n",
    "# #################### GET DATA FOR ALL PAGES \n",
    "    for index, row in df_links_with_page_maxpage.iterrows():\n",
    "        url = row[\"URL\"]\n",
    "        city = row['City']\n",
    "        category = row['Category']\n",
    "        page = row['Page'] + 1\n",
    "        max_page = round(row['MaxPage']*0.8, 0)\n",
    "        print(f'run_all_pages_in_dataframe: {city} - {max_page} - {url}')\n",
    "        send_url_to_process_scraperapi(url, city, category, page, max_page)\n",
    "        \n",
    "    while not df_links_with_page_maxpage.empty:\n",
    "        print('run_all_pages_in_dataframe:' , len(df_links))\n",
    "        df_links_with_page_maxpage, processed_cities = check_status_and_process_city_data(df_links_with_page_maxpage)\n",
    "        print(f'Processed cities: {processed_cities}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83aac548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name):\n",
    "    global mapping_currency\n",
    "    global date_today\n",
    "    global currency_list\n",
    "    exclude_sheets = ['Sheet1', 'Data', 'Re-Run', 'DONE']\n",
    "    # Define the Azure Blob Storage connection details\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "    # Read the Excel file into a Pandas DataFrame\n",
    "    rates_eur = get_rates(date_today, 'EUR')\n",
    "#     rates_gbp = get_rates(date_today, 'EUR')\n",
    "#     GBP AND USD ARE NOT SUPORTED WITHING THIS CURRENT SUBSRICPTION UPGRADE PLAN\n",
    "#     rates_gbp = get_rates(date_today, 'GBP')\n",
    "\n",
    "#     rates_usd = get_rates(date_today, 'USD')\n",
    "    excel_data = pd.read_excel(local_file_path, sheet_name=None)\n",
    "    output_file_path = file_path_output_processed\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        for sheet_name, df in excel_data.items():\n",
    "            if sheet_name in exclude_sheets:\n",
    "                continue\n",
    "            # Make changes to the df DataFrame as needed\n",
    "            df['Data zestawienia'] = df['Data zestawienia'].astype('str')\n",
    "            df['IloscOpini'].fillna(0, inplace= True)\n",
    "            df['Opinia'].fillna('N/A', inplace=True)\n",
    "            df = df[df['Tytul'] != 'Tytul']\n",
    "            df = df[df['Data zestawienia'] != 'Data zestawienia']\n",
    "            df = df[df['Data zestawienia'].str.len() > 4]\n",
    "            df.drop(columns=['VPN_City', 'Tekst'], inplace=True)\n",
    "            df['Booked'] = df['Booked'].astype('str')\n",
    "            df['Przecena'] = df['Przecena'].astype('str')\n",
    "    #         df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "            df['Booked'] = df['Booked'].str.replace('New activity', 'nan')\n",
    "            df['Booked'] = df['Booked'].map(lambda x: x.split('Booked')[1].split()[0] if len(x) > 5 else x)\n",
    "            df['Przecena'] = df['Przecena'].map(lambda x: x.split()[1].replace(\",\", \"\") if len(x) > 4 else x)\n",
    "            df['Przecena'].fillna(\"NULL\", inplace= True)\n",
    "            #     df['VPN_City'].fillna(\"NULL\", inplace= True)\n",
    "            df['Booked'].fillna(\"NULL\", inplace= True)\n",
    "            for index, row in df.iterrows():\n",
    "                currency = ''\n",
    "                if 'per group' in row['Cena']:\n",
    "                    df.at[index, 'Cena'] = row['Cena'].split('per group')[0]\n",
    "                    row['Cena']= row['Cena'].split('per group')[0]\n",
    "\n",
    "                for i in range(len(row['Cena']) - 1):\n",
    "                    if not row['Cena'][i].isnumeric():\n",
    "                        currency = currency + (row['Cena'][i])\n",
    "                    else:\n",
    "                        if row['Cena'][i] == '¹':\n",
    "                            currency = currency + (row['Cena'][i])\n",
    "                            continue\n",
    "                        try:\n",
    "                            price = float(row['Cena'][i:].split()[0].replace(',',''))\n",
    "                        except ValueError as ve:\n",
    "    ## #                         CHECK IF THE CURRENCY CODE IS NOT IN THE LAST POART OF CURRECNY EXAMPLE 324.14zł\n",
    "                            for j in range(len(row['Cena']) - 1, -1, -1):\n",
    "                                if not row['Cena'][j].isnumeric():\n",
    "                                    currency = currency + (row['Cena'][j])\n",
    "                                else:\n",
    "                                    if row['Cena'][j] == '¹':\n",
    "                                        currency = currency + (row['Cena'][j])\n",
    "                                        continue\n",
    "                                    try:\n",
    "    #                                     INVERT STRING FROM łz TO zł\n",
    "                                        currency = currency[::-1]\n",
    "                                        price = float(row['Cena'][:j].split()[0].replace(',',''))  \n",
    "                                        break\n",
    "                                    except Exception as e:\n",
    "                                        print(e)\n",
    "                        total_price = row['Cena']\n",
    "                        break\n",
    "                if sheet_name in EUR_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency][0:3]])\n",
    "                    except:\n",
    "                        print(currency, '||', sheet_name, row['Cena'], row['Tytul'])\n",
    "                        currency_list.append(currency)\n",
    "                elif sheet_name in GBP_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency][0:3]])\n",
    "                    except:\n",
    "                        print(currency, sheet_name)\n",
    "                elif sheet_name in USD_City:\n",
    "                    try:\n",
    "                        conversion_rate = float(rates_eur[mapping_currency[currency]][0:3])\n",
    "                    except:\n",
    "                        print(currency, sheet_name)\n",
    "#                 print(f'{mapping_currency[currency][0:3]} conversion rate: {conversion_rate}')\n",
    "\n",
    "                if conversion_rate != 1:\n",
    "#                     print(f'{total_price}- price: {price} - covnersion: {price/(conversion_rate*1.020)}')\n",
    "                    df.at[index, 'Cena'] = round(price/(conversion_rate*1.0185), 2)\n",
    "                else:\n",
    "                    df.at[index, 'Cena'] = price\n",
    "            \n",
    "                currency_list.append(currency)\n",
    "\n",
    "            currency_list = list(set(currency_list))\n",
    "#             display(df)\n",
    "\n",
    "    #         df['Cena'] = df['Cena'].map(lambda x: x.split(x[0])[1].strip() if not x[0].isnumeric() else x)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # Create a connection to Azure Blob Storage\n",
    "#     blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "#     container_client = blob_service_client.get_container_client(container_name_refined)\n",
    "\n",
    "#     # Upload the modified Excel file to Azure Blob Storage\n",
    "#     with open(output_file_path, \"rb\") as data:\n",
    "#         container_client.upload_blob(name=blob_name, data=data)\n",
    "        \n",
    "#     print(\"File uploaded successfully to Azure Blob Storage (refined).\")\n",
    "#     os.remove(output_file_path)\n",
    "#     create_log_done('Refined')\n",
    "    return 'Added to Blob', currency_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac26d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_run_viator_all_links(df_links=pd.DataFrame()):\n",
    "    \n",
    "    df_links = pd.read_csv(all_links_file)\n",
    "    df_links = df_links[(df_links['Category'] == 'Global') & (df_links['Run'] == 1)]\n",
    "    \n",
    "    if os.path.exists(file_path_output):\n",
    "        print(f'Today ({date_today}) GYG done')\n",
    "        return 'Done'     \n",
    "    run_1st_page_in_dataframe(df_links)\n",
    "    run_all_pages_in_dataframe(df_links)\n",
    "    return 'Done'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f7ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msg_output = ''\n",
    "while True:\n",
    "    if msg_output == 'Done':\n",
    "        print('Break')\n",
    "        break\n",
    "    try:\n",
    "        msg_output = weekly_run_viator_all_links()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "try:\n",
    "    combine_csv_to_xlsx()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    text, currency_list = transform_upload_to_refined(local_file_path, storage_account_name, storage_account_key, container_name_refined, blob_name)\n",
    "    print(currency_list)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce4fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fba644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589f0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
