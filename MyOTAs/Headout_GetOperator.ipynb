{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35025a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common_functions\n",
    "import os\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import common_functions\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446d458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "site = \"Headout\"\n",
    "file_manager = common_functions.FilePathManager(site, \"NA\")\n",
    "headout_file_path_xlsx_operator = file_manager.get_file_paths()['file_path_xlsx_operator']\n",
    "\n",
    "site = \"Musement\"\n",
    "file_manager = common_functions.FilePathManager(site, \"NA\")\n",
    "musement_file_path_xlsx_operator = file_manager.get_file_paths()['file_path_xlsx_operator']\n",
    "\n",
    "site = \"GYG\"\n",
    "file_manager = common_functions.FilePathManager(site, \"NA\")\n",
    "gyg_file_path_xlsx_operator = file_manager.get_file_paths()['file_path_xlsx_operator']\n",
    "\n",
    "\n",
    "viator_file_path_xlsx_operator = file_manager.get_file_paths()['file_path_xlsx_operator'].replace('Operators_GYG', 'Operators_Groups')\n",
    "\n",
    "logger = common_functions.LoggerManager(file_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575c341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e51269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all dataset filenames\n",
    "dataset_files = [headout_file_path_xlsx_operator, musement_file_path_xlsx_operator, gyg_file_path_xlsx_operator, viator_file_path_xlsx_operator]  # Add all your dataset filenames here\n",
    "\n",
    "# Load datasets into a dictionary of DataFrames\n",
    "datasets = {}\n",
    "for file in dataset_files:\n",
    "    dataset_name = file.split(\"\\\\\")[-1] # e.g., 'dataset_A'\n",
    "    datasets[dataset_name] = pd.read_excel(os.path.join(file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Optional: Define a list of stopwords to exclude\n",
    "stopwords = set([\n",
    "    'and', 'or', 'the', 'a', 'an', 'to', 'from', 'of', 'in', 'with', 'on', 'for', 'by'\n",
    "    # Add more stopwords as needed\n",
    "])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply preprocessing to all datasets\n",
    "for name, df in datasets.items():\n",
    "    try:\n",
    "        df['Tytul_preprocessed'] = df['Tytul'].apply(preprocess).apply(remove_stopwords)\n",
    "        df['City_preprocessed'] = df['City'].apply(preprocess).apply(remove_stopwords)\n",
    "        logger.logger_info.info(f\"Preprocessed 'Tytul' and 'City' columns for dataset: {name}\")\n",
    "    except Exception as e:\n",
    "        logger.logger_err.error(f\"Error preprocessing dataset {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd9e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_threshold(text_length):\n",
    "    if text_length < 20:\n",
    "        return 90  # Higher threshold for short strings\n",
    "    elif text_length < 40:\n",
    "        return 80\n",
    "    else:\n",
    "        return 70  # Lower threshold for longer strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1752d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(a, b):\n",
    "    return fuzz.token_set_ratio(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dataset A\n",
    "dataset_A = datasets.get('Operators_Headout.xlsx')\n",
    "if dataset_A is None:\n",
    "    logger.logger_err.error(\"Dataset A not found. Please ensure 'Operators_Headout.xlsx' is in the dataset folder.\")\n",
    "    raise FileNotFoundError(\"Dataset A not found.\")\n",
    "\n",
    "# Enforce 'Operator' column to be string across all datasets\n",
    "for name, df in datasets.items():\n",
    "    try:\n",
    "        df['Operator'] = df['Operator'].astype(str)\n",
    "    except Exception as e:\n",
    "        logger.logger_err.error(f\"Error converting 'Operator' to string in dataset {name}: {e}\")\n",
    "\n",
    "# Initialize new columns with empty strings to match DataFrame length\n",
    "new_columns = ['Matched_Operators', 'Similarity_Scores', 'Matched_Tytuls', 'Links']\n",
    "for col in new_columns:\n",
    "    dataset_A[col] = ''\n",
    "\n",
    "logger.logger_info.info(\"Initialized new columns: Matched_Operators, Similarity_Scores, Matched_Tytuls, Links.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all other datasets into a single DataFrame for comparison\n",
    "other_datasets = [df for name, df in datasets.items() if name != 'Operators_Headout.xlsx']\n",
    "if not other_datasets:\n",
    "    logger.logger_err.error(\"No other datasets found to compare with Dataset A.\")\n",
    "    raise ValueError(\"No other datasets available for comparison.\")\n",
    "\n",
    "combined_other = pd.concat(other_datasets, ignore_index=True)\n",
    "combined_other = combined_other.drop_duplicates(subset=['Tytul_preprocessed', 'City_preprocessed'])\n",
    "logger.logger_info.info(\"Combined all other datasets into 'combined_other' DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9db36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a minimum length for Tytul to be considered for matching\n",
    "MIN_TYTL_LENGTH = 5  # Adjust as needed\n",
    "\n",
    "total_rows = len(dataset_A)\n",
    "logger.logger_info.info(f\"Starting mapping operators for {total_rows} entries in Dataset A.\")\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "with tqdm(total=total_rows, desc=\"Processing Dataset A\", unit=\"row\") as pbar:\n",
    "    for index, row in dataset_A.iterrows():\n",
    "        try:\n",
    "            city = row['City_preprocessed']\n",
    "            tytul = row['Tytul_preprocessed']\n",
    "            \n",
    "            # Skip if tytul is too short\n",
    "            if len(tytul) < MIN_TYTL_LENGTH:\n",
    "                logger.logger_info.info(f\"Skipped '{row['Tytul']}' due to short length.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Determine similarity threshold based on tytul length\n",
    "            threshold = get_similarity_threshold(len(tytul))\n",
    "            \n",
    "            # Filter combined_other for the same city\n",
    "            same_city = combined_other[combined_other['City_preprocessed'] == city]\n",
    "            \n",
    "            # Log if no entries found in the same city\n",
    "            if same_city.empty:\n",
    "                logger.logger_info.info(f\"No matches found for '{row['Tytul']}' in city '{row['City']}'.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Use RapidFuzz's process.extract to find all matches above the threshold\n",
    "            matches = process.extract(\n",
    "                tytul,\n",
    "                same_city['Tytul_preprocessed'],\n",
    "                scorer=fuzz.token_set_ratio,\n",
    "                limit=None\n",
    "            )\n",
    "            \n",
    "            # Filter matches above the dynamic threshold\n",
    "            good_matches = [match for match in matches if match[1] >= threshold]\n",
    "            \n",
    "            logger.logger_info.info(f\"Found {len(good_matches)} good matches for '{row['Tytul']}' in city '{row['City']}' with threshold {threshold}.\")\n",
    "            \n",
    "            if good_matches:\n",
    "                # Sort good_matches by similarity descending\n",
    "                good_matches_sorted = sorted(good_matches, key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                operator_similarity = {}\n",
    "                matched_tytuls = {}\n",
    "                links = {}\n",
    "                \n",
    "                for match in good_matches_sorted:\n",
    "                    matched_tytul_preprocessed = match[0]\n",
    "                    similarity = match[1]\n",
    "                    # Get the Operator and Link from the matched record\n",
    "                    matched_records = same_city[same_city['Tytul_preprocessed'] == matched_tytul_preprocessed]\n",
    "                    if not matched_records.empty:\n",
    "                        operator = matched_records.iloc[0]['Operator']\n",
    "                        link = matched_records.iloc[0].get('Link', '')  # Ensure 'Link' exists\n",
    "                        # Ensure operator is a valid string\n",
    "                        if not isinstance(operator, str) or operator.lower() == 'nan':\n",
    "                            logger.logger_err.error(f\"Invalid Operator '{operator}' for Tytul: '{matched_tytul_preprocessed}'. Skipping this match.\")\n",
    "                            continue\n",
    "                        # Avoid matching generic terms like 'Combo'\n",
    "                        if operator.lower() in stopwords:\n",
    "                            logger.logger_info.info(f\"Skipped generic operator '{operator}' for Tytul: '{matched_tytul_preprocessed}'.\")\n",
    "                            continue\n",
    "                        # Keep the highest similarity score for each operator\n",
    "                        if operator not in operator_similarity or similarity > operator_similarity[operator]:\n",
    "                            operator_similarity[operator] = similarity\n",
    "                            matched_tytuls[operator] = matched_records.iloc[0]['Tytul']  # Original Tytul\n",
    "                            links[operator] = link\n",
    "                    else:\n",
    "                        logger.logger_err.error(f\"No Operator found for matched Tytul: '{matched_tytul_preprocessed}'.\")\n",
    "                \n",
    "                # Now, operator_similarity has operator: highest_similarity_score\n",
    "                matched_operators_unique = list(operator_similarity.keys())\n",
    "                similarity_scores_unique = [str(int(sim)) for sim in operator_similarity.values()]\n",
    "                matched_tytuls_unique = [matched_tytuls[op] for op in matched_operators_unique]\n",
    "                links_unique = [links[op] for op in matched_operators_unique]\n",
    "                \n",
    "                # Assign to the DataFrame\n",
    "                dataset_A.at[index, 'Matched_Operators'] = ', '.join(matched_operators_unique)\n",
    "                dataset_A.at[index, 'Similarity_Scores'] = ', '.join(similarity_scores_unique)\n",
    "                dataset_A.at[index, 'Matched_Tytuls'] = ', '.join(matched_tytuls_unique)\n",
    "                dataset_A.at[index, 'Links'] = ', '.join(links_unique)\n",
    "                \n",
    "                logger.logger_info.info(\n",
    "                    f\"Matched Operators for '{row['Tytul']}': {matched_operators_unique} with similarities {similarity_scores_unique}, Matched Tytuls {matched_tytuls_unique}, Links {links_unique}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.logger_err.error(f\"Error processing row index {index} for Tytul '{row['Tytul']}': {e}\")\n",
    "        finally:\n",
    "            pbar.update(1)  # Update the progress bar regardless of success or failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd45603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b321be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf1223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Step 8: Explode the Matched Columns into Separate Rows\n",
    "# ==========================================\n",
    "\n",
    "try:\n",
    "    dataset_A_exploded = dataset_A.copy()\n",
    "    \n",
    "    # Split the concatenated strings into lists\n",
    "    dataset_A_exploded['Matched_Operators'] = dataset_A_exploded['Matched_Operators'].str.split(', ')\n",
    "    dataset_A_exploded['Similarity_Scores'] = dataset_A_exploded['Similarity_Scores'].str.split(', ')\n",
    "    dataset_A_exploded['Matched_Tytuls'] = dataset_A_exploded['Matched_Tytuls'].str.split(', ')\n",
    "    dataset_A_exploded['Links'] = dataset_A_exploded['Links'].str.split(', ')\n",
    "    \n",
    "    # Verify that all lists have the same number of elements\n",
    "    mismatched_counts = dataset_A_exploded[\n",
    "        (dataset_A_exploded['Matched_Operators'].apply(len) != dataset_A_exploded['Similarity_Scores'].apply(len)) |\n",
    "        (dataset_A_exploded['Matched_Operators'].apply(len) != dataset_A_exploded['Matched_Tytuls'].apply(len)) |\n",
    "        (dataset_A_exploded['Matched_Operators'].apply(len) != dataset_A_exploded['Links'].apply(len))\n",
    "    ]\n",
    "    \n",
    "    if not mismatched_counts.empty:\n",
    "        logger.logger_err.error(f\"Found {len(mismatched_counts)} rows with mismatched list lengths. These rows will be skipped during explosion.\")\n",
    "        # Optionally, log the first few rows for inspection\n",
    "        logger.logger_err.error(f\"Sample mismatched rows:\\n{mismatched_counts.head()}\")\n",
    "        # Remove these rows to prevent explosion errors\n",
    "        dataset_A_exploded = dataset_A_exploded[\n",
    "            (dataset_A_exploded['Matched_Operators'].apply(len) == dataset_A_exploded['Similarity_Scores'].apply(len)) &\n",
    "            (dataset_A_exploded['Matched_Operators'].apply(len) == dataset_A_exploded['Matched_Tytuls'].apply(len)) &\n",
    "            (dataset_A_exploded['Matched_Operators'].apply(len) == dataset_A_exploded['Links'].apply(len))\n",
    "        ]\n",
    "    \n",
    "    # Explode the lists to create separate rows for each match\n",
    "    dataset_A_exploded = dataset_A_exploded.explode(['Matched_Operators', 'Similarity_Scores', 'Matched_Tytuls', 'Links'])\n",
    "    \n",
    "    # Define a safe conversion function for similarity scores\n",
    "    def safe_convert_sim(x):\n",
    "        try:\n",
    "            return int(float(x))\n",
    "        except ValueError:\n",
    "            logger.logger_err.error(f\"Cannot convert similarity score '{x}' to int.\")\n",
    "            return None\n",
    "    \n",
    "    # Convert similarity scores to integers\n",
    "    dataset_A_exploded['Similarity_Score'] = dataset_A_exploded['Similarity_Scores'].apply(safe_convert_sim)\n",
    "    \n",
    "    # Drop rows where similarity score conversion failed\n",
    "    num_failed_conversions = dataset_A_exploded['Similarity_Score'].isnull().sum()\n",
    "    if num_failed_conversions > 0:\n",
    "        logger.logger_err.error(f\"Found {num_failed_conversions} rows with invalid similarity scores after conversion. These rows will be removed.\")\n",
    "        dataset_A_exploded = dataset_A_exploded.dropna(subset=['Similarity_Score'])\n",
    "    \n",
    "    # Sort the exploded DataFrame by 'Similarity_Score' in descending order\n",
    "    dataset_A_exploded = dataset_A_exploded.sort_values(by='Similarity_Score', ascending=False)\n",
    "    \n",
    "    # Drop the old 'Similarity_Scores' column\n",
    "    dataset_A_exploded = dataset_A_exploded.drop(columns=['Similarity_Scores'])\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    dataset_A_exploded = dataset_A_exploded.rename(columns={\n",
    "        'Matched_Operators': 'Matched_Operator',\n",
    "        'Similarity_Score': 'Similarity_Score'\n",
    "    })\n",
    "    \n",
    "    logger.logger_info.info(\"Exploded matched operators, similarity scores, matched Tytuls, and links into separate rows, sorted by similarity.\")\n",
    "except Exception as e:\n",
    "    logger.logger_err.error(f\"Error exploding matched operators: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c15673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Step 9: Save the Updated Datasets\n",
    "# ==========================================\n",
    "\n",
    "try:\n",
    "    updated_dataset_path = 'Operators_Headout_updated.xlsx'  # Replace with your desired path\n",
    "    dataset_A.to_excel(updated_dataset_path, index=False)\n",
    "    logger.logger_done.info(f\"Successfully saved the updated Dataset A to '{updated_dataset_path}'.\")\n",
    "    \n",
    "    # Save the exploded version\n",
    "    exploded_dataset_path = 'Operators_Headout_exploded.xlsx'  # Replace with your desired path\n",
    "    dataset_A_exploded.to_excel(exploded_dataset_path, index=False)\n",
    "    logger.logger_done.info(f\"Successfully saved the exploded Dataset A to '{exploded_dataset_path}'.\")\n",
    "except Exception as e:\n",
    "    logger.logger_err.error(f\"Error saving updated datasets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38528ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
